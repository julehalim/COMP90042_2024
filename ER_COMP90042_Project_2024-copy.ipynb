{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32yCsRUo8H33"
   },
   "source": [
    "# 2024 COMP90042 Project\n",
    "*Make sure you change the file name with your group id.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCybYoGz8YWQ"
   },
   "source": [
    "# Readme\n",
    "*If there is something to be noted for the marker, please mention here.*\n",
    "\n",
    "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6po98qVA8bJD"
   },
   "source": [
    "# 1.DataSet Processing\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvff21Hv8zjk"
   },
   "source": [
    "## Read Files to get raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def readJsFile(filename) :\n",
    "    return json.load(open(filename,'r'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' define functions to preprocess raw texts with tokenization, lemmatization, and remove stopwords'''\n",
    "import nltk,re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "tt = TweetTokenizer()\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "stopwords = set(stopwords.words('english'))     # note: stopwords are all in lowercase\n",
    "stopwords.remove('not')                         # not, no are not meaningless\n",
    "stopwords.remove('no')\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "    \n",
    "def preprocess_text(text):\n",
    "    # Tokenization and lemmatization\n",
    "    tokens = tt.tokenize(text.lower())\n",
    "    tokens = [lemmatize(t) for t in tokens]\n",
    "    \n",
    "    # remove any word that does not contain any English letters,\n",
    "    # including punctuation\n",
    "    valid_tokens = []\n",
    "    for t in tokens:\n",
    "        if re.search(r\"[a-z]\",t):\n",
    "            valid_tokens.append(t)\n",
    "\n",
    "    # remove stop words\n",
    "    tokens = []\n",
    "    for t in valid_tokens:\n",
    "        if t not in stopwords:\n",
    "            tokens.append(t)\n",
    "\n",
    "    # resemble processed tokens back to text\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "'not' in stopwords and 'no' in stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into **claim-texts, cliam-ids**, & **claim-labels** & **evidences list**\n",
    "\n",
    "to run code below, change the block from **raw block** to **code block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= readJsFile('data/train-claims.json')\n",
    "dev_data = readJsFile('data/dev-claims.json')\n",
    "test_data = readJsFile('data/test-claims-unlabelled.json')\n",
    "all_evidences = readJsFile('data/evidence.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, isTest=False, isEvd=False):\n",
    "    ids = []\n",
    "    text = []\n",
    "    labels = []\n",
    "    evidences = []\n",
    "\n",
    "    # split test data\n",
    "    if isTest and not isEvd:\n",
    "        for claim_id, data in data.items():\n",
    "            ids.append(claim_id)\n",
    "            text.append(preprocess_text(data['claim_text']))\n",
    "    # split evidences\n",
    "    elif isEvd and not isTest:\n",
    "        for evidence_id, data in data.items():\n",
    "            evidences.append(preprocess_text(data))\n",
    "    # split train data\n",
    "    elif not isTest and not isEvd:\n",
    "        for claim_id, data in data.items():\n",
    "            ids.append(claim_id)\n",
    "            text.append(preprocess_text(data['claim_text']))\n",
    "            labels.append(data['claim_label'])\n",
    "            evidences.append(data['evidences'])\n",
    "    else:\n",
    "        print('Wrong Mode, please check your arguments: isTest and isEvd')\n",
    "        \n",
    "    return ids, text, labels, evidences"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_ids, train_texts, train_labels, train_evidences = split_data(train_data)\n",
    "test_ids, test_texts, _, _ = split_data(test_data, isTest=True)\n",
    "dev_ids, dev_texts, dev_labels, dev_evidences = split_data(dev_data)\n",
    "#_,_,_, evidences_lst = split_data(all_evidences, isEvd=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# this step is time-consuming\n",
    "_,_,_, evidences_lst = split_data(all_evidences, isEvd=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# save data for debugging\n",
    "# please comment the code out when submmiting\n",
    "\n",
    "def save_data(data, filename):\n",
    "    json.dump(data, open(filename, \"w\"))\n",
    "\n",
    "save_data(train_ids, 'temp_data/train_ids.json')\n",
    "save_data(train_texts, 'temp_data/train_texts.json')\n",
    "save_data(train_labels, 'temp_data/train_labels.json')\n",
    "save_data(train_evidences, 'temp_data/train_evidences.json')\n",
    "\n",
    "save_data(dev_ids, 'temp_data/dev_ids.json')\n",
    "save_data(dev_texts, 'temp_data/dev_texts.json')\n",
    "save_data(dev_labels, 'temp_data/dev_labels.json')\n",
    "save_data(dev_evidences, 'temp_data/dev_evidences.json')\n",
    "\n",
    "save_data(test_ids, 'temp_data/test_ids.json')\n",
    "save_data(test_texts, 'temp_data/test_texts.json')\n",
    "\n",
    "save_data(evidences_lst, 'temp_data/evidences_lst.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved data\n",
    "# please comment the code out when submmiting\n",
    "import json\n",
    "\n",
    "def load_data(filename):\n",
    "    return json.load(open(filename, \"r\"))\n",
    "\n",
    "train_ids = load_data(\"temp_data/train_ids.json\")\n",
    "train_texts = load_data(\"temp_data/train_texts.json\")\n",
    "train_labels = load_data(\"temp_data/train_labels.json\")\n",
    "train_evidences = load_data(\"temp_data/train_evidences.json\")\n",
    "\n",
    "dev_ids = load_data(\"temp_data/dev_ids.json\")\n",
    "dev_texts = load_data(\"temp_data/dev_texts.json\")\n",
    "dev_labels = load_data(\"temp_data/dev_labels.json\")\n",
    "dev_evidences = load_data(\"temp_data/dev_evidences.json\")\n",
    "\n",
    "test_ids = load_data(\"temp_data/test_ids.json\")\n",
    "test_texts = load_data(\"temp_data/test_texts.json\")\n",
    "\n",
    "evidences_lst = load_data(\"temp_data/evidences_lst.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not no scientific evidence co2 pollutant higher co2 concentration actually help ecosystem support plant animal life\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'john bennet lawes english entrepreneur agricultural scientist'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying correctness\n",
    "print(train_texts[0])\n",
    "evidences_lst[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tfidf word embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1210055, 500000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = evidences_lst + train_texts              # text documents\n",
    "vectorizer = TfidfVectorizer(max_features=500000) # initialization\n",
    "\n",
    "# Fit the vectorizer to the data and transform the documents into TF-IDF vectors\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.shape)\n",
    "\n",
    "# To see the feature names (terms)\n",
    "#feature_names = vectorizer.get_feature_names_out()\n",
    "#print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1228, 500000)\n",
      "(154, 500000)\n",
      "(153, 500000)\n",
      "(1208827, 500000)\n"
     ]
    }
   ],
   "source": [
    "# transform data into tf-idf vectors\n",
    "train_tfidf = vectorizer.transform(train_texts)\n",
    "print(train_tfidf.shape)\n",
    "\n",
    "dev_tfidf = vectorizer.transform(dev_texts)\n",
    "print(dev_tfidf.shape)\n",
    "\n",
    "test_tfidf = vectorizer.transform(test_texts)\n",
    "print(test_tfidf.shape)\n",
    "\n",
    "evidence_tfidf = vectorizer.transform(evidences_lst)\n",
    "print(evidence_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cosine similarity\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "train_cos_sims = cosine_similarity(train_tfidf, evidence_tfidf) # needs large memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1228, 1208827)\n",
      "(153, 1208827)\n",
      "(154, 1208827)\n"
     ]
    }
   ],
   "source": [
    "print(train_cos_sims.shape)\n",
    "test_cos_sims = cosine_similarity(test_tfidf, evidence_tfidf)\n",
    "print(test_cos_sims.shape)\n",
    "dev_cos_sims = cosine_similarity(dev_tfidf, evidence_tfidf)\n",
    "print(dev_cos_sims.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we cannot use 1208827 intances in our training process\n",
    "# we have to evaluate and decide how many information will be disposed\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# def top_n_indices(ls, n):\n",
    "#     ''' Returns indices of the 20 highest numbers.'''\n",
    "#     sorted_indices = sorted(range(len(ls)), key=lambda i: ls[i], reverse=True)\n",
    "#     return sorted_indices[:n]\n",
    "\n",
    "def top_n_similarity_recall(n, cos_sims, evidences):\n",
    "    '''\n",
    "    Calculates the recall of correct evidences within the top N evidences with the highest similarity scores.\n",
    "\n",
    "    Parameters:\n",
    "        n (int): Represents the number of top similarity score items to select.\n",
    "        cos_sims (2D-list): A list where each item contains similarity score of a train text with all evidence text.\n",
    "        evidences (2D-list): A list wheere each item contains all correct evidences for each train text .\n",
    "\n",
    "    Returns:\n",
    "        The proportion of correct evidences among the top N evidences, relative to the total number of correct evidences.\n",
    "    '''\n",
    "    res = []\n",
    "    for i in range(cos_sims.shape[0]):\n",
    "        # retreive evidence_id of top n scores\n",
    "        scores = cos_sims[i]\n",
    "        # topn_idx = top_n_indices(scores, n)\n",
    "        topn_idx = np.argpartition(scores, -n)[-n:] # more efficient, find indecies of topn elements in the list\n",
    "        \n",
    "        total = 0\n",
    "        recall = 0\n",
    "        # count number of evidences we find in topn evidences\n",
    "        for e in evidences[i]:\n",
    "            e_id = int(e.split('-')[1]) # e = evidence-xxxx\n",
    "            if e_id in topn_idx:\n",
    "                recall += 1\n",
    "            total += 1\n",
    "        res.append(recall / total)\n",
    "    return sum(res) / len(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topn:100, train: 0.39066232356134567, dev: 0.44058441558441563\n"
     ]
    }
   ],
   "source": [
    "# this step is time-consuming\n",
    "# topns = [100, 200, 500, 1000]\n",
    "# for topn in topns:\n",
    "#     train_recall = top_n_similarity_recall(topn,train_cos_sims, train_evidences)\n",
    "#     dev_recall   = top_n_similarity_recall(topn,dev_cos_sims,dev_evidences)\n",
    "#     print(topn,train_recall, dev_recall)\n",
    "\n",
    "topn = 100\n",
    "train_recall = top_n_similarity_recall(topn,train_cos_sims, train_evidences)\n",
    "dev_recall   = top_n_similarity_recall(topn,dev_cos_sims,dev_evidences)\n",
    "print(f'topn:{topn}, train: {train_recall}, dev: {dev_recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 0.4706026058631922 0.5366883116883118\n"
     ]
    }
   ],
   "source": [
    "topn = 200\n",
    "train_recall = top_n_similarity_recall(topn,train_cos_sims, train_evidences)\n",
    "dev_recall   = top_n_similarity_recall(topn,dev_cos_sims,dev_evidences)\n",
    "print(topn,train_recall, dev_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topn = 500\n",
    "# train_recall = top_n_similarity_recall(topn,train_cos_sims, train_evidences)\n",
    "# dev_recall   = top_n_similarity_recall(topn,dev_cos_sims,dev_evidences)\n",
    "# print(topn,train_recall, dev_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topn = 1000\n",
    "# train_recall = top_n_similarity_recall(topn,train_cos_sims, train_evidences)\n",
    "# dev_recall   = top_n_similarity_recall(topn,dev_cos_sims,dev_evidences)\n",
    "# print(topn,train_recall, dev_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nagetive sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' find correct evidences from parts of evidences rather than all evidences, for example, first 1000 evidences instead of 120887\n",
    "    transform the multi-class classification problem to binary classification'''\n",
    "def nagetive_sampling(cos_sims, texts, evidences, all_evidence, topn, isTrain=False):\n",
    "    samples = []\n",
    "    data = []\n",
    "    label = []\n",
    "\n",
    "    for i in range(cos_sims.shape[0]):  # text similarity with all evidence texts\n",
    "        # samples\n",
    "        if isTrain:  \n",
    "            # use all positive trainning data during training process\n",
    "            for e in evidences[i]:\n",
    "                e_id = int(e.split('-')[1]) # e = evidence-xxxx\n",
    "                data.append('<cls> ' + texts[i] + '<sep> ' + all_evidence[e_id])  # combine Query and Document, like BERT\n",
    "                label.append(1)\n",
    "            # topn_ids = np.argsort(-cos_sims[i])[:topn].tolist()\n",
    "            topn_ids = np.argsort(-cos_sims[i])[25:topn+25].tolist() # dispose first 25 items with highest sim-score\n",
    "        else: # we take only topn samples in test sets\n",
    "            topn_ids = np.argpartition(cos_sims[i], -topn)[-topn:].tolist()\n",
    "        samples.append(topn_ids)\n",
    "\n",
    "        # labels & data\n",
    "        for j in topn_ids:\n",
    "            data.append('<cls>' + texts[i] + '<sep>' + all_evidence[j])\n",
    "            # if current evidence is one of the correct ones, label it true\n",
    "            j = 'evidence-'+str(j)\n",
    "            if evidences is not None: # some test data have no relative evidences\n",
    "                if j in evidences[i]:\n",
    "                    label.append(1)\n",
    "                else:\n",
    "                    label.append(0)\n",
    "\n",
    "    return samples, data, label\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "topn = 200\n",
    "train_ns_samples, train_ns_data, train_ns_label = nagetive_sampling(\n",
    "    train_cos_sims, train_texts, train_evidences, evidences_lst, topn, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_ns_samples, dev_ns_data, dev_ns_label = nagetive_sampling(\n",
    "    dev_cos_sims, dev_texts, dev_evidences, evidences_lst, topn, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ns_samples, test_ns_data, _ = nagetive_sampling(\n",
    "    test_cos_sims, test_texts, None, evidences_lst, topn, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 244637, 1: 5085})\n",
      "Counter({0: 30555, 1: 245})\n",
      "ratio: 1.0\n"
     ]
    }
   ],
   "source": [
    "# data inspection\n",
    "from collections import Counter\n",
    "\n",
    "train_ns_label = np.array(train_ns_label)\n",
    "dev_ns_label = np.array(dev_ns_label)\n",
    "\n",
    "print(Counter(train_ns_label))\n",
    "print(Counter(dev_ns_label))\n",
    "print('ratio:',train_ns_label[0] / train_ns_label[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: imbalanced data set\n",
    "# 1.resampling\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.10953785644051"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "244637 / 5085"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9830891951850458"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "245499 / (245499+4223)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99230319563523"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "30555 / (30555+237)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249722"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ns_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<cls> not no scientific evidence co2 pollutant higher co2 concentration actually help ecosystem support plant animal life<sep> high concentration time atmospheric concentration greater carbon dioxide toxic animal life raise concentration ppm higher several hour eliminate pest whitefly spider mite greenhouse'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ns_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ns_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30800"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_ns_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30600"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ns_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38547\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(train_ns_data) \n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # padding\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenise the input into word sequences\n",
    "\n",
    "train_seq = tokenizer.texts_to_sequences(train_ns_data)\n",
    "dev_seq = tokenizer.texts_to_sequences(dev_ns_data)\n",
    "test_seq = tokenizer.texts_to_sequences(test_ns_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249722"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207\n",
      "168\n",
      "maxlen: 207\n"
     ]
    }
   ],
   "source": [
    "# padding matrix to the same length\n",
    "max_i = 0\n",
    "for i in train_seq:\n",
    "    max_i = max(max_i, len(i))\n",
    "max_t = max_i\n",
    "print(max_t)\n",
    "\n",
    "max_i = 0\n",
    "for i in dev_seq:\n",
    "    max_i = max(max_i, len(i))\n",
    "max_v = max_i\n",
    "print(max_v)\n",
    "\n",
    "maxlen = max_t if max_v <= max_t else max_v\n",
    "print('maxlen:',maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "train_seq = pad_sequences(train_seq, padding='post', maxlen=maxlen)\n",
    "dev_seq = pad_sequences(dev_seq, padding='post', maxlen=maxlen)\n",
    "test_seq = pad_sequences(test_seq, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FA2ao2l8hOg"
   },
   "source": [
    "# 2. Model Implementation\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "QIEqDDT78q39",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM_G6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 207, 60)           2312820   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 207, 60)           0         \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 207, 200)          128800    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 200)               240800    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                10050     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2692521 (10.27 MB)\n",
      "Trainable params: 2692521 (10.27 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import LSTM\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "# TODO: fine-tunning\n",
    "embedding_dim = 60\n",
    "hidden_dim = 100\n",
    "\n",
    "# model\n",
    "model = Sequential(name=\"LSTM_G6\")\n",
    "\n",
    "# embedding layer\n",
    "model.add(layers.Embedding(\n",
    "    input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen))\n",
    "\n",
    "##\n",
    "## experiments here to verify what technology is effective\n",
    "## for reports\n",
    "##\n",
    "\n",
    "# one direction\n",
    "# model.add(layers.Dropout(0.1)) # increase robustness using dropout\n",
    "# model.add(LSTM(hidden_dim, return_sequences=True, dropout=0.1)) # double layer\n",
    "# model.add(LSTM(hidden_dim, dropout=0.1))                        # single layer - baseline\n",
    "\n",
    "# bidirectional\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Bidirectional(LSTM(hidden_dim, return_sequences=True, dropout=0.1)))\n",
    "model.add(layers.Bidirectional(LSTM(hidden_dim, dropout=0.1)))\n",
    "\n",
    "# output layer\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(hidden_dim // 2, activation='tanh'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "## finish the model construction\n",
    "\n",
    "## Learning Rates: keras.cosineDecay\n",
    "## TODO\n",
    "decay_steps = 3000\n",
    "learning_rate = 1e-2\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    learning_rate, decay_steps\n",
    ")\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# binary cross entropy loss for binary classification problem\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras.metrics.Recall()])\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249722"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249722"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ns_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "cpus = tf.config.list_physical_devices('CPU')\n",
    "\n",
    "print(gpus,cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "112/250 [============>.................] - ETA: 38:08 - loss: 2.0038"
     ]
    }
   ],
   "source": [
    "# trianning\n",
    "model.fit(train_seq, train_ns_label, \n",
    "          epochs=3, verbose=True, \n",
    "          validation_data=(dev_seq, dev_ns_label), \n",
    "          batch_size=1000, class_weight={0: 1, 1: 120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzGuzHPE87Ya"
   },
   "source": [
    "# 3.Testing and Evaluation\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mefSOe8eTmGP"
   },
   "source": [
    "## Object Oriented Programming codes here\n",
    "\n",
    "*You can use multiple code snippets. Just add more if needed*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
