{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32yCsRUo8H33"
   },
   "source": [
    "# 2024 COMP90042 Project\n",
    "*Make sure you change the file name with your group id.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCybYoGz8YWQ"
   },
   "source": [
    "# Readme\n",
    "*If there is something to be noted for the marker, please mention here.*\n",
    "\n",
    "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We use keras, tensorflow, nltk, scikit-learn in this project.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6po98qVA8bJD"
   },
   "source": [
    "# 1.DataSet Processing\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvff21Hv8zjk",
    "tags": []
   },
   "source": [
    "## PreProcess for evidence and claims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "dev_cls_data = json.load(open(\"dev_cls_data.json\", \"r\"))\n",
    "test_cls_data = json.load(open(\"test_cls_data.json\", \"r\"))\n",
    "\n",
    "train_claims = json.load(open(\"data/train-claims.json\", \"r\"))\n",
    "train_texts = json.load(open(\"temp_data/train_texts.json\", \"r\"))\n",
    "train_ids = json.load(open(\"temp_data/train_ids.json\", \"r\"))\n",
    "train_evidences = json.load(open(\"temp_data/train_evidences.json\", \"r\"))\n",
    "\n",
    "test_claims = json.load(open(\"data/test-claims-unlabelled.json\", \"r\"))\n",
    "evidences = json.load(open(\"data/evidence.json\", \"r\"))\n",
    "evidences_ids = json.load(open(\"temp_data/evidences_ids.json\", \"r\"))\n",
    "evidences_id_dict = json.load(open(\"temp_data/evidences_id_dict.json\", \"r\"))\n",
    "evidences_texts = json.load(open(\"temp_data/evidences_texts.json\", \"r\"))\n",
    "\n",
    "\n",
    "train_ids_dict = {}\n",
    "for i, j in enumerate(train_ids):\n",
    "    train_ids_dict[j] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combine claim-text with evidence-text\n",
    "import random\n",
    "select_evidence_k = 6 # the same as retrieval\n",
    "\n",
    "train_cls_data = []\n",
    "\n",
    "for train_id in train_ids:\n",
    "    cur_data = {\"label\": train_claims[train_id][\"claim_label\"]}\n",
    "    select_evidence_ids = train_evidences[train_ids_dict[train_id]]\n",
    "    while len(select_evidence_ids) < select_evidence_k:\n",
    "        evidence_id = random.choice(range(len(evidences_texts)))\n",
    "        while evidence_id in select_evidence_ids:\n",
    "            evidence_id = random.choice(range(len(evidences_texts)))\n",
    "        select_evidence_ids.append(evidence_id)\n",
    "        \n",
    "    cur_data['text'] = \"<sep>\".join([\"<cls>\" + train_texts[train_ids_dict[train_id]]] + [evidences_texts[i] for i in select_evidence_ids])\n",
    "    train_cls_data.append(cur_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct training data\n",
    "\n",
    "def prepare_data(cls_data):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    for i in cls_data:\n",
    "        x_data.append(i['text'])\n",
    "        if 'label' in i:\n",
    "            y_data.append(i['label'])\n",
    "    return x_data, y_data\n",
    "\n",
    "train_x, train_y = prepare_data(train_cls_data)\n",
    "dev_x, dev_y = prepare_data(dev_cls_data)\n",
    "test_x, _ = prepare_data(test_cls_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'SUPPORTS': 519, 'NOT_ENOUGH_INFO': 386, 'REFUTES': 199, 'DISPUTED': 124})\n",
      "Counter({'SUPPORTS': 68, 'NOT_ENOUGH_INFO': 41, 'REFUTES': 27, 'DISPUTED': 18})\n"
     ]
    }
   ],
   "source": [
    "# data inspection\n",
    "\n",
    "from collections import Counter\n",
    "print(Counter(train_y))\n",
    "print(Counter(dev_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 519, 3: 386, 1: 199, 0: 124})\n",
      "Counter({2: 68, 3: 41, 1: 27, 0: 18})\n"
     ]
    }
   ],
   "source": [
    "# tranform class string to class numbers\n",
    "\n",
    "import numpy as np\n",
    "label_dict = {}\n",
    "new_train_y = []\n",
    "j = 0\n",
    "for i in train_y:\n",
    "    if i not in label_dict:\n",
    "        label_dict[i] = j\n",
    "        new_train_y.append(j)\n",
    "        j += 1\n",
    "    else:\n",
    "        new_train_y.append(label_dict[i])\n",
    "print(Counter(new_train_y))\n",
    "print(Counter([label_dict[i] for i in dev_y]))\n",
    "\n",
    "idx2label_ = sorted(label_dict.items(), key=lambda x:x[1])\n",
    "idx2label = [i for i, _ in idx2label_]\n",
    "\n",
    "train_y = []\n",
    "for i in new_train_y:\n",
    "    temp = [0] * len(idx2label)\n",
    "    temp[i] = 1\n",
    "    train_y.append(temp)\n",
    "\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "dev_y_new = []\n",
    "for i in dev_y:\n",
    "    temp = [0] * len(idx2label)\n",
    "    temp[label_dict[i]] = 1\n",
    "    dev_y_new.append(temp)\n",
    "    \n",
    "dev_y = np.array(dev_y_new)\n",
    "\n",
    "# dev_y = np.array([label_dict[i] for i in dev_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 18:56:11.302796: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17970\n"
     ]
    }
   ],
   "source": [
    "# same in evidence retreival \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # 0 is padding token\n",
    "print(vocab_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same in evidence retreival \n",
    "# tokenise the input into word sequences\n",
    "\n",
    "xseq_train = tokenizer.texts_to_sequences(train_x)\n",
    "xseq_dev = tokenizer.texts_to_sequences(dev_x)\n",
    "xseq_test = tokenizer.texts_to_sequences(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "488\n",
      "146\n"
     ]
    }
   ],
   "source": [
    "# align dimension of data\n",
    "\n",
    "max_i = 0\n",
    "for i in xseq_train:\n",
    "    max_i = max(max_i, len(i))\n",
    "print(max_i)\n",
    "\n",
    "max_i = 0\n",
    "for i in xseq_dev:\n",
    "    max_i = max(max_i, len(i))\n",
    "print(max_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same in evidence retreival \n",
    "# transform texts to sequences\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 350\n",
    "xseq_train = pad_sequences(xseq_train, padding='post', maxlen=maxlen)\n",
    "xseq_dev = pad_sequences(xseq_dev, padding='post', maxlen=maxlen)\n",
    "xseq_test = pad_sequences(xseq_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FA2ao2l8hOg"
   },
   "source": [
    "# 2. Model Implementation\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM_G6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 350, 60)           1078200   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 350, 60)           0         \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, 350, 200)         128800    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirectio  (None, 200)              240800    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 50)                10050     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,458,054\n",
      "Trainable params: 1,458,054\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "# TODO: fine-tunning\n",
    "embedding_dim = 60\n",
    "hidden_dim = 100\n",
    "\n",
    "# model\n",
    "model = Sequential(name=\"LSTM_G6\")\n",
    "\n",
    "# embedding layer\n",
    "model.add(layers.Embedding(\n",
    "    input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen))\n",
    "\n",
    "# bidirectional\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Bidirectional(LSTM(hidden_dim, return_sequences=True, dropout=0.1)))\n",
    "model.add(layers.Bidirectional(LSTM(hidden_dim, dropout=0.1)))\n",
    "\n",
    "# output layer\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(hidden_dim // 2, activation='tanh'))\n",
    "model.add(layers.Dense(4, activation='softmax'))\n",
    "\n",
    "#since it's a binary classification problem, we use a binary cross entropy loss here\n",
    "decay_steps = 35\n",
    "learning_rate = 1e-2\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    learning_rate, decay_steps\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "model.fit(xseq_train, train_y, epochs=10, verbose=True, validation_data=(xseq_dev, dev_y), batch_size=500, class_weight={0: 2, 1: 2, 2: 1, 3: 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzGuzHPE87Ya"
   },
   "source": [
    "# 3.Testing and Evaluation\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6ZVeNYIH9IaL"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dev_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(xseq_dev, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m test_logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(xseq_test, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "dev_logits = model.predict(xseq_dev, batch_size=200, verbose=True)\n",
    "test_logits = model.predict(xseq_test, batch_size=200, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dev_logits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dev_classes \u001b[38;5;241m=\u001b[39m \u001b[43mdev_logits\u001b[49m\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m test_classes \u001b[38;5;241m=\u001b[39m test_logits\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dev_logits' is not defined"
     ]
    }
   ],
   "source": [
    "dev_classes = dev_logits.argmax(axis=-1)\n",
    "test_classes = test_logits.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Counter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mCounter\u001b[49m(dev_classes))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Counter' is not defined"
     ]
    }
   ],
   "source": [
    "print(Counter(dev_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pred_dev_claims \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_dev_claims_retrieval.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      2\u001b[0m pred_test_claims \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_test_claims_retrieval.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(dev_cls_data, dev_classes):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "pred_dev_claims = json.load(open(\"pred_dev_claims_retrieval.json\", \"r\"))\n",
    "pred_test_claims = json.load(open(\"pred_test_claims_retrieval.json\", \"r\"))\n",
    "\n",
    "for i, j in zip(dev_cls_data, dev_classes):\n",
    "    pred_dev_claims[i['claim_id']]['claim_label'] = idx2label[j]\n",
    "\n",
    "for i, j in zip(test_cls_data, test_classes):\n",
    "    pred_test_claims[i['claim_id']]['claim_label'] = idx2label[j]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## save cls data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mdump(pred_dev_claims, \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_dev_claims.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      3\u001b[0m json\u001b[38;5;241m.\u001b[39mdump(pred_test_claims, \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_test_claims.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "## save cls data\n",
    "json.dump(pred_dev_claims, open(\"pred_dev_claims.json\", \"w\"))\n",
    "json.dump(pred_test_claims, open(\"pred_test_claims.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mefSOe8eTmGP",
    "tags": []
   },
   "source": [
    "## Object Oriented Programming codes here\n",
    "\n",
    "*You can use multiple code snippets. Just add more if needed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python eval.py --predictions pred_dev_claims.json --groundtruth data/dev-claims.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
