{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "## package imports\n",
        "\n",
        "#!pip install pandas scikit-learn torch torchtext\n",
        "\n",
        "## deep-learning libraries\n",
        "import tensorflow\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import keras\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "## NLP preprocessing libraries\n",
        "import nltk\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "##others\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "import string\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [],
      "source": [
        "## reading in json files\n",
        "\n",
        "\"\"\"\n",
        "Description of json files\n",
        "* [train-claims,dev-claims].json: JSON files for the labelled training and development set; \n",
        "* evidence.json: JSON file containing a large number of evidence passages (i.e. the “knowledge source”); \n",
        "* dev-claims-baseline.json: JSON file containing predictions of a baseline system on the development set;\n",
        "\"\"\"\n",
        "\n",
        "## relative file paths\n",
        "\n",
        "## baseline system - will not be used for any training/evaluation\n",
        "devClaimsBaselineFile='./data/dev-claims-baseline.json'\n",
        "## use this for model training\n",
        "trainClaimsFile='./data/train-claims.json'\n",
        "## use this set for hyperparameter tuning and evaluation metric \n",
        "devClaimsFile='./data/dev-claims.json'\n",
        "## evidence files need to be downloaded through https://drive.google.com/file/d/1JlUzRufknsHzKzvrEjgw8D3n_IRpjzo6/view?usp=sharing as it is to big to be uploaded to github\n",
        "evidenceFile='./data/evidence.json'\n",
        "\n",
        "## import as pandas dataframe\n",
        "devClaimsBaseline=pd.read_json(devClaimsBaselineFile)\n",
        "trainClaims=pd.read_json(trainClaimsFile)\n",
        "devClaims=pd.read_json(devClaimsFile)\n",
        "evidence=pd.read_json(evidenceFile,orient='index')\n",
        "\n",
        "## Separate claim_text,claim_label, and evidences from training and development sets, saved as pd dataframes\n",
        "claimTextTrain=trainClaims.loc['claim_text'].to_frame()\n",
        "claimLabelTrain=trainClaims.loc['claim_label'].to_frame()\n",
        "evidenceTrain=trainClaims.loc['evidences'].to_frame()\n",
        "\n",
        "claimTextDev=devClaims.loc['claim_text'].to_frame()\n",
        "claimLabelDev=devClaims.loc['claim_label'].to_frame()\n",
        "evidenceDev=devClaims.loc['evidences'].to_frame()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Obtain the evidence texts and add it as a new column at the evidence pd dataframes\n",
        "## This is in place transformation, but should always overwrite the evidence_text column so does not matter\n",
        "\n",
        "def getTextEvidence(evidenceList):\n",
        "    texts=[]\n",
        "    for evidenceID in evidenceList:\n",
        "        text=evidence.loc[evidenceID]\n",
        "        texts.append(text)\n",
        "    return texts\n",
        "\n",
        "## keeps the original df to visually check that evidences are correct\n",
        "evidenceDev['evidence_text']=evidenceDev['evidences'].apply(getTextEvidence)\n",
        "evidenceTrain['evidence_text']=evidenceTrain['evidences'].apply(getTextEvidence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Preprocessing data -- lowercase, tokenize, and stopword removal\n",
        "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
        "punctuations=string.punctuation\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens=nltk.word_tokenize(text.lower())\n",
        "    cleanedTokens=[t for t in tokens if (t not in stopwords) and (t not in punctuations)]\n",
        "    return ' '.join(cleanedTokens)\n",
        "\n",
        "claimTextDev['claim_text']=claimTextDev['claim_text'].apply(preprocess)\n",
        "claimTextTrain['claim_text']=claimTextTrain['claim_text'].apply(preprocess)\n",
        "\n",
        "##used to preprocess evidence, as structure is a bit different for evidence and claim dataframes\n",
        "for sentence,i in zip(evidenceDev['evidence_text'],range(len(evidenceDev['evidence_text']))):\n",
        "    for n in range(len(sentence)):\n",
        "        evidenceDev['evidence_text'][i][n]=evidenceDev['evidence_text'][i][n].apply(preprocess)\n",
        "        \n",
        "for sentence,i in zip(evidenceTrain['evidence_text'],range(len(evidenceTrain['evidence_text']))):\n",
        "    for n in range(len(sentence)):\n",
        "        evidenceTrain['evidence_text'][i][n]=evidenceTrain['evidence_text'][i][n].apply(preprocess)\n",
        "\n",
        "\n",
        "\n",
        "## evidence only pd dataframes\n",
        "evidenceDevText=evidenceDev['evidence_text']\n",
        "evidenceTrainText=evidenceTrain['evidence_text']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "## use TF-IDF word embedding\n",
        "\n",
        "tfidfVector=TfidfVectorizer()\n",
        "tfidfVector.fit(claimTextTrain['claim_text'])\n",
        "\n",
        "tfidfTrainSet=tfidfVector.transform(claimTextTrain['claim_text'])\n",
        "\n",
        "tfidfDevSet=tfidfVector.transform(claimTextDev['claim_text'])\n",
        "\n",
        "features=tfidfVector.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QIEqDDT78q39"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\julev\\anaconda3\\envs\\Python-3-8\\lib\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.012987012987012988\n"
          ]
        }
      ],
      "source": [
        "## Simple baseline logistic regression\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classifier=LogisticRegression()\n",
        "classifier.fit(tfidfTrainSet,claimTextTrain)\n",
        "score=classifier.score(tfidfDevSet,claimTextDev)\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
