{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 332,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext==0.4.0 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (0.4.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (4.66.2)\n",
            "Requirement already satisfied: requests in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (2.31.0)\n",
            "Requirement already satisfied: torch in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (2.3.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (1.24.3)\n",
            "Requirement already satisfied: six in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from requests->torchtext==0.4.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from requests->torchtext==0.4.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from requests->torchtext==0.4.0) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from requests->torchtext==0.4.0) (2024.2.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (2024.3.1)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (2021.4.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from tqdm->torchtext==0.4.0) (0.4.6)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->torchtext==0.4.0) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->torchtext==0.4.0) (2021.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from jinja2->torch->torchtext==0.4.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from sympy->torch->torchtext==0.4.0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "## package imports\n",
        "\n",
        "#!pip install pandas scikit-learn torch torchtext\n",
        "!pip3 install torchtext==0.4.0\n",
        "\n",
        "## deep-learning libraries\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import keras\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.data import Field, LabelField, Example, Dataset\n",
        "from tensorflow import convert_to_tensor\n",
        "\n",
        "## NLP preprocessing libraries\n",
        "import nltk\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "##others\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "import string\n",
        "import json\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 333,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [],
      "source": [
        "## reading in json files\n",
        "\n",
        "\"\"\"\n",
        "Description of json files\n",
        "* [train-claims,dev-claims].json: JSON files for the labelled training and development set; \n",
        "* evidence.json: JSON file containing a large number of evidence passages (i.e. the “knowledge source”); \n",
        "* dev-claims-baseline.json: JSON file containing predictions of a baseline system on the development set;\n",
        "\"\"\"\n",
        "\n",
        "## relative file paths\n",
        "\n",
        "## baseline system - will not be used for any training/evaluation\n",
        "devClaimsBaselineFile='./data/dev-claims-baseline.json'\n",
        "## use this for model training\n",
        "trainClaimsFile='./data/train-claims.json'\n",
        "## use this set for hyperparameter tuning and evaluation metric \n",
        "devClaimsFile='./data/dev-claims.json'\n",
        "## evidence files need to be downloaded through https://drive.google.com/file/d/1JlUzRufknsHzKzvrEjgw8D3n_IRpjzo6/view?usp=sharing as it is to big to be uploaded to github\n",
        "evidenceFile='./data/evidence.json'\n",
        "## test unlabelled dataset\n",
        "testFile='./data/test-claims-unlabelled.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 334,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the JSON data\n",
        "with open(trainClaimsFile, 'r') as file:\n",
        "    trainClaims=json.load(file)\n",
        "with open(devClaimsFile, 'r') as file:\n",
        "    devClaims=json.load(file)\n",
        "with open(evidenceFile, 'r') as file:\n",
        "    evidenceData=json.load(file)\n",
        "\n",
        "## Preprocessing data -- lowercase, tokenize, and stopword removal\n",
        "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
        "tokenizer=get_tokenizer('basic_english')\n",
        "punctuations=string.punctuation\n",
        "\n",
        "def preprocess(text):\n",
        "    token=tokenizer(text.lower())\n",
        "    cleanedTokens=[t for t in token if (t not in stopwords) and (t not in punctuations)]\n",
        "    return ' '.join(cleanedTokens)\n",
        "\n",
        "for ids, texts in evidenceData.items():\n",
        "    evidenceData[ids]=preprocess(texts)\n",
        "\n",
        "# Function to create DataFrame and merge evidence IDs with Text\n",
        "def createDF(claims, evidence):\n",
        "    combinedData=[]\n",
        "    for claimID, claimText in claims.items():\n",
        "        # Combine the ID with its corresponding evidences\n",
        "        evidenceID=claimText['evidences']\n",
        "        evidenceText=(evidence[i] for i in evidenceID if i in evidence)\n",
        "        combinedData.append({\n",
        "            'claim_id': claimID,\n",
        "            'claim_text': preprocess(claimText['claim_text']),\n",
        "            'evidence_id': evidenceID,\n",
        "            'evidence_text': \" \".join(evidenceText),\n",
        "            'claim_label': claimText['claim_label']\n",
        "        })\n",
        "    # Create DataFrame\n",
        "    return pd.DataFrame(combinedData)\n",
        "\n",
        "# Create CSV Files\n",
        "trainFullMerged=createDF(trainClaims,evidenceData)\n",
        "devFullMerged=createDF(devClaims,evidenceData)\n",
        "trainFullMerged.to_csv(\"data/trainFullMerged.csv\", index=False)\n",
        "devFullMerged.to_csv(\"data/devFullMerged.csv\", index=False)\n",
        "\n",
        "# Convert evidence into csv as well\n",
        "evidenceFinal=pd.DataFrame(list(evidenceData.items()),columns=['evidence_id','evidence_text'])\n",
        "evidenceFinal.to_csv('data/evidencePreprocessed.csv',index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 335,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert unlabelled Data into CSV as well\n",
        "with open(testFile, 'r') as file:\n",
        "    testData=json.load(file)\n",
        "for ids, texts in testData.items():\n",
        "    claim_text = texts['claim_text']\n",
        "    testData[ids]=preprocess(claim_text)\n",
        "testFinal=pd.DataFrame(list(testData.items()), columns=['claim_id', 'claim_text'])\n",
        "testFinal.to_csv('data/testPreprocessed.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 338,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "If using all scalar values, you must pass an index",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[338], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m trainDataframe\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(trainClaimsFile)\n\u001b[0;32m     16\u001b[0m devDataframe\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(devClaimsFile)\n\u001b[1;32m---> 17\u001b[0m evidenceDataframe\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevidenceFile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m## One consideration is whether we want the evidence_id at all\u001b[39;00m\n\u001b[0;32m     21\u001b[0m trainDataframe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevidence_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mtrainDataframe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevidence_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[]\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\julev\\anaconda3\\envs\\Python-3-8\\lib\\site-packages\\pandas\\io\\json\\_json.py:784\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[0;32m    783\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\julev\\anaconda3\\envs\\Python-3-8\\lib\\site-packages\\pandas\\io\\json\\_json.py:975\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_lines(data_lines))\n\u001b[0;32m    974\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 975\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mconvert_dtypes(\n\u001b[0;32m    978\u001b[0m         infer_objects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype_backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype_backend\n\u001b[0;32m    979\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\julev\\anaconda3\\envs\\Python-3-8\\lib\\site-packages\\pandas\\io\\json\\_json.py:1001\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m    999\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1001\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1004\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
            "File \u001b[1;32mc:\\Users\\julev\\anaconda3\\envs\\Python-3-8\\lib\\site-packages\\pandas\\io\\json\\_json.py:1134\u001b[0m, in \u001b[0;36mParser.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 1134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1137\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\julev\\anaconda3\\envs\\Python-3-8\\lib\\site-packages\\pandas\\io\\json\\_json.py:1319\u001b[0m, in \u001b[0;36mFrameParser._parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1316\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[0;32m   1318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1319\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1323\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1324\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[0;32m   1325\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1326\u001b[0m     }\n",
            "File \u001b[1;32mc:\\Users\\julev\\anaconda3\\envs\\Python-3-8\\lib\\site-packages\\pandas\\core\\frame.py:709\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    703\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    704\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    705\u001b[0m     )\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 709\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
            "File \u001b[1;32mc:\\Users\\julev\\anaconda3\\envs\\Python-3-8\\lib\\site-packages\\pandas\\core\\internals\\construction.py:481\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    478\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    479\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\julev\\anaconda3\\envs\\Python-3-8\\lib\\site-packages\\pandas\\core\\internals\\construction.py:115\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 115\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
            "File \u001b[1;32mc:\\Users\\julev\\anaconda3\\envs\\Python-3-8\\lib\\site-packages\\pandas\\core\\internals\\construction.py:645\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n\u001b[1;32m--> 645\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf using all scalar values, you must pass an index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_series:\n\u001b[0;32m    648\u001b[0m     index \u001b[38;5;241m=\u001b[39m union_indexes(indexes)\n",
            "\u001b[1;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
          ]
        }
      ],
      "source": [
        "# Reading in the created CSV files\n",
        "\n",
        "## used this for setting how much to see when printing dataframes, 50 is default\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "## use this for model training\n",
        "trainClaimsFile='./data/trainFullMerged.csv'\n",
        "## use this set for hyperparameter tuning and evaluation metric \n",
        "devClaimsFile='./data/devFullMerged.csv'\n",
        "## evidence files need to be downloaded through googledrive (https://drive.google.com/file/d/1OyihwdAWfqHIOueCB4bLBkYg4hTN_OKm/view?usp=sharing)\n",
        "evidenceFile='./data/evidencePreprocessed.csv'\n",
        "## test unlabelled dataset\n",
        "testFile='./data/testPreprocessed.csv'\n",
        "\n",
        "trainDataframe=pd.read_csv(trainClaimsFile)\n",
        "devDataframe=pd.read_csv(devClaimsFile)\n",
        "evidenceDataframe=pd.read_csv(evidenceFile)\n",
        "\n",
        "## One consideration is whether we want the evidence_id at all\n",
        "\n",
        "trainDataframe['evidence_id']=trainDataframe['evidence_id'].astype(str).str.strip('[]').str.strip(\"'\")\n",
        "trainDataframe['combined_evidence']=trainDataframe['evidence_id']+\" \"+trainDataframe['evidence_text']\n",
        "devDataframe['evidence_id']=devDataframe['evidence_id'].astype(str).str.strip('[]').str.strip(\"'\")\n",
        "devDataframe['combined_evidence']=devDataframe['evidence_id'] +\" \"+ devDataframe['evidence_text']\n",
        "evidenceDataframe['combined_evidence']=evidenceDataframe['evidence_id']+\" \"+evidenceDataframe['evidence_text']\n",
        "evidenceDataframe['combined_evidence']=evidenceDataframe['combined_evidence'].astype(str).str.strip(\"'\")\n",
        "\n",
        "print('shape of training set:',trainDataframe.shape,trainDataframe.columns)\n",
        "print('shape of development set:',devDataframe.shape,devDataframe.columns)\n",
        "print('shape of evidence:',evidenceDataframe.shape,evidenceDataframe.columns)\n",
        "\n",
        "\n",
        "TEXT=torchtext.data.Field(tokenize=preprocess,\n",
        "                          init_token='<text1>',\n",
        "                          eos_token='<text2>',\n",
        "                          lower=True)\n",
        "LABEL = torchtext.data.LabelField(tokenize=preprocess,\n",
        "                          lower=True)\n",
        "\n",
        "## This converts dataframes into tensors, it does preprocessing twice as the data is already preprocessed but will leave it as is for now\n",
        "def createDatasetEncoderInput(dataframe,textTransform,labelTransform):\n",
        "    fields=[(('reviewTextInput'),textTransform),(('evidenceTextOutput'),labelTransform)]\n",
        "    examples=[]\n",
        "    for _, row in dataframe.iterrows():\n",
        "        reviewTextInput=row['claim_text']\n",
        "        evidenceTextOutput=row['combined_evidence']\n",
        "        examples.append(Example.fromlist([reviewTextInput,evidenceTextOutput], fields))\n",
        "    return Dataset(examples,fields)\n",
        "\n",
        "trainTensor=createDatasetEncoderInput(trainDataframe,TEXT,LABEL)\n",
        "devTensor=createDatasetEncoderInput(devDataframe,TEXT,LABEL)\n",
        "    \n",
        "## now, start to create the decoder inputs in the form of DATASETS for the combined_evidence\n",
        "def createDatasetDecoderInput(dataframe,labelTransform):\n",
        "    fields=[(('evidenceTextOutput'),labelTransform)]\n",
        "    examples=[]\n",
        "    for _, row in dataframe.iterrows():\n",
        "        evidenceTextOutput=row['combined_evidence']\n",
        "        examples.append(Example.fromlist([evidenceTextOutput], fields))\n",
        "    return Dataset(examples,fields)\n",
        "\n",
        "evidenceTensor=createDatasetDecoderInput(evidenceDataframe,LABEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 337,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train {'reviewTextInput': 'scientific evidence co2 pollutant higher co2 concentrations actually help ecosystems support plant animal life', 'evidenceTextOutput': \"evidence-442946', 'evidence-1194317', 'evidence-12171 high concentrations 100 times atmospheric concentration greater carbon dioxide toxic animal life raising concentration 10 000 ppm 1% higher several hours eliminate pests whiteflies spider mites greenhouse plants grow much 50 percent faster concentrations 1 000 ppm co 2 compared ambient conditions though assumes change climate limitation nutrients higher carbon dioxide concentrations favourably affect plant growth demand water\"}\n",
            "dev {'reviewTextInput': '[south australia] expensive electricity world', 'evidenceTextOutput': \"evidence-67732', 'evidence-572512 [citation needed] south australia highest retail price electricity country south australia highest power prices world\"}\n",
            "evidence {'evidenceTextOutput': 'evidence-0 john bennet lawes english entrepreneur agricultural scientist'}\n",
            "train {'reviewTextInput': 'el niño drove record highs global temperatures suggesting rise may man-made emissions', 'evidenceTextOutput': \"evidence-338219', 'evidence-1127398 ‘climate change’ due natural forces human activity substantial evidence indicate human activity – specifically increased greenhouse gas ghgs emissions – key factor pace extent global temperature increases acceleration due mostly human-caused global warming driving thermal expansion seawater melting land-based ice sheets glaciers\"}\n",
            "dev {'reviewTextInput': '3 per cent total annual global emissions carbon dioxide humans australia prod\\xaduces 1 3 per cent 3 per cent amount emissions reductio\\xadn effect global climate', 'evidenceTextOutput': \"evidence-996421', 'evidence-1080858', 'evidence-208053', 'evidence-699212', 'evidence-832334 2011 unep green economy report states [a]agricultural operations excluding land use changes produce approximately 13 per cent anthropogenic global ghg emissions market share 30% potentially clean electricity heat pumps could reduce global co 2 emissions 8% annually modern era emissions atmosphere volcanoes approximately 0 645 billion tonnes co 2 per year whereas humans contribute 29 billion tonnes co 2 year cumulative anthropogenic e human-emitted emissions co 2 fossil fuel use major cause global warming give indication countries contributed human-induced climate change countries fast growing emissions south korea iran australia apart oil rich persian gulf states highest percapita emission rate world\"}\n",
            "evidence {'evidenceTextOutput': 'evidence-1 lindberg began professional career age 16 eventually moving new york city 1977'}\n",
            "train {'reviewTextInput': '1946 pdo switched cool phase', 'evidenceTextOutput': \"evidence-530063', 'evidence-984887 evidence reversals prevailing polarity meaning changes cool surface waters versus warm surface waters within region oscillation occurring around 1925 1947 1977 last two reversals corresponded dramatic shifts salmon production regimes north pacific ocean 1945/1946 pdo changed cool phase pattern regime shift similar 1970s episode maximum amplitude subarctic subtropical front greater signature near japan 1970s shift stronger near american west coast\"}\n",
            "dev {'reviewTextInput': 'means world 1c warmer pre-industrial times', 'evidenceTextOutput': \"evidence-889933', 'evidence-694262 multiple independently produced instrumental datasets confirm 2009–2018 decade 0 93 ± 0 07 °c warmer pre-industrial baseline 1850–1900 planet 0 8 °c warmer pre-industrial times\"}\n",
            "evidence {'evidenceTextOutput': 'evidence-2 ``boston ladies cambridge vampire weekend'}\n"
          ]
        }
      ],
      "source": [
        "assert (len(trainDataframe)==len(trainTensor)) and (len(devDataframe)==len(devTensor) and (len(evidenceDataframe)==len(evidenceTensor)))\n",
        "\n",
        "# checking how the tensors look\n",
        "for i in range(min(len(trainTensor), 3)):\n",
        "    print('train',vars(trainTensor.examples[i]))\n",
        "    print('dev',vars(devTensor.examples[i]))\n",
        "    print('evidence',vars(evidenceTensor.examples[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
