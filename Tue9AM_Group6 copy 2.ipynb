{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext==0.4.0 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (0.4.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (4.66.2)\n",
            "Requirement already satisfied: requests in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (2.31.0)\n",
            "Requirement already satisfied: torch in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (2.3.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (1.24.3)\n",
            "Requirement already satisfied: six in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from requests->torchtext==0.4.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from requests->torchtext==0.4.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from requests->torchtext==0.4.0) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from requests->torchtext==0.4.0) (2024.2.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (2024.3.1)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (2021.4.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from tqdm->torchtext==0.4.0) (0.4.6)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->torchtext==0.4.0) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->torchtext==0.4.0) (2021.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from jinja2->torch->torchtext==0.4.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from sympy->torch->torchtext==0.4.0) (1.3.0)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip3 install torchtext==0.4.0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m## deep-learning libraries\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\julev\\anaconda3\\envs\\Python-3-8\\lib\\site-packages\\tensorflow\\__init__.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\julev\\anaconda3\\envs\\Python-3-8\\lib\\site-packages\\tensorflow\\python\\__init__.py:105\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resource_loader\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sysconfig \u001b[38;5;28;01mas\u001b[39;00m sysconfig_lib\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m test\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2_compat\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mall_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_all\n",
            "File \u001b[1;32mc:\\Users\\julev\\anaconda3\\envs\\Python-3-8\\lib\\site-packages\\tensorflow\\python\\platform\\test.py:20\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"Testing.\"\"\"\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-bad-import-order\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m test_util \u001b[38;5;28;01mas\u001b[39;00m _test_util\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m googletest \u001b[38;5;28;01mas\u001b[39;00m _googletest\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\julev\\anaconda3\\envs\\Python-3-8\\lib\\site-packages\\tensorflow\\python\\framework\\test_util.py:51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m def_function\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _test_metrics_util\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device \u001b[38;5;28;01mas\u001b[39;00m pydev\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "## package imports\n",
        "\n",
        "#!pip install pandas scikit-learn torch torchtext\n",
        "!pip3 install torchtext==0.4.0\n",
        "\n",
        "## deep-learning libraries\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import keras\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.data import Field, LabelField, Example, Dataset\n",
        "from tensorflow import convert_to_tensor\n",
        "\n",
        "## NLP preprocessing libraries\n",
        "import nltk\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "##others\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import json\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [],
      "source": [
        "## reading in json files\n",
        "\n",
        "\"\"\"\n",
        "Description of json files\n",
        "* [train-claims,dev-claims].json: JSON files for the labelled training and development set; \n",
        "* evidence.json: JSON file containing a large number of evidence passages (i.e. the “knowledge source”); \n",
        "* dev-claims-baseline.json: JSON file containing predictions of a baseline system on the development set;\n",
        "\"\"\"\n",
        "\n",
        "## relative file paths\n",
        "\n",
        "## baseline system - will not be used for any training/evaluation\n",
        "devClaimsBaselineFile='./data/dev-claims-baseline.json'\n",
        "## use this for model training\n",
        "trainClaimsFile='./data/train-claims.json'\n",
        "## use this set for hyperparameter tuning and evaluation metric \n",
        "devClaimsFile='./data/dev-claims.json'\n",
        "## evidence files need to be downloaded through https://drive.google.com/file/d/1JlUzRufknsHzKzvrEjgw8D3n_IRpjzo6/view?usp=sharing as it is to big to be uploaded to github\n",
        "evidenceFile='./data/evidence.json'\n",
        "## test unlabelled dataset\n",
        "testFile='./data/test-claims-unlabelled.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the JSON data\n",
        "with open(trainClaimsFile, 'r') as file:\n",
        "    trainClaims=json.load(file)\n",
        "with open(devClaimsFile, 'r') as file:\n",
        "    devClaims=json.load(file)\n",
        "with open(evidenceFile, 'r') as file:\n",
        "    evidenceData=json.load(file)\n",
        "\n",
        "## Preprocessing data -- lowercase, tokenize, and stopword removal\n",
        "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
        "tokenizer=get_tokenizer('basic_english')\n",
        "punctuations=string.punctuation\n",
        "\n",
        "def preprocess(text):\n",
        "    token=tokenizer(text.lower())\n",
        "    cleanedTokens=[t for t in token if (t not in stopwords) and (t not in punctuations)]\n",
        "    return ' '.join(cleanedTokens)\n",
        "\n",
        "for ids, texts in evidenceData.items():\n",
        "    evidenceData[ids]=preprocess(texts)\n",
        "\n",
        "# Function to create DataFrame and merge evidence IDs with Text\n",
        "def createDF(claims, evidence):\n",
        "    combinedData=[]\n",
        "    for claimID, claimText in claims.items():\n",
        "        # Combine the ID with its corresponding evidences\n",
        "        evidenceID=claimText['evidences']\n",
        "        evidenceText=(evidence[i] for i in evidenceID if i in evidence)\n",
        "        combinedData.append({\n",
        "            'claim_id': claimID,\n",
        "            'claim_text': preprocess(claimText['claim_text']),\n",
        "            'evidence_id': evidenceID,\n",
        "            'evidence_text': \" \".join(evidenceText),\n",
        "            'claim_label': claimText['claim_label']\n",
        "        })\n",
        "    # Create DataFrame\n",
        "    return pd.DataFrame(combinedData)\n",
        "\n",
        "# Create CSV Files\n",
        "trainFullMerged=createDF(trainClaims,evidenceData)\n",
        "devFullMerged=createDF(devClaims,evidenceData)\n",
        "trainFullMerged.to_csv(\"data/trainFullMerged.csv\", index=False)\n",
        "devFullMerged.to_csv(\"data/devFullMerged.csv\", index=False)\n",
        "\n",
        "# Convert evidence into csv as well\n",
        "evidenceFinal=pd.DataFrame(list(evidenceData.items()),columns=['evidence_id','evidence_text'])\n",
        "evidenceFinal.to_csv('data/evidencePreprocessed.csv',index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert unlabelled Data into CSV as well\n",
        "with open(testFile, 'r') as file:\n",
        "    testData=json.load(file)\n",
        "for ids, texts in testData.items():\n",
        "    claim_text = texts['claim_text']\n",
        "    testData[ids]=preprocess(claim_text)\n",
        "testFinal=pd.DataFrame(list(testData.items()), columns=['claim_id', 'claim_text'])\n",
        "testFinal.to_csv('data/testPreprocessed.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of training set: (1228, 6) Index(['claim_id', 'claim_text', 'evidence_id', 'evidence_text', 'claim_label',\n",
            "       'combined_evidence'],\n",
            "      dtype='object')\n",
            "shape of development set: (154, 6) Index(['claim_id', 'claim_text', 'evidence_id', 'evidence_text', 'claim_label',\n",
            "       'combined_evidence'],\n",
            "      dtype='object')\n",
            "shape of evidence: (1208827, 3) Index(['evidence_id', 'evidence_text', 'combined_evidence'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Reading in the created CSV files\n",
        "\n",
        "## used this for setting how much to see when printing dataframes, 50 is default\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "## use this for model training\n",
        "trainClaimsFile='./data/trainFullMerged.csv'\n",
        "## use this set for hyperparameter tuning and evaluation metric \n",
        "devClaimsFile='./data/devFullMerged.csv'\n",
        "## evidence files need to be downloaded through googledrive (https://drive.google.com/file/d/1OyihwdAWfqHIOueCB4bLBkYg4hTN_OKm/view?usp=sharing)\n",
        "evidenceFile='./data/evidencePreprocessed.csv'\n",
        "## test unlabelled dataset\n",
        "testFile='./data/testPreprocessed.csv'\n",
        "\n",
        "trainDataframe=pd.read_csv(trainClaimsFile)\n",
        "devDataframe=pd.read_csv(devClaimsFile)\n",
        "evidenceDataframe=pd.read_csv(evidenceFile)\n",
        "\n",
        "## One consideration is whether we want the evidence_id at all\n",
        "\n",
        "# trainDataframe['claim_text_unvectorized']=trainDataframe['claim_text']\n",
        "# trainDataframe['evidence_id']=trainDataframe['evidence_id'].astype(str).str.strip('[]').str.strip(\"'\").apply(preprocess)\n",
        "# trainDataframe['combined_evidence_unvectorized']=trainDataframe['evidence_id']+\" \"+trainDataframe['evidence_text']\n",
        "\n",
        "# devDataframe['claim_text_unvectorized']=devDataframe['claim_text']\n",
        "# devDataframe['evidence_id']=devDataframe['evidence_id'].astype(str).str.strip('[]').str.strip(\"'\").apply(preprocess)\n",
        "# devDataframe['combined_evidence_unvectorized']=devDataframe['evidence_id'] +\" \"+ devDataframe['evidence_text']\n",
        "\n",
        "# evidenceDataframe['combined_evidence_unvectorized']=evidenceDataframe['evidence_id']+\" \"+evidenceDataframe['evidence_text']\n",
        "# evidenceDataframe['combined_evidence_unvectorized']=evidenceDataframe['combined_evidence_unvectorized'].astype(str).str.strip(\"'\").apply(preprocess)\n",
        "\n",
        "trainDataframe['claim_text']=trainDataframe['claim_text']\n",
        "trainDataframe['evidence_id']=trainDataframe['evidence_id'].astype(str).str.strip('[]').str.strip(\"'\").apply(preprocess)\n",
        "trainDataframe['combined_evidence']=trainDataframe['evidence_id']+\" \"+trainDataframe['evidence_text']\n",
        "\n",
        "devDataframe['claim_text']=devDataframe['claim_text']\n",
        "devDataframe['evidence_id']=devDataframe['evidence_id'].astype(str).str.strip('[]').str.strip(\"'\").apply(preprocess)\n",
        "devDataframe['combined_evidence']=devDataframe['evidence_id'] +\" \"+ devDataframe['evidence_text']\n",
        "\n",
        "evidenceDataframe['combined_evidence']=evidenceDataframe['evidence_id']+\" \"+evidenceDataframe['evidence_text']\n",
        "evidenceDataframe['combined_evidence']=evidenceDataframe['combined_evidence'].astype(str).str.strip(\"'\").apply(preprocess)\n",
        "\n",
        "print('shape of training set:',trainDataframe.shape,trainDataframe.columns)\n",
        "print('shape of development set:',devDataframe.shape,devDataframe.columns)\n",
        "print('shape of evidence:',evidenceDataframe.shape,evidenceDataframe.columns)\n",
        "\n",
        "# textVector=TfidfVectorizer()\n",
        "# labelVector=TfidfVectorizer(max_features=1000)\n",
        "\n",
        "# textVector.fit(trainDataframe['claim_text_unvectorized'])\n",
        "# labelVector.fit(evidenceDataframe['combined_evidence_unvectorized'])\n",
        "\n",
        "# trainDataframe['combined_evidence']=list(labelVector.transform(trainDataframe['combined_evidence_unvectorized']))\n",
        "# trainDataframe['claim_text']=list(textVector.transform(trainDataframe['claim_text_unvectorized']))\n",
        "\n",
        "# devDataframe['combined_evidence']=list(labelVector.transform(devDataframe['combined_evidence_unvectorized']))\n",
        "# devDataframe['claim_text']=list(textVector.transform(devDataframe['claim_text_unvectorized']))\n",
        "\n",
        "# evidenceDataframe['combined_evidence']=list(labelVector.transform(evidenceDataframe['combined_evidence_unvectorized']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def convertTensorflow(features, labels=None):\n",
        "#     if labels is not None:\n",
        "#         dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "#     else:\n",
        "#         dataset = tf.data.Dataset.from_tensor_slices(features)\n",
        "#     return dataset\n",
        "\n",
        "# train_features = np.array([x.toarray() for x in trainDataframe['claim_text']])\n",
        "# train_labels = np.array([x.toarray() for x in trainDataframe['combined_evidence']])\n",
        "\n",
        "# trainTFDataset = convertTensorflow(train_features, train_labels)\n",
        "\n",
        "# dev_features = np.array([x.toarray() for x in devDataframe['claim_text']])\n",
        "# dev_labels = np.array([x.toarray() for x in devDataframe['combined_evidence']])\n",
        "\n",
        "# devTFDataset = convertTensorflow(dev_features, dev_labels)\n",
        "\n",
        "# evidence_features = np.array([x.toarray() for x in evidenceDataframe['combined_evidence']])\n",
        "\n",
        "# evidenceTFDataset = convertTensorflow(evidence_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(evidenceTFDataset)\n",
        "# for sample in evidenceTFDataset.take(3):  # Adjust the number taken based on dataset size\n",
        "#     print(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TEXT=torchtext.data.Field(tokenize=preprocess,\n",
        "                          init_token='<text1>',\n",
        "                          eos_token='<text2>',\n",
        "                          lower=True)\n",
        "LABEL = torchtext.data.LabelField(tokenize=preprocess,\n",
        "                          lower=True)\n",
        "\n",
        "## This converts dataframes into tensors, it does preprocessing twice as the data is already preprocessed but will leave it as is for now\n",
        "def createDatasetEncoderInput(dataframe,textTransform,labelTransform):\n",
        "    field=[(('reviewTextInput'),textTransform),(('evidenceTextOutput'),labelTransform)]\n",
        "    examples=[]\n",
        "    for _, row in dataframe.iterrows():\n",
        "        reviewTextInput=row['claim_text']\n",
        "        evidenceTextOutput=row['combined_evidence']\n",
        "        examples.append(Example.fromlist([reviewTextInput,evidenceTextOutput], field))\n",
        "    return Dataset(examples,fields=field)\n",
        "\n",
        "trainTensor=createDatasetEncoderInput(trainDataframe,TEXT,LABEL)\n",
        "devTensor=createDatasetEncoderInput(devDataframe,TEXT,LABEL)\n",
        "    \n",
        "## now, start to create the decoder inputs in the form of DATASETS for the combined_evidence\n",
        "def createDatasetDecoderInput(dataframe,labelTransform):\n",
        "    field=[(('evidenceTextOutput'),labelTransform)]\n",
        "    examples=[]\n",
        "    for _, row in dataframe.iterrows():\n",
        "        evidenceTextOutput=row['combined_evidence']\n",
        "        examples.append(Example.fromlist([evidenceTextOutput], field))\n",
        "    return Dataset(examples,fields=field)\n",
        "\n",
        "evidenceTensor=createDatasetDecoderInput(evidenceDataframe,LABEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train {'reviewTextInput': 'scientific evidence co2 pollutant higher co2 concentrations actually help ecosystems support plant animal life', 'evidenceTextOutput': 'evidence-442946 evidence-1194317 evidence-12171 high concentrations 100 times atmospheric concentration greater carbon dioxide toxic animal life raising concentration 10 000 ppm 1% higher several hours eliminate pests whiteflies spider mites greenhouse plants grow much 50 percent faster concentrations 1 000 ppm co 2 compared ambient conditions though assumes change climate limitation nutrients higher carbon dioxide concentrations favourably affect plant growth demand water'}\n",
            "dev {'reviewTextInput': '[south australia] expensive electricity world', 'evidenceTextOutput': 'evidence-67732 evidence-572512 [citation needed] south australia highest retail price electricity country south australia highest power prices world'}\n",
            "evidence {'evidenceTextOutput': 'evidence-0 john bennet lawes english entrepreneur agricultural scientist'}\n",
            "train {'reviewTextInput': 'el niño drove record highs global temperatures suggesting rise may man-made emissions', 'evidenceTextOutput': 'evidence-338219 evidence-1127398 ‘climate change’ due natural forces human activity substantial evidence indicate human activity – specifically increased greenhouse gas ghgs emissions – key factor pace extent global temperature increases acceleration due mostly human-caused global warming driving thermal expansion seawater melting land-based ice sheets glaciers'}\n",
            "dev {'reviewTextInput': '3 per cent total annual global emissions carbon dioxide humans australia prod\\xaduces 1 3 per cent 3 per cent amount emissions reductio\\xadn effect global climate', 'evidenceTextOutput': 'evidence-996421 evidence-1080858 evidence-208053 evidence-699212 evidence-832334 2011 unep green economy report states [a]agricultural operations excluding land use changes produce approximately 13 per cent anthropogenic global ghg emissions market share 30% potentially clean electricity heat pumps could reduce global co 2 emissions 8% annually modern era emissions atmosphere volcanoes approximately 0 645 billion tonnes co 2 per year whereas humans contribute 29 billion tonnes co 2 year cumulative anthropogenic e human-emitted emissions co 2 fossil fuel use major cause global warming give indication countries contributed human-induced climate change countries fast growing emissions south korea iran australia apart oil rich persian gulf states highest percapita emission rate world'}\n",
            "evidence {'evidenceTextOutput': 'evidence-1 lindberg began professional career age 16 eventually moving new york city 1977'}\n",
            "train {'reviewTextInput': '1946 pdo switched cool phase', 'evidenceTextOutput': 'evidence-530063 evidence-984887 evidence reversals prevailing polarity meaning changes cool surface waters versus warm surface waters within region oscillation occurring around 1925 1947 1977 last two reversals corresponded dramatic shifts salmon production regimes north pacific ocean 1945/1946 pdo changed cool phase pattern regime shift similar 1970s episode maximum amplitude subarctic subtropical front greater signature near japan 1970s shift stronger near american west coast'}\n",
            "dev {'reviewTextInput': 'means world 1c warmer pre-industrial times', 'evidenceTextOutput': 'evidence-889933 evidence-694262 multiple independently produced instrumental datasets confirm 2009–2018 decade 0 93 ± 0 07 °c warmer pre-industrial baseline 1850–1900 planet 0 8 °c warmer pre-industrial times'}\n",
            "evidence {'evidenceTextOutput': 'evidence-2 ``boston ladies cambridge vampire weekend'}\n"
          ]
        }
      ],
      "source": [
        "assert (len(trainDataframe)==len(trainTensor)) and (len(devDataframe)==len(devTensor) and (len(evidenceDataframe)==len(evidenceTensor)))\n",
        "\n",
        "# checking how the tensors look\n",
        "for i in range(min(len(trainTensor), 3)):\n",
        "    print('train',vars(trainTensor.examples[i]))\n",
        "    print('dev',vars(devTensor.examples[i]))\n",
        "    print('evidence',vars(evidenceTensor.examples[i]))\n",
        "\n",
        "## this used the pytorch pipeline, change it to tensorflow pipelines now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## convert into tensorflow tensors\n",
        "\n",
        "def extractTorchtext(dataset,fields,type):\n",
        "    texts=[]\n",
        "    labels=[]\n",
        "    hasLabel=True\n",
        "    if type==\"output\":\n",
        "        hasLabel=False\n",
        "    for examples in dataset:\n",
        "        text=getattr(examples,fields[0][0])\n",
        "        texts.append(text)\n",
        "        if hasLabel:\n",
        "            label=getattr(examples,fields[1][0])\n",
        "            labels.append(label)\n",
        "    return texts,labels\n",
        "\n",
        "trainTensorText,trainTensorLabel=extractTorchtext(trainTensor,[('reviewTextInput',None),('evidenceTextOutput',None)],'input')\n",
        "devTensorText,devTensorLabel=extractTorchtext(devTensor,[('reviewTextInput',None),('evidenceTextOutput',None)],'input')\n",
        "evidenceTexts,_=extractTorchtext(evidenceTensor,[('evidenceTextOutput',None)],'output')\n",
        "\n",
        "def convertToTensorflow(texts,labels=None):\n",
        "    if labels is not None:\n",
        "        dataset=tf.data.Dataset.from_tensor_slices((texts, labels))\n",
        "    else:\n",
        "        dataset=tf.data.Dataset.from_tensor_slices(texts)\n",
        "    return dataset\n",
        "\n",
        "trainTFDataset=convertToTensorflow(trainTensorText,trainTensorLabel)\n",
        "devTFDataset=convertToTensorflow(devTensorText,devTensorLabel)\n",
        "evidenceTFDataset=convertToTensorflow(evidenceTexts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## THIS CODE BLOCK IS REDUNDANT, IT WAS USED TO PERFORM COUNT EMBEDDINGS BUT MAYBE WILL IMPLEMENT LATER\n",
        "\n",
        "# print('Train dataset element spec:', trainTFDataset.element_spec)\n",
        "# print('Dev dataset element spec:', devTFDataset.element_spec)\n",
        "# print('Evidence dataset element spec:', evidenceTFDataset.element_spec)\n",
        "\n",
        "# from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "# textVector = TextVectorization(\n",
        "#     output_mode='int')\n",
        "\n",
        "# textData=trainTFDataset.map(lambda x, y: x)  # Extract just the text part from the dataset\n",
        "\n",
        "# # Adapt the vectorization layer to the text data\n",
        "# textVector.adapt(trainTFDataset.map(lambda x, y:x))\n",
        "\n",
        "# labelVector=TextVectorization(\n",
        "#     output_mode='int'\n",
        "# )\n",
        "\n",
        "# labelVector.adapt(evidenceTFDataset)\n",
        "\n",
        "# def vectorizeTexts(text,label):\n",
        "#     text=textVector(text)\n",
        "#     label=labelVector(label)\n",
        "#     return text,label\n",
        "\n",
        "# def vectorizeEvidence(label):\n",
        "#     label=labelVector(label)\n",
        "#     return label\n",
        "\n",
        "# vectorizedTrain=trainTFDataset.map(vectorizeTexts)\n",
        "# vectorizedDev=devTFDataset.map(vectorizeTexts)\n",
        "# vectorizedEvidence=evidenceTFDataset.map(vectorizeEvidence)\n",
        "\n",
        "# print(vectorizedEvidence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## THIS CODE BLOCK IS REDUNDANT, IT WAS USED TO PERFORM COUNT EMBEDDINGS BUT MAYBE WILL IMPLEMENT LATER\n",
        "\n",
        "# print(vectorizedTrain.element_spec)\n",
        "# for sample in vectorizedEvidence.take(3):\n",
        "#     print('Sample:', sample.numpy())\n",
        "# vocabText=textVector.get_vocabulary()\n",
        "# vocabLabel=labelVector.get_vocabulary()\n",
        "# print(vocabText)\n",
        "# print(vocabLabel)\n",
        "# print(vectorizedTrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## if we wanted to use word embeddings, process the TFDatasets first then run this\n",
        "## use these as tensors (these are EagerTensors)\n",
        "\n",
        "tensorTrain=tf.convert_to_tensor(list(trainTFDataset))\n",
        "tensorDev=tf.convert_to_tensor(list(devTFDataset))\n",
        "tensorEvidence=tf.convert_to_tensor(list(evidenceTFDataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# trainNumpyText=tensorTrain[:,0].numpy()\n",
        "# trainNumpyLabel=tensorTrain[:,1].numpy()\n",
        "# devNumpyText=tensorDev[:,0].numpy()\n",
        "# devNumpyLabel=tensorDev[:,1].numpy()\n",
        "# evidenceNumpy=tensorEvidence.numpy()\n",
        "\n",
        "# tfidfVectorText=TfidfVectorizer()\n",
        "# tfidfVectorLabel=TfidfVectorizer(max_features=300)\n",
        "\n",
        "# tfidfVectorText.fit(trainNumpyText)\n",
        "# tfidfVectorLabel.fit(evidenceNumpy)\n",
        "\n",
        "# trainNumpyText=tfidfVectorText.transform(trainNumpyText)\n",
        "# trainNumpyLabel=tfidfVectorLabel.transform(trainNumpyLabel)\n",
        "\n",
        "# devNumpyText=tfidfVectorText.transform(devNumpyText)\n",
        "# devNumpyLabel=tfidfVectorLabel.transform(devNumpyLabel)\n",
        "\n",
        "# evidenceNumpy=tfidfVectorLabel.transform(evidenceNumpy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #MemoryError: Unable to allocate 16.5 GiB for an array with shape (1228, 1807303) and data type int64\n",
        "\n",
        "# tensorTFIDFTrainText=tf.convert_to_tensor(trainNumpyText.toarray(),dtype=tf.float32)\n",
        "# tensorTFIDFTrainLabel=tf.convert_to_tensor(trainNumpyLabel.toarray(),dtype=tf.float32)\n",
        "\n",
        "# tensorTFIDFDevText=tf.convert_to_tensor(devNumpyText.toarray(),dtype=tf.float32)\n",
        "# tensorTFIDFDevLabel=tf.convert_to_tensor(devNumpyLabel.toarray(),dtype=tf.float32)\n",
        "\n",
        "# tensorTFIDFEvidence=tf.convert_to_tensor(evidenceNumpy.toarray(),dtype=tf.float32)\n",
        "\n",
        "# tensorTFIDFTrain=tf.concat([tensorTFIDFDevText,tensorTFIDFDevLabel],axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![alt text](image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## POSITIONAL ENCODINGS\n",
        "\n",
        "from keras.models import Model\n",
        "\n",
        "def positional_encoding(length, depth):\n",
        "    positions=np.arange(length)[:, np.newaxis]\n",
        "    depth_indices=np.arange(depth)[np.newaxis, :]\n",
        "    angle_rates = 1 / np.power(10000, (2 * (depth_indices // 2)) / np.float32(depth))\n",
        "    angle_rads = positions * angle_rates\n",
        "    # sine applied to even indices; cosine applied to odd indices\n",
        "    pos_encoding = np.concatenate([np.sin(angle_rads[:, 0::2]), np.cos(angle_rads[:, 1::2])], axis=-1)\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "  \n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        self.d_model=d_model\n",
        "        self.embedding=tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "        self.pos_encoding=positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "    def compute_mask(self, *args, **kwargs):\n",
        "        return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "    def call(self, x):\n",
        "        length = tf.shape(x)[1]\n",
        "        x=self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[tf.newaxis, :length, :]\n",
        "        return x\n",
        "  \n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "## I only use one encoder here, but we can try to use two encoders, one for label and one for the review text\n",
        "\n",
        "tensorTrainText=tensorTrain[:,1]\n",
        "tensorTrainLabels=tensorTrain[:,0]\n",
        "\n",
        "vectorizer=TextVectorization(output_mode='int',output_sequence_length=512)\n",
        "\n",
        "\n",
        "## CAN CHANGE THIS ADAPT TO RUN ON THE EVIDENCE INSTEAD, MIGHT BE BETTER BUT IT SLOWS DOWN MY PC A LOT SO WILL LEAVE AS LABELS ONLY FOR NOW\n",
        "vectorizer.adapt(tensorTrainLabels)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedText=PositionalEmbedding(vocab_size=(len(vectorizer.get_vocabulary())),d_model=512)\n",
        "embedLabel=PositionalEmbedding(vocab_size=(len(vectorizer.get_vocabulary())),d_model=512)\n",
        "\n",
        "vectorizedTensorTrainText=vectorizer(tensorTrainText)\n",
        "vectorizedTensorTrainLabels=vectorizer(tensorTrainLabels)\n",
        "vectorizedEvidence=vectorizer(tensorEvidence)\n",
        "\n",
        "\n",
        "inputEmbedded=embedText(vectorizedTensorTrainText)\n",
        "outputEmbedded=embedText(vectorizedTensorTrainLabels)\n",
        "\n",
        "## CANNOT EMBED THIS, CAUSES OUT OF MEMORY ERROR\n",
        "#outputEmbedded=embedLabel(vectorizedEvidence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1228, 512), dtype=bool, numpy=\n",
              "array([[ True,  True,  True, ..., False, False, False],\n",
              "       [ True,  True,  True, ..., False, False, False],\n",
              "       [ True,  True,  True, ..., False, False, False],\n",
              "       ...,\n",
              "       [ True,  True,  True, ..., False, False, False],\n",
              "       [ True,  True,  True, ..., False, False, False],\n",
              "       [ True,  True,  True, ..., False, False, False]])>"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Check shape\n",
        "inputEmbedded._keras_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Base attention layers\n",
        "\n",
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        self.mha=tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "        self.layernorm=tf.keras.layers.LayerNormalization()\n",
        "        self.add=tf.keras.layers.Add()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1228, 512, 512)\n",
            "(1228, 512, 512)\n",
            "(1228, 512, 512)\n"
          ]
        }
      ],
      "source": [
        "## cross attention layer\n",
        "\n",
        "from tensorflow.keras.layers import Layer, MultiHeadAttention, LayerNormalization, Add\n",
        "\n",
        "class CrossAttention(Layer):\n",
        "    def __init__(self, num_heads, key_dim, **kwargs):\n",
        "        super(CrossAttention, self).__init__(**kwargs)\n",
        "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
        "        self.layernorm = LayerNormalization(epsilon=1e-6)\n",
        "        self.add = Add()\n",
        "\n",
        "    def call(self, query, context):\n",
        "        attn_output, attn_scores = self.mha(query=query, value=context, key=context, return_attention_scores=True)\n",
        "        self.last_attn_scores = attn_scores\n",
        "        x = self.add([query, attn_output])\n",
        "        x = self.layernorm(x)\n",
        "        return x\n",
        "\n",
        "## key_dim CAN BE INCREASED BUT IT SLOWS DOWN PC\n",
        "sample_ca=CrossAttention(num_heads=2, key_dim=5)\n",
        "\n",
        "result=sample_ca(outputEmbedded, inputEmbedded)\n",
        "\n",
        "print(inputEmbedded.shape)\n",
        "print(outputEmbedded.shape)\n",
        "print(result.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1228, 512, 512)\n",
            "(1228, 512, 512)\n"
          ]
        }
      ],
      "source": [
        "## global self-attention layer\n",
        "\n",
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "\n",
        "sample_gsa = GlobalSelfAttention(num_heads=2, key_dim=5)\n",
        "\n",
        "print(inputEmbedded.shape)\n",
        "print(sample_gsa(inputEmbedded).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1228, 512, 512)\n",
            "(1228, 512, 512)\n"
          ]
        }
      ],
      "source": [
        "## Causal self attention layer\n",
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "\n",
        "sample_csa = CausalSelfAttention(num_heads=2, key_dim=5)\n",
        "\n",
        "print(inputEmbedded.shape)\n",
        "print(sample_csa(inputEmbedded).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max difference: 4.7683716e-07\n"
          ]
        }
      ],
      "source": [
        "out1 = sample_csa(inputEmbedded[:, :3])\n",
        "out2 = sample_csa(inputEmbedded)[:, :3]\n",
        "\n",
        "diff = tf.reduce_max(abs(out1 - out2))\n",
        "print(\"Max difference:\", diff.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1228, 512, 512)\n",
            "(1228, 512, 512)\n"
          ]
        }
      ],
      "source": [
        "## Feedforward layer\n",
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x) \n",
        "    return x\n",
        "\n",
        "## Edit these numbers -- the shape needs to be the same for the ffn numbers\n",
        "sample_ffn = FeedForward(512, 512)\n",
        "\n",
        "print(inputEmbedded.shape)\n",
        "print(sample_ffn(inputEmbedded).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tf' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Encoder Layer\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mEncoderLayer\u001b[39;00m(\u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLayer):\n\u001b[0;32m      4\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\u001b[38;5;241m*\u001b[39m, d_model, num_heads, dff, dropout_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
            "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ],
      "source": [
        "## Encoder Layer\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x\n",
        "\n",
        "sample_encoder_layer = EncoderLayer(d_model=5, num_heads=8, dff=5)\n",
        "\n",
        "print(inputEmbedded.shape)\n",
        "print(sample_encoder_layer(inputEmbedded).shape)\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size, d_model=d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    # `x` is token-IDs shape: (batch, seq_len)\n",
        "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "    # Add dropout.\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "\n",
        "    return x  # Shape `(batch_size, seq_len, d_model)`.\n",
        "  \n",
        "  # Instantiate the encoder.\n",
        "sample_encoder = Encoder(num_layers=4,\n",
        "                         d_model=512,\n",
        "                         num_heads=8,\n",
        "                         dff=512,\n",
        "                         vocab_size=len(vectorizer.get_vocabulary()))\n",
        "\n",
        "sample_encoder_output = sample_encoder(inputEmbedded, training=False)\n",
        "\n",
        "# Print the shape.\n",
        "print(inputEmbedded.shape)\n",
        "print(sample_encoder_output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
