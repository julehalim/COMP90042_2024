{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext==0.4.0 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (0.4.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (4.66.2)\n",
            "Requirement already satisfied: requests in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (2.31.0)\n",
            "Requirement already satisfied: torch in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (2.3.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (1.24.3)\n",
            "Requirement already satisfied: six in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from requests->torchtext==0.4.0) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from requests->torchtext==0.4.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from requests->torchtext==0.4.0) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from requests->torchtext==0.4.0) (2024.2.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (2024.3.1)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (2021.4.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from tqdm->torchtext==0.4.0) (0.4.6)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->torchtext==0.4.0) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->torchtext==0.4.0) (2021.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from jinja2->torch->torchtext==0.4.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from sympy->torch->torchtext==0.4.0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "## package imports\n",
        "\n",
        "#!pip install pandas scikit-learn torch torchtext\n",
        "!pip3 install torchtext==0.4.0\n",
        "\n",
        "## deep-learning libraries\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import keras\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.data import Field, LabelField, Example, Dataset\n",
        "from tensorflow import convert_to_tensor\n",
        "\n",
        "## NLP preprocessing libraries\n",
        "import nltk\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "##others\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import json\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [],
      "source": [
        "## reading in json files\n",
        "\n",
        "\"\"\"\n",
        "Description of json files\n",
        "* [train-claims,dev-claims].json: JSON files for the labelled training and development set; \n",
        "* evidence.json: JSON file containing a large number of evidence passages (i.e. the “knowledge source”); \n",
        "* dev-claims-baseline.json: JSON file containing predictions of a baseline system on the development set;\n",
        "\"\"\"\n",
        "\n",
        "## relative file paths\n",
        "\n",
        "## baseline system - will not be used for any training/evaluation\n",
        "devClaimsBaselineFile='./data/dev-claims-baseline.json'\n",
        "## use this for model training\n",
        "trainClaimsFile='./data/train-claims.json'\n",
        "## use this set for hyperparameter tuning and evaluation metric \n",
        "devClaimsFile='./data/dev-claims.json'\n",
        "## evidence files need to be downloaded through https://drive.google.com/file/d/1JlUzRufknsHzKzvrEjgw8D3n_IRpjzo6/view?usp=sharing as it is to big to be uploaded to github\n",
        "evidenceFile='./data/evidence.json'\n",
        "## test unlabelled dataset\n",
        "testFile='./data/test-claims-unlabelled.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the JSON data\n",
        "with open(trainClaimsFile, 'r') as file:\n",
        "    trainClaims=json.load(file)\n",
        "with open(devClaimsFile, 'r') as file:\n",
        "    devClaims=json.load(file)\n",
        "with open(evidenceFile, 'r') as file:\n",
        "    evidenceData=json.load(file)\n",
        "\n",
        "## Preprocessing data -- lowercase, tokenize, and stopword removal\n",
        "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
        "tokenizer=get_tokenizer('basic_english')\n",
        "punctuations=string.punctuation\n",
        "\n",
        "def preprocess(text):\n",
        "    token=tokenizer(text.lower())\n",
        "    cleanedTokens=[t for t in token if (t not in stopwords) and (t not in punctuations)]\n",
        "    return ' '.join(cleanedTokens)\n",
        "\n",
        "for ids, texts in evidenceData.items():\n",
        "    evidenceData[ids]=preprocess(texts)\n",
        "\n",
        "# Function to create DataFrame and merge evidence IDs with Text\n",
        "def createDF(claims, evidence):\n",
        "    combinedData=[]\n",
        "    for claimID, claimText in claims.items():\n",
        "        # Combine the ID with its corresponding evidences\n",
        "        evidenceID=claimText['evidences']\n",
        "        evidenceText=(evidence[i] for i in evidenceID if i in evidence)\n",
        "        combinedData.append({\n",
        "            'claim_id': claimID,\n",
        "            'claim_text': preprocess(claimText['claim_text']),\n",
        "            'evidence_id': evidenceID,\n",
        "            'evidence_text': \" \".join(evidenceText),\n",
        "            'claim_label': claimText['claim_label']\n",
        "        })\n",
        "    # Create DataFrame\n",
        "    return pd.DataFrame(combinedData)\n",
        "\n",
        "# Create CSV Files\n",
        "trainFullMerged=createDF(trainClaims,evidenceData)\n",
        "devFullMerged=createDF(devClaims,evidenceData)\n",
        "trainFullMerged.to_csv(\"data/trainFullMerged.csv\", index=False)\n",
        "devFullMerged.to_csv(\"data/devFullMerged.csv\", index=False)\n",
        "\n",
        "# Convert evidence into csv as well\n",
        "evidenceFinal=pd.DataFrame(list(evidenceData.items()),columns=['evidence_id','evidence_text'])\n",
        "evidenceFinal.to_csv('data/evidencePreprocessed.csv',index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert unlabelled Data into CSV as well\n",
        "with open(testFile, 'r') as file:\n",
        "    testData=json.load(file)\n",
        "for ids, texts in testData.items():\n",
        "    claim_text = texts['claim_text']\n",
        "    testData[ids]=preprocess(claim_text)\n",
        "testFinal=pd.DataFrame(list(testData.items()), columns=['claim_id', 'claim_text'])\n",
        "testFinal.to_csv('data/testPreprocessed.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of training set: (1228, 6) Index(['claim_id', 'claim_text', 'evidence_id', 'evidence_text', 'claim_label',\n",
            "       'combined_evidence'],\n",
            "      dtype='object')\n",
            "shape of development set: (154, 6) Index(['claim_id', 'claim_text', 'evidence_id', 'evidence_text', 'claim_label',\n",
            "       'combined_evidence'],\n",
            "      dtype='object')\n",
            "shape of evidence: (1208827, 3) Index(['evidence_id', 'evidence_text', 'combined_evidence'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Reading in the created CSV files\n",
        "\n",
        "## used this for setting how much to see when printing dataframes, 50 is default\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "## use this for model training\n",
        "trainClaimsFile='./data/trainFullMerged.csv'\n",
        "## use this set for hyperparameter tuning and evaluation metric \n",
        "devClaimsFile='./data/devFullMerged.csv'\n",
        "## evidence files need to be downloaded through googledrive (https://drive.google.com/file/d/1OyihwdAWfqHIOueCB4bLBkYg4hTN_OKm/view?usp=sharing)\n",
        "evidenceFile='./data/evidencePreprocessed.csv'\n",
        "## test unlabelled dataset\n",
        "testFile='./data/testPreprocessed.csv'\n",
        "\n",
        "trainDataframe=pd.read_csv(trainClaimsFile)\n",
        "devDataframe=pd.read_csv(devClaimsFile)\n",
        "evidenceDataframe=pd.read_csv(evidenceFile)\n",
        "\n",
        "## One consideration is whether we want the evidence_id at all\n",
        "\n",
        "# trainDataframe['claim_text_unvectorized']=trainDataframe['claim_text']\n",
        "# trainDataframe['evidence_id']=trainDataframe['evidence_id'].astype(str).str.strip('[]').str.strip(\"'\").apply(preprocess)\n",
        "# trainDataframe['combined_evidence_unvectorized']=trainDataframe['evidence_id']+\" \"+trainDataframe['evidence_text']\n",
        "\n",
        "# devDataframe['claim_text_unvectorized']=devDataframe['claim_text']\n",
        "# devDataframe['evidence_id']=devDataframe['evidence_id'].astype(str).str.strip('[]').str.strip(\"'\").apply(preprocess)\n",
        "# devDataframe['combined_evidence_unvectorized']=devDataframe['evidence_id'] +\" \"+ devDataframe['evidence_text']\n",
        "\n",
        "# evidenceDataframe['combined_evidence_unvectorized']=evidenceDataframe['evidence_id']+\" \"+evidenceDataframe['evidence_text']\n",
        "# evidenceDataframe['combined_evidence_unvectorized']=evidenceDataframe['combined_evidence_unvectorized'].astype(str).str.strip(\"'\").apply(preprocess)\n",
        "\n",
        "trainDataframe['claim_text']=trainDataframe['claim_text']\n",
        "trainDataframe['evidence_id']=trainDataframe['evidence_id'].astype(str).str.strip('[]').str.strip(\"'\").apply(preprocess)\n",
        "trainDataframe['combined_evidence']=trainDataframe['evidence_id']+\" \"+trainDataframe['evidence_text']\n",
        "\n",
        "devDataframe['claim_text']=devDataframe['claim_text']\n",
        "devDataframe['evidence_id']=devDataframe['evidence_id'].astype(str).str.strip('[]').str.strip(\"'\").apply(preprocess)\n",
        "devDataframe['combined_evidence']=devDataframe['evidence_id'] +\" \"+ devDataframe['evidence_text']\n",
        "\n",
        "evidenceDataframe['combined_evidence']=evidenceDataframe['evidence_id']+\" \"+evidenceDataframe['evidence_text']\n",
        "evidenceDataframe['combined_evidence']=evidenceDataframe['combined_evidence'].astype(str).str.strip(\"'\").apply(preprocess)\n",
        "\n",
        "print('shape of training set:',trainDataframe.shape,trainDataframe.columns)\n",
        "print('shape of development set:',devDataframe.shape,devDataframe.columns)\n",
        "print('shape of evidence:',evidenceDataframe.shape,evidenceDataframe.columns)\n",
        "\n",
        "# textVector=TfidfVectorizer()\n",
        "# labelVector=TfidfVectorizer(max_features=1000)\n",
        "\n",
        "# textVector.fit(trainDataframe['claim_text_unvectorized'])\n",
        "# labelVector.fit(evidenceDataframe['combined_evidence_unvectorized'])\n",
        "\n",
        "# trainDataframe['combined_evidence']=list(labelVector.transform(trainDataframe['combined_evidence_unvectorized']))\n",
        "# trainDataframe['claim_text']=list(textVector.transform(trainDataframe['claim_text_unvectorized']))\n",
        "\n",
        "# devDataframe['combined_evidence']=list(labelVector.transform(devDataframe['combined_evidence_unvectorized']))\n",
        "# devDataframe['claim_text']=list(textVector.transform(devDataframe['claim_text_unvectorized']))\n",
        "\n",
        "# evidenceDataframe['combined_evidence']=list(labelVector.transform(evidenceDataframe['combined_evidence_unvectorized']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def convertTensorflow(features, labels=None):\n",
        "#     if labels is not None:\n",
        "#         dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "#     else:\n",
        "#         dataset = tf.data.Dataset.from_tensor_slices(features)\n",
        "#     return dataset\n",
        "\n",
        "# train_features = np.array([x.toarray() for x in trainDataframe['claim_text']])\n",
        "# train_labels = np.array([x.toarray() for x in trainDataframe['combined_evidence']])\n",
        "\n",
        "# trainTFDataset = convertTensorflow(train_features, train_labels)\n",
        "\n",
        "# dev_features = np.array([x.toarray() for x in devDataframe['claim_text']])\n",
        "# dev_labels = np.array([x.toarray() for x in devDataframe['combined_evidence']])\n",
        "\n",
        "# devTFDataset = convertTensorflow(dev_features, dev_labels)\n",
        "\n",
        "# evidence_features = np.array([x.toarray() for x in evidenceDataframe['combined_evidence']])\n",
        "\n",
        "# evidenceTFDataset = convertTensorflow(evidence_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(evidenceTFDataset)\n",
        "# for sample in evidenceTFDataset.take(3):  # Adjust the number taken based on dataset size\n",
        "#     print(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEXT Vocabulary:\n",
            "<unk>: 0\n",
            "<pad>: 1\n",
            "<text1>: 2\n",
            "<text2>: 3\n",
            " : 4\n",
            "e: 5\n",
            "a: 6\n",
            "r: 7\n",
            "i: 8\n",
            "s: 9\n",
            "t: 10\n",
            "n: 11\n",
            "o: 12\n",
            "l: 13\n",
            "c: 14\n",
            "d: 15\n",
            "m: 16\n",
            "g: 17\n",
            "u: 18\n",
            "p: 19\n",
            "h: 20\n",
            "y: 21\n",
            "w: 22\n",
            "b: 23\n",
            "f: 24\n",
            "v: 25\n",
            "0: 26\n",
            "k: 27\n",
            "2: 28\n",
            "1: 29\n",
            "x: 30\n",
            "-: 31\n",
            "9: 32\n",
            "’: 33\n",
            "j: 34\n",
            "5: 35\n",
            "3: 36\n",
            "“: 37\n",
            "7: 38\n",
            "q: 39\n",
            "8: 40\n",
            "4: 41\n",
            "”: 42\n",
            "[: 43\n",
            "6: 44\n",
            "]: 45\n",
            "z: 46\n",
            "—: 47\n",
            "‘: 48\n",
            "%: 49\n",
            "…: 50\n",
            "°: 51\n",
            "–: 52\n",
            "/: 53\n",
            "ñ: 54\n",
            "$: 55\n",
            "​: 56\n",
            "­: 57\n",
            "â: 58\n",
            "~: 59\n",
            ": 60\n",
            "˚: 61\n",
            ": 62\n",
            ": 63\n",
            "¦: 64\n",
            "à: 65\n",
            "ø: 66\n",
            "₂: 67\n"
          ]
        }
      ],
      "source": [
        "TEXT=torchtext.data.Field(tokenize=preprocess,\n",
        "                          init_token='<text1>',\n",
        "                          eos_token='<text2>',\n",
        "                          lower=True)\n",
        "LABEL = torchtext.data.LabelField(tokenize=preprocess,\n",
        "                          lower=True)\n",
        "\n",
        "## This converts dataframes into tensors, it does preprocessing twice as the data is already preprocessed but will leave it as is for now\n",
        "def createDatasetEncoderInput(dataframe,textTransform,labelTransform):\n",
        "    field=[(('reviewTextInput'),textTransform),(('evidenceTextOutput'),labelTransform)]\n",
        "    examples=[]\n",
        "    for _, row in dataframe.iterrows():\n",
        "        reviewTextInput=row['claim_text']\n",
        "        evidenceTextOutput=row['combined_evidence']\n",
        "        examples.append(Example.fromlist([reviewTextInput,evidenceTextOutput], field))\n",
        "    return Dataset(examples,fields=field)\n",
        "\n",
        "trainTensor=createDatasetEncoderInput(trainDataframe,TEXT,LABEL)\n",
        "devTensor=createDatasetEncoderInput(devDataframe,TEXT,LABEL)\n",
        "    \n",
        "## now, start to create the decoder inputs in the form of DATASETS for the combined_evidence\n",
        "def createDatasetDecoderInput(dataframe,labelTransform):\n",
        "    field=[(('evidenceTextOutput'),labelTransform)]\n",
        "    examples=[]\n",
        "    for _, row in dataframe.iterrows():\n",
        "        evidenceTextOutput=row['combined_evidence']\n",
        "        examples.append(Example.fromlist([evidenceTextOutput], field))\n",
        "    return Dataset(examples,fields=field)\n",
        "\n",
        "evidenceTensor=createDatasetDecoderInput(evidenceDataframe,LABEL)\n",
        "\n",
        "TEXT.build_vocab(trainTensor)\n",
        "print(\"TEXT Vocabulary:\")\n",
        "for word, index in TEXT.vocab.stoi.items():\n",
        "    print(f\"{word}: {index}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train {'reviewTextInput': 'scientific evidence co2 pollutant higher co2 concentrations actually help ecosystems support plant animal life', 'evidenceTextOutput': 'evidence-442946 evidence-1194317 evidence-12171 high concentrations 100 times atmospheric concentration greater carbon dioxide toxic animal life raising concentration 10 000 ppm 1% higher several hours eliminate pests whiteflies spider mites greenhouse plants grow much 50 percent faster concentrations 1 000 ppm co 2 compared ambient conditions though assumes change climate limitation nutrients higher carbon dioxide concentrations favourably affect plant growth demand water'}\n",
            "dev {'reviewTextInput': '[south australia] expensive electricity world', 'evidenceTextOutput': 'evidence-67732 evidence-572512 [citation needed] south australia highest retail price electricity country south australia highest power prices world'}\n",
            "evidence {'evidenceTextOutput': 'evidence-0 john bennet lawes english entrepreneur agricultural scientist'}\n",
            "train {'reviewTextInput': 'el niño drove record highs global temperatures suggesting rise may man-made emissions', 'evidenceTextOutput': 'evidence-338219 evidence-1127398 ‘climate change’ due natural forces human activity substantial evidence indicate human activity – specifically increased greenhouse gas ghgs emissions – key factor pace extent global temperature increases acceleration due mostly human-caused global warming driving thermal expansion seawater melting land-based ice sheets glaciers'}\n",
            "dev {'reviewTextInput': '3 per cent total annual global emissions carbon dioxide humans australia prod\\xaduces 1 3 per cent 3 per cent amount emissions reductio\\xadn effect global climate', 'evidenceTextOutput': 'evidence-996421 evidence-1080858 evidence-208053 evidence-699212 evidence-832334 2011 unep green economy report states [a]agricultural operations excluding land use changes produce approximately 13 per cent anthropogenic global ghg emissions market share 30% potentially clean electricity heat pumps could reduce global co 2 emissions 8% annually modern era emissions atmosphere volcanoes approximately 0 645 billion tonnes co 2 per year whereas humans contribute 29 billion tonnes co 2 year cumulative anthropogenic e human-emitted emissions co 2 fossil fuel use major cause global warming give indication countries contributed human-induced climate change countries fast growing emissions south korea iran australia apart oil rich persian gulf states highest percapita emission rate world'}\n",
            "evidence {'evidenceTextOutput': 'evidence-1 lindberg began professional career age 16 eventually moving new york city 1977'}\n",
            "train {'reviewTextInput': '1946 pdo switched cool phase', 'evidenceTextOutput': 'evidence-530063 evidence-984887 evidence reversals prevailing polarity meaning changes cool surface waters versus warm surface waters within region oscillation occurring around 1925 1947 1977 last two reversals corresponded dramatic shifts salmon production regimes north pacific ocean 1945/1946 pdo changed cool phase pattern regime shift similar 1970s episode maximum amplitude subarctic subtropical front greater signature near japan 1970s shift stronger near american west coast'}\n",
            "dev {'reviewTextInput': 'means world 1c warmer pre-industrial times', 'evidenceTextOutput': 'evidence-889933 evidence-694262 multiple independently produced instrumental datasets confirm 2009–2018 decade 0 93 ± 0 07 °c warmer pre-industrial baseline 1850–1900 planet 0 8 °c warmer pre-industrial times'}\n",
            "evidence {'evidenceTextOutput': 'evidence-2 ``boston ladies cambridge vampire weekend'}\n"
          ]
        }
      ],
      "source": [
        "assert (len(trainDataframe)==len(trainTensor)) and (len(devDataframe)==len(devTensor) and (len(evidenceDataframe)==len(evidenceTensor)))\n",
        "\n",
        "# checking how the tensors look\n",
        "for i in range(min(len(trainTensor), 3)):\n",
        "    print('train',vars(trainTensor.examples[i]))\n",
        "    print('dev',vars(devTensor.examples[i]))\n",
        "    print('evidence',vars(evidenceTensor.examples[i]))\n",
        "\n",
        "## this used the pytorch pipeline, change it to tensorflow pipelines now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "## convert into tensorflow tensors\n",
        "\n",
        "def extractTorchtext(dataset,fields,type):\n",
        "    texts=[]\n",
        "    labels=[]\n",
        "    hasLabel=True\n",
        "    if type==\"output\":\n",
        "        hasLabel=False\n",
        "    for examples in dataset:\n",
        "        text=getattr(examples,fields[0][0])\n",
        "        texts.append(text)\n",
        "        if hasLabel:\n",
        "            label=getattr(examples,fields[1][0])\n",
        "            labels.append(label)\n",
        "    return texts,labels\n",
        "\n",
        "trainTensorText,trainTensorLabel=extractTorchtext(trainTensor,[('reviewTextInput',None),('evidenceTextOutput',None)],'input')\n",
        "devTensorText,devTensorLabel=extractTorchtext(devTensor,[('reviewTextInput',None),('evidenceTextOutput',None)],'input')\n",
        "evidenceTexts,_=extractTorchtext(evidenceTensor,[('evidenceTextOutput',None)],'output')\n",
        "\n",
        "def convertToTensorflow(texts,labels=None):\n",
        "    if labels is not None:\n",
        "        dataset=tf.data.Dataset.from_tensor_slices((texts, labels))\n",
        "    else:\n",
        "        dataset=tf.data.Dataset.from_tensor_slices(texts)\n",
        "    return dataset\n",
        "\n",
        "trainTFDataset=convertToTensorflow(trainTensorText,trainTensorLabel)\n",
        "devTFDataset=convertToTensorflow(devTensorText,devTensorLabel)\n",
        "#evidenceTFDataset=convertToTensorflow(evidenceTexts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "## THIS CODE BLOCK IS REDUNDANT, IT WAS USED TO PERFORM COUNT EMBEDDINGS BUT MAYBE WILL IMPLEMENT LATER\n",
        "\n",
        "# print('Train dataset element spec:', trainTFDataset.element_spec)\n",
        "# print('Dev dataset element spec:', devTFDataset.element_spec)\n",
        "# print('Evidence dataset element spec:', evidenceTFDataset.element_spec)\n",
        "\n",
        "# from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "# textVector = TextVectorization(\n",
        "#     output_mode='int')\n",
        "\n",
        "# textData=trainTFDataset.map(lambda x, y: x)  # Extract just the text part from the dataset\n",
        "\n",
        "# # Adapt the vectorization layer to the text data\n",
        "# textVector.adapt(trainTFDataset.map(lambda x, y:x))\n",
        "\n",
        "# labelVector=TextVectorization(\n",
        "#     output_mode='int'\n",
        "# )\n",
        "\n",
        "# labelVector.adapt(evidenceTFDataset)\n",
        "\n",
        "# def vectorizeTexts(text,label):\n",
        "#     text=textVector(text)\n",
        "#     label=labelVector(label)\n",
        "#     return text,label\n",
        "\n",
        "# def vectorizeEvidence(label):\n",
        "#     label=labelVector(label)\n",
        "#     return label\n",
        "\n",
        "# vectorizedTrain=trainTFDataset.map(vectorizeTexts)\n",
        "# vectorizedDev=devTFDataset.map(vectorizeTexts)\n",
        "# vectorizedEvidence=evidenceTFDataset.map(vectorizeEvidence)\n",
        "\n",
        "# print(vectorizedEvidence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "## THIS CODE BLOCK IS REDUNDANT, IT WAS USED TO PERFORM COUNT EMBEDDINGS BUT MAYBE WILL IMPLEMENT LATER\n",
        "\n",
        "# print(vectorizedTrain.element_spec)\n",
        "# for sample in vectorizedEvidence.take(3):\n",
        "#     print('Sample:', sample.numpy())\n",
        "# vocabText=textVector.get_vocabulary()\n",
        "# vocabLabel=labelVector.get_vocabulary()\n",
        "# print(vocabText)\n",
        "# print(vocabLabel)\n",
        "# print(vectorizedTrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "## if we wanted to use word embeddings, process the TFDatasets first then run this\n",
        "## use these as tensors (these are EagerTensors)\n",
        "\n",
        "tensorTrain=tf.convert_to_tensor(list(trainTFDataset))\n",
        "tensorDev=tf.convert_to_tensor(list(devTFDataset))\n",
        "#tensorEvidence=tf.convert_to_tensor(list(evidenceTFDataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# trainNumpyText=tensorTrain[:,0].numpy()\n",
        "# trainNumpyLabel=tensorTrain[:,1].numpy()\n",
        "# devNumpyText=tensorDev[:,0].numpy()\n",
        "# devNumpyLabel=tensorDev[:,1].numpy()\n",
        "# evidenceNumpy=tensorEvidence.numpy()\n",
        "\n",
        "# tfidfVectorText=TfidfVectorizer()\n",
        "# tfidfVectorLabel=TfidfVectorizer(max_features=300)\n",
        "\n",
        "# tfidfVectorText.fit(trainNumpyText)\n",
        "# tfidfVectorLabel.fit(evidenceNumpy)\n",
        "\n",
        "# trainNumpyText=tfidfVectorText.transform(trainNumpyText)\n",
        "# trainNumpyLabel=tfidfVectorLabel.transform(trainNumpyLabel)\n",
        "\n",
        "# devNumpyText=tfidfVectorText.transform(devNumpyText)\n",
        "# devNumpyLabel=tfidfVectorLabel.transform(devNumpyLabel)\n",
        "\n",
        "# evidenceNumpy=tfidfVectorLabel.transform(evidenceNumpy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #MemoryError: Unable to allocate 16.5 GiB for an array with shape (1228, 1807303) and data type int64\n",
        "\n",
        "# tensorTFIDFTrainText=tf.convert_to_tensor(trainNumpyText.toarray(),dtype=tf.float32)\n",
        "# tensorTFIDFTrainLabel=tf.convert_to_tensor(trainNumpyLabel.toarray(),dtype=tf.float32)\n",
        "\n",
        "# tensorTFIDFDevText=tf.convert_to_tensor(devNumpyText.toarray(),dtype=tf.float32)\n",
        "# tensorTFIDFDevLabel=tf.convert_to_tensor(devNumpyLabel.toarray(),dtype=tf.float32)\n",
        "\n",
        "# tensorTFIDFEvidence=tf.convert_to_tensor(evidenceNumpy.toarray(),dtype=tf.float32)\n",
        "\n",
        "# tensorTFIDFTrain=tf.concat([tensorTFIDFDevText,tensorTFIDFDevLabel],axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![alt text](image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {},
      "outputs": [],
      "source": [
        "## POSITIONAL ENCODINGS\n",
        "\n",
        "from keras.models import Model\n",
        "\n",
        "def positional_encoding(length, depth):\n",
        "    positions=np.arange(length)[:, np.newaxis]\n",
        "    depth_indices=np.arange(depth)[np.newaxis, :]\n",
        "    angle_rates = 1 / np.power(10000, (2 * (depth_indices // 2)) / np.float32(depth))\n",
        "    angle_rads = positions * angle_rates\n",
        "    # sine applied to even indices; cosine applied to odd indices\n",
        "    pos_encoding = np.concatenate([np.sin(angle_rads[:, 0::2]), np.cos(angle_rads[:, 1::2])], axis=-1)\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "  \n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        self.d_model=d_model\n",
        "        self.embedding=tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "        self.pos_encoding=positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "    def compute_mask(self, *args, **kwargs):\n",
        "        return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "    def call(self, x):\n",
        "        length = tf.shape(x)[1]\n",
        "        x=self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[tf.newaxis, :length, :]\n",
        "        return x\n",
        "  \n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "## I only use one encoder here, but we can try to use two encoders, one for label and one for the review text\n",
        "\n",
        "tensorTrainText=tensorTrain[:,1]\n",
        "tensorTrainLabels=tensorTrain[:,0]\n",
        "tensorTrainConcat=tf.concat([tensorTrainLabels,tensorTrainText],axis=0)\n",
        "\n",
        "vectorizer=TextVectorization(output_mode='int',output_sequence_length=15)\n",
        "\n",
        "## CAN CHANGE THIS ADAPT TO RUN ON THE EVIDENCE INSTEAD, MIGHT BE BETTER BUT IT SLOWS DOWN MY PC A LOT SO WILL LEAVE AS LABELS ONLY FOR NOW\n",
        "vectorizer.adapt(tensorTrainConcat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedText=PositionalEmbedding(vocab_size=(len(vectorizer.get_vocabulary())),d_model=512)\n",
        "embedLabel=PositionalEmbedding(vocab_size=(len(vectorizer.get_vocabulary())),d_model=512)\n",
        "\n",
        "vectorizedTensorTrainText=vectorizer(tensorTrainText)\n",
        "vectorizedTensorTrainLabels=vectorizer(tensorTrainLabels)\n",
        "#vectorizedEvidence=vectorizer(tensorEvidence)\n",
        "\n",
        "\n",
        "inputEmbedded=embedText(vectorizedTensorTrainText)\n",
        "outputEmbedded=embedText(vectorizedTensorTrainLabels)\n",
        "\n",
        "## CANNOT EMBED THIS, CAUSES OUT OF MEMORY ERROR\n",
        "#outputEmbedded=embedLabel(vectorizedEvidence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1228, 15), dtype=bool, numpy=\n",
              "array([[ True,  True,  True, ...,  True,  True,  True],\n",
              "       [ True,  True,  True, ...,  True,  True,  True],\n",
              "       [ True,  True,  True, ...,  True,  True,  True],\n",
              "       ...,\n",
              "       [ True,  True,  True, ...,  True,  True,  True],\n",
              "       [ True,  True,  True, ...,  True,  True,  True],\n",
              "       [ True,  True,  True, ...,  True,  True,  True]])>"
            ]
          },
          "execution_count": 187,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Check shape\n",
        "inputEmbedded._keras_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Base attention layers\n",
        "\n",
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        self.mha=tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "        self.layernorm=tf.keras.layers.LayerNormalization()\n",
        "        self.add=tf.keras.layers.Add()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1228, 15, 512)\n",
            "(1228, 15, 512)\n",
            "(1228, 15, 512)\n"
          ]
        }
      ],
      "source": [
        "## cross attention layer\n",
        "\n",
        "from tensorflow.keras.layers import Layer, MultiHeadAttention, LayerNormalization, Add\n",
        "\n",
        "class CrossAttention(Layer):\n",
        "    def __init__(self, num_heads, key_dim, **kwargs):\n",
        "        super(CrossAttention, self).__init__(**kwargs)\n",
        "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
        "        self.layernorm = LayerNormalization(epsilon=1e-6)\n",
        "        self.add = Add()\n",
        "\n",
        "    def call(self, query, context):\n",
        "        attn_output, attn_scores = self.mha(query=query, value=context, key=context, return_attention_scores=True)\n",
        "        self.last_attn_scores = attn_scores\n",
        "        x = self.add([query, attn_output])\n",
        "        x = self.layernorm(x)\n",
        "        return x\n",
        "\n",
        "## key_dim CAN BE INCREASED BUT IT SLOWS DOWN PC\n",
        "sample_ca=CrossAttention(num_heads=2, key_dim=5)\n",
        "\n",
        "result=sample_ca(outputEmbedded, inputEmbedded)\n",
        "\n",
        "print(inputEmbedded.shape)\n",
        "print(outputEmbedded.shape)\n",
        "print(result.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1228, 15, 512)\n",
            "(1228, 15, 512)\n"
          ]
        }
      ],
      "source": [
        "## global self-attention layer\n",
        "\n",
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "\n",
        "sample_gsa = GlobalSelfAttention(num_heads=2, key_dim=5)\n",
        "\n",
        "print(inputEmbedded.shape)\n",
        "print(sample_gsa(inputEmbedded).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1228, 15, 512)\n",
            "(1228, 15, 512)\n"
          ]
        }
      ],
      "source": [
        "## Causal self attention layer\n",
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "\n",
        "sample_csa = CausalSelfAttention(num_heads=2, key_dim=5)\n",
        "\n",
        "print(inputEmbedded.shape)\n",
        "print(sample_csa(inputEmbedded).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max difference: 4.7683716e-07\n"
          ]
        }
      ],
      "source": [
        "out1 = sample_csa(inputEmbedded[:, :3])\n",
        "out2 = sample_csa(inputEmbedded)[:, :3]\n",
        "\n",
        "diff = tf.reduce_max(abs(out1 - out2))\n",
        "print(\"Max difference:\", diff.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1228, 15, 512)\n",
            "(1228, 15, 512)\n"
          ]
        }
      ],
      "source": [
        "## Feedforward layer\n",
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x) \n",
        "    return x\n",
        "\n",
        "## Edit these numbers -- the shape needs to be the same for the ffn numbers\n",
        "sample_ffn = FeedForward(512, 150)\n",
        "\n",
        "print(inputEmbedded.shape)\n",
        "print(sample_ffn(inputEmbedded).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1228, 15, 512)\n",
            "(1228, 15, 512)\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "Exception encountered when calling layer 'positional_embedding_82' (type PositionalEmbedding).\n\n{{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} slice index 1 of dimension 0 out of bounds. [Op:StridedSlice] name: encoder_54/positional_embedding_82/strided_slice/\n\nCall arguments received by layer 'positional_embedding_82' (type PositionalEmbedding):\n  • x=tf.Tensor(shape=(2456,), dtype=string)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[200], line 62\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Instantiate the encoder.\u001b[39;00m\n\u001b[0;32m     56\u001b[0m sample_encoder \u001b[38;5;241m=\u001b[39m Encoder(num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m     57\u001b[0m                          d_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[0;32m     58\u001b[0m                          num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m     59\u001b[0m                          dff\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m,\n\u001b[0;32m     60\u001b[0m                          vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m513\u001b[39m)\n\u001b[1;32m---> 62\u001b[0m sample_encoder_output \u001b[38;5;241m=\u001b[39m \u001b[43msample_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensorTrainConcat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Print the shape.\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(tensorTrainConcat\u001b[38;5;241m.\u001b[39mshape)\n",
            "File \u001b[1;32mc:\\Users\\julev\\anaconda3\\envs\\Python-3-8\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "Cell \u001b[1;32mIn[200], line 45\u001b[0m, in \u001b[0;36mEncoder.call\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     44\u001b[0m   \u001b[38;5;66;03m# `x` is token-IDs shape: (batch, seq_len)\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape `(batch_size, seq_len, d_model)`.\u001b[39;00m\n\u001b[0;32m     47\u001b[0m   \u001b[38;5;66;03m# Add dropout.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
            "Cell \u001b[1;32mIn[185], line 25\u001b[0m, in \u001b[0;36mPositionalEmbedding.call\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 25\u001b[0m     length \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     26\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x[\u001b[38;5;241m0\u001b[39m:])\n\u001b[0;32m     27\u001b[0m     x \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39msqrt(tf\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model, tf\u001b[38;5;241m.\u001b[39mfloat32))\n",
            "\u001b[1;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'positional_embedding_82' (type PositionalEmbedding).\n\n{{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} slice index 1 of dimension 0 out of bounds. [Op:StridedSlice] name: encoder_54/positional_embedding_82/strided_slice/\n\nCall arguments received by layer 'positional_embedding_82' (type PositionalEmbedding):\n  • x=tf.Tensor(shape=(2456,), dtype=string)"
          ]
        }
      ],
      "source": [
        "## Encoder Layer\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x\n",
        "\n",
        "sample_encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)\n",
        "\n",
        "print(inputEmbedded.shape)\n",
        "print(sample_encoder_layer(inputEmbedded).shape)\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size, d_model=d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    # `x` is token-IDs shape: (batch, seq_len)\n",
        "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "    # Add dropout.\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "\n",
        "    return x  # Shape `(batch_size, seq_len, d_model)`\n",
        "  \n",
        "# Instantiate the encoder.\n",
        "sample_encoder = Encoder(num_layers=4,\n",
        "                         d_model=512,\n",
        "                         num_heads=8,\n",
        "                         dff=2048,\n",
        "                         vocab_size=8500)\n",
        "\n",
        "sample_encoder_output = sample_encoder(tensorTrainConcat, training=False)\n",
        "\n",
        "# Print the shape.\n",
        "print(tensorTrainConcat.shape)\n",
        "print(sample_encoder_output.shape)  # Shape `(batch_size, input_seq_len, d_model)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
