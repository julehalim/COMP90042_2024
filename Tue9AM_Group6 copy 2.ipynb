{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 332,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext==0.4.0 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (0.4.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (4.66.2)\n",
            "Requirement already satisfied: requests in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (2.31.0)\n",
            "Requirement already satisfied: torch in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (2.3.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (1.24.3)\n",
            "Requirement already satisfied: six in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from requests->torchtext==0.4.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from requests->torchtext==0.4.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from requests->torchtext==0.4.0) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from requests->torchtext==0.4.0) (2024.2.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (2024.3.1)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (2021.4.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from tqdm->torchtext==0.4.0) (0.4.6)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->torchtext==0.4.0) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->torchtext==0.4.0) (2021.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from jinja2->torch->torchtext==0.4.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from sympy->torch->torchtext==0.4.0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "## package imports\n",
        "\n",
        "#!pip install pandas scikit-learn torch torchtext\n",
        "!pip3 install torchtext==0.4.0\n",
        "\n",
        "## deep-learning libraries\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import keras\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.data import Field, LabelField, Example, Dataset\n",
        "from tensorflow import convert_to_tensor\n",
        "\n",
        "## NLP preprocessing libraries\n",
        "import nltk\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "##others\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "import string\n",
        "import json\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 333,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [],
      "source": [
        "## reading in json files\n",
        "\n",
        "\"\"\"\n",
        "Description of json files\n",
        "* [train-claims,dev-claims].json: JSON files for the labelled training and development set; \n",
        "* evidence.json: JSON file containing a large number of evidence passages (i.e. the “knowledge source”); \n",
        "* dev-claims-baseline.json: JSON file containing predictions of a baseline system on the development set;\n",
        "\"\"\"\n",
        "\n",
        "## relative file paths\n",
        "\n",
        "## baseline system - will not be used for any training/evaluation\n",
        "devClaimsBaselineFile='./data/dev-claims-baseline.json'\n",
        "## use this for model training\n",
        "trainClaimsFile='./data/train-claims.json'\n",
        "## use this set for hyperparameter tuning and evaluation metric \n",
        "devClaimsFile='./data/dev-claims.json'\n",
        "## evidence files need to be downloaded through https://drive.google.com/file/d/1JlUzRufknsHzKzvrEjgw8D3n_IRpjzo6/view?usp=sharing as it is to big to be uploaded to github\n",
        "evidenceFile='./data/evidence.json'\n",
        "## test unlabelled dataset\n",
        "testFile='./data/test-claims-unlabelled.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 334,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the JSON data\n",
        "with open(trainClaimsFile, 'r') as file:\n",
        "    trainClaims=json.load(file)\n",
        "with open(devClaimsFile, 'r') as file:\n",
        "    devClaims=json.load(file)\n",
        "with open(evidenceFile, 'r') as file:\n",
        "    evidenceData=json.load(file)\n",
        "\n",
        "## Preprocessing data -- lowercase, tokenize, and stopword removal\n",
        "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
        "tokenizer=get_tokenizer('basic_english')\n",
        "punctuations=string.punctuation\n",
        "\n",
        "def preprocess(text):\n",
        "    token=tokenizer(text.lower())\n",
        "    cleanedTokens=[t for t in token if (t not in stopwords) and (t not in punctuations)]\n",
        "    return ' '.join(cleanedTokens)\n",
        "\n",
        "for ids, texts in evidenceData.items():\n",
        "    evidenceData[ids]=preprocess(texts)\n",
        "\n",
        "# Function to create DataFrame and merge evidence IDs with Text\n",
        "def createDF(claims, evidence):\n",
        "    combinedData=[]\n",
        "    for claimID, claimText in claims.items():\n",
        "        # Combine the ID with its corresponding evidences\n",
        "        evidenceID=claimText['evidences']\n",
        "        evidenceText=(evidence[i] for i in evidenceID if i in evidence)\n",
        "        combinedData.append({\n",
        "            'claim_id': claimID,\n",
        "            'claim_text': preprocess(claimText['claim_text']),\n",
        "            'evidence_id': evidenceID,\n",
        "            'evidence_text': \" \".join(evidenceText),\n",
        "            'claim_label': claimText['claim_label']\n",
        "        })\n",
        "    # Create DataFrame\n",
        "    return pd.DataFrame(combinedData)\n",
        "\n",
        "# Create CSV Files\n",
        "trainFullMerged=createDF(trainClaims,evidenceData)\n",
        "devFullMerged=createDF(devClaims,evidenceData)\n",
        "trainFullMerged.to_csv(\"data/trainFullMerged.csv\", index=False)\n",
        "devFullMerged.to_csv(\"data/devFullMerged.csv\", index=False)\n",
        "\n",
        "# Convert evidence into csv as well\n",
        "evidenceFinal=pd.DataFrame(list(evidenceData.items()),columns=['evidence_id','evidence_text'])\n",
        "evidenceFinal.to_csv('data/evidencePreprocessed.csv',index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 335,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert unlabelled Data into CSV as well\n",
        "with open(testFile, 'r') as file:\n",
        "    testData=json.load(file)\n",
        "for ids, texts in testData.items():\n",
        "    claim_text = texts['claim_text']\n",
        "    testData[ids]=preprocess(claim_text)\n",
        "testFinal=pd.DataFrame(list(testData.items()), columns=['claim_id', 'claim_text'])\n",
        "testFinal.to_csv('data/testPreprocessed.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 336,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of training set: (1228, 6) Index(['claim_id', 'claim_text', 'evidence_id', 'evidence_text', 'claim_label',\n",
            "       'combined_evidence'],\n",
            "      dtype='object')\n",
            "shape of development set: (154, 6) Index(['claim_id', 'claim_text', 'evidence_id', 'evidence_text', 'claim_label',\n",
            "       'combined_evidence'],\n",
            "      dtype='object')\n",
            "shape of evidence: (1208827, 3) Index(['evidence_id', 'evidence_text', 'combined_evidence'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Reading in the created CSV files\n",
        "\n",
        "## used this for setting how much to see when printing dataframes, 50 is default\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "## use this for model training\n",
        "trainClaimsFile='./data/trainFullMerged.csv'\n",
        "## use this set for hyperparameter tuning and evaluation metric \n",
        "devClaimsFile='./data/devFullMerged.csv'\n",
        "## evidence files need to be downloaded through https://drive.google.com/file/d/1JlUzRufknsHzKzvrEjgw8D3n_IRpjzo6/view?usp=sharing as it is to big to be uploaded to github\n",
        "evidenceFile='./data/evidencePreprocessed.csv'\n",
        "## test unlabelled dataset\n",
        "testFile='./data/testPreprocessed.csv'\n",
        "\n",
        "trainDataframe=pd.read_csv(trainClaimsFile)\n",
        "devDataframe=pd.read_csv(devClaimsFile)\n",
        "evidenceDataframe=pd.read_csv(evidenceFile)\n",
        "\n",
        "## One consideration is whether we want the evidence_id at all\n",
        "\n",
        "trainDataframe['evidence_id']=trainDataframe['evidence_id'].astype(str).str.strip('[]').str.strip(\"'\")\n",
        "trainDataframe['combined_evidence']=trainDataframe['evidence_id']+\" \"+trainDataframe['evidence_text']\n",
        "devDataframe['evidence_id']=devDataframe['evidence_id'].astype(str).str.strip('[]').str.strip(\"'\")\n",
        "devDataframe['combined_evidence']=devDataframe['evidence_id'] +\" \"+ devDataframe['evidence_text']\n",
        "evidenceDataframe['combined_evidence']=evidenceDataframe['evidence_id']+\" \"+evidenceDataframe['evidence_text']\n",
        "evidenceDataframe['combined_evidence']=evidenceDataframe['combined_evidence'].astype(str).str.strip(\"'\")\n",
        "\n",
        "print('shape of training set:',trainDataframe.shape,trainDataframe.columns)\n",
        "print('shape of development set:',devDataframe.shape,devDataframe.columns)\n",
        "print('shape of evidence:',evidenceDataframe.shape,evidenceDataframe.columns)\n",
        "\n",
        "\n",
        "TEXT=torchtext.data.Field(tokenize=preprocess,\n",
        "                          init_token='<text1>',\n",
        "                          eos_token='<text2>',\n",
        "                          lower=True)\n",
        "LABEL = torchtext.data.LabelField(tokenize=preprocess,\n",
        "                          lower=True)\n",
        "\n",
        "## This converts dataframes into tensors, it does preprocessing twice as the data is already preprocessed but will leave it as is for now\n",
        "def createDatasetEncoderInput(dataframe,textTransform,labelTransform):\n",
        "    fields=[(('reviewTextInput'),textTransform),(('evidenceTextOutput'),labelTransform)]\n",
        "    examples=[]\n",
        "    for _, row in dataframe.iterrows():\n",
        "        reviewTextInput=row['claim_text']\n",
        "        evidenceTextOutput=row['combined_evidence']\n",
        "        examples.append(Example.fromlist([reviewTextInput,evidenceTextOutput], fields))\n",
        "    return Dataset(examples,fields)\n",
        "\n",
        "trainTensor=createDatasetEncoderInput(trainDataframe,TEXT,LABEL)\n",
        "devTensor=createDatasetEncoderInput(devDataframe,TEXT,LABEL)\n",
        "    \n",
        "## now, start to create the decoder inputs in the form of DATASETS for the combined_evidence\n",
        "def createDatasetDecoderInput(dataframe,labelTransform):\n",
        "    fields=[(('evidenceTextOutput'),labelTransform)]\n",
        "    examples=[]\n",
        "    for _, row in dataframe.iterrows():\n",
        "        evidenceTextOutput=row['combined_evidence']\n",
        "        examples.append(Example.fromlist([evidenceTextOutput], fields))\n",
        "    return Dataset(examples,fields)\n",
        "\n",
        "evidenceTensor=createDatasetDecoderInput(evidenceDataframe,LABEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 337,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train {'reviewTextInput': 'scientific evidence co2 pollutant higher co2 concentrations actually help ecosystems support plant animal life', 'evidenceTextOutput': \"evidence-442946', 'evidence-1194317', 'evidence-12171 high concentrations 100 times atmospheric concentration greater carbon dioxide toxic animal life raising concentration 10 000 ppm 1% higher several hours eliminate pests whiteflies spider mites greenhouse plants grow much 50 percent faster concentrations 1 000 ppm co 2 compared ambient conditions though assumes change climate limitation nutrients higher carbon dioxide concentrations favourably affect plant growth demand water\"}\n",
            "dev {'reviewTextInput': '[south australia] expensive electricity world', 'evidenceTextOutput': \"evidence-67732', 'evidence-572512 [citation needed] south australia highest retail price electricity country south australia highest power prices world\"}\n",
            "evidence {'evidenceTextOutput': 'evidence-0 john bennet lawes english entrepreneur agricultural scientist'}\n",
            "train {'reviewTextInput': 'el niño drove record highs global temperatures suggesting rise may man-made emissions', 'evidenceTextOutput': \"evidence-338219', 'evidence-1127398 ‘climate change’ due natural forces human activity substantial evidence indicate human activity – specifically increased greenhouse gas ghgs emissions – key factor pace extent global temperature increases acceleration due mostly human-caused global warming driving thermal expansion seawater melting land-based ice sheets glaciers\"}\n",
            "dev {'reviewTextInput': '3 per cent total annual global emissions carbon dioxide humans australia prod\\xaduces 1 3 per cent 3 per cent amount emissions reductio\\xadn effect global climate', 'evidenceTextOutput': \"evidence-996421', 'evidence-1080858', 'evidence-208053', 'evidence-699212', 'evidence-832334 2011 unep green economy report states [a]agricultural operations excluding land use changes produce approximately 13 per cent anthropogenic global ghg emissions market share 30% potentially clean electricity heat pumps could reduce global co 2 emissions 8% annually modern era emissions atmosphere volcanoes approximately 0 645 billion tonnes co 2 per year whereas humans contribute 29 billion tonnes co 2 year cumulative anthropogenic e human-emitted emissions co 2 fossil fuel use major cause global warming give indication countries contributed human-induced climate change countries fast growing emissions south korea iran australia apart oil rich persian gulf states highest percapita emission rate world\"}\n",
            "evidence {'evidenceTextOutput': 'evidence-1 lindberg began professional career age 16 eventually moving new york city 1977'}\n",
            "train {'reviewTextInput': '1946 pdo switched cool phase', 'evidenceTextOutput': \"evidence-530063', 'evidence-984887 evidence reversals prevailing polarity meaning changes cool surface waters versus warm surface waters within region oscillation occurring around 1925 1947 1977 last two reversals corresponded dramatic shifts salmon production regimes north pacific ocean 1945/1946 pdo changed cool phase pattern regime shift similar 1970s episode maximum amplitude subarctic subtropical front greater signature near japan 1970s shift stronger near american west coast\"}\n",
            "dev {'reviewTextInput': 'means world 1c warmer pre-industrial times', 'evidenceTextOutput': \"evidence-889933', 'evidence-694262 multiple independently produced instrumental datasets confirm 2009–2018 decade 0 93 ± 0 07 °c warmer pre-industrial baseline 1850–1900 planet 0 8 °c warmer pre-industrial times\"}\n",
            "evidence {'evidenceTextOutput': 'evidence-2 ``boston ladies cambridge vampire weekend'}\n"
          ]
        }
      ],
      "source": [
        "assert (len(trainDataframe)==len(trainTensor)) and (len(devDataframe)==len(devTensor) and (len(evidenceDataframe)==len(evidenceTensor)))\n",
        "\n",
        "# checking how the tensors look\n",
        "for i in range(min(len(trainTensor), 3)):\n",
        "    print('train',vars(trainTensor.examples[i]))\n",
        "    print('dev',vars(devTensor.examples[i]))\n",
        "    print('evidence',vars(evidenceTensor.examples[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
