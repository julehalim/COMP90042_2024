{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext==0.4.0 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (0.4.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (4.66.2)\n",
            "Requirement already satisfied: requests in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (2.31.0)\n",
            "Requirement already satisfied: torch in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (2.3.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (1.24.3)\n",
            "Requirement already satisfied: six in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from requests->torchtext==0.4.0) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from requests->torchtext==0.4.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from requests->torchtext==0.4.0) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from requests->torchtext==0.4.0) (2024.2.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (2024.3.1)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (2021.4.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from tqdm->torchtext==0.4.0) (0.4.6)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->torchtext==0.4.0) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->torchtext==0.4.0) (2021.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from jinja2->torch->torchtext==0.4.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from sympy->torch->torchtext==0.4.0) (1.3.0)\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Preprocessing data -- lowercase, tokenize, and stopword removal\n",
        "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
        "tokenizer=get_tokenizer('basic_english')\n",
        "punctuations=set(string.punctuation)\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens=tokenizer(text)\n",
        "    cleanedTokens = [token for token in tokens if token not in stopwords and token not in punctuations]\n",
        "    return cleanedTokens\n",
        "\n",
        "TEXT = Field(tokenize=preprocess, lower=True, init_token='<sos>', eos_token='<eos>', batch_first=False)\n",
        "LABEL=LabelField(sequential=False)\n",
        "\n",
        "def createDatasetEncoderInput(dataframe,textTransform,labelTransform):\n",
        "    field=[('reviewTextInput',textTransform),('evidenceTextOutput',labelTransform)]\n",
        "    examples=[]\n",
        "    for _, row in dataframe.iterrows():\n",
        "        reviewTextInput=row['combined_input']\n",
        "        evidenceTextOutput=row['claim_label']\n",
        "        examples.append(Example.fromlist([reviewTextInput,evidenceTextOutput], field))\n",
        "    return Dataset(examples,fields=field)\n",
        "\n",
        "def createDatasetOutput(dataframe,labelTransform):\n",
        "    field=[('evidenceTextOutput',labelTransform)]\n",
        "    examples=[]\n",
        "    for _, row in dataframe.iterrows():\n",
        "        evidenceTextOutput=row['claim_label']\n",
        "        examples.append(Example.fromlist([evidenceTextOutput], field))\n",
        "    return Dataset(examples,fields=field)\n",
        "\n",
        "def createDatasetTest(dataframe,textTransform):\n",
        "    field=[('reviewTextInput',textTransform)]\n",
        "    examples=[]\n",
        "    for _, row in dataframe.iterrows():\n",
        "        reviewTextInput=row['combined_input']\n",
        "        examples.append(Example.fromlist([reviewTextInput], field))\n",
        "    return Dataset(examples,fields=field)\n",
        "\n",
        "trainTensor = createDatasetEncoderInput(trainDataframe, TEXT,LABEL)\n",
        "devTensor = createDatasetEncoderInput(devDataframe, TEXT,LABEL)\n",
        "#evidenceTensor=createDatasetOutput(evidenceDataframe,LABEL)\n",
        "#testTensor=createDatasetTest(testDataframe,TEXT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "TEXT.build_vocab(trainTensor)\n",
        "LABEL.build_vocab(trainTensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_iterator, dev_iterator = BucketIterator.splits(\n",
        "    (trainTensor, devTensor),\n",
        "    batch_size=32,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['SUPPORTS', 'NOT_ENOUGH_INFO', 'REFUTES', 'DISPUTED']\n"
          ]
        }
      ],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, vocab_size=5000, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(vocab_size, d_model)\n",
        "        position = torch.arange(0, vocab_size, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float()\n",
        "            * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, : x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "    \n",
        "class Net(nn.Module):\n",
        "    \"\"\"\n",
        "    Text classifier based on a pytorch TransformerEncoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embeddings,\n",
        "        nhead=8,\n",
        "        dim_feedforward=2048,\n",
        "        num_layers=6,\n",
        "        dropout=0.1,\n",
        "        activation=\"relu\",\n",
        "        classifier_dropout=0.1,\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        vocab_size, d_model = embeddings.size()\n",
        "        assert d_model % nhead == 0, \"nheads must divide evenly into d_model\"\n",
        "\n",
        "        self.emb = nn.Embedding.from_pretrained(embeddings, freeze=False)\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(\n",
        "            d_model=d_model,\n",
        "            dropout=dropout,\n",
        "            vocab_size=vocab_size,\n",
        "        )\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers,\n",
        "        )\n",
        "        self.classifier = nn.Linear(d_model, 2)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'size'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mNet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTEXT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnhead\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# the number of heads in the multiheadattention models\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim_feedforward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# the dimension of the feedforward network model in nn.TransformerEncoder\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclassifier_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m     13\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-4\u001b[39m\n",
            "Cell \u001b[1;32mIn[13], line 43\u001b[0m, in \u001b[0;36mNet.__init__\u001b[1;34m(self, embeddings, nhead, dim_feedforward, num_layers, dropout, activation, classifier_dropout)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     32\u001b[0m     embeddings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     classifier_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     39\u001b[0m ):\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m---> 43\u001b[0m     vocab_size, d_model \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m()\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m d_model \u001b[38;5;241m%\u001b[39m nhead \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnheads must divide evenly into d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding\u001b[38;5;241m.\u001b[39mfrom_pretrained(embeddings, freeze\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'size'"
          ]
        }
      ],
      "source": [
        "epochs = 50\n",
        "model = Net(\n",
        "    TEXT.vocab.vectors,\n",
        "    nhead=5,  # the number of heads in the multiheadattention models\n",
        "    dim_feedforward=50,  # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "    num_layers=6,\n",
        "    dropout=0.0,\n",
        "    classifier_dropout=0.0,\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "lr = 1e-4\n",
        "optimizer = torch.optim.Adam(\n",
        "    (p for p in model.parameters() if p.requires_grad), lr=lr\n",
        ")\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "print(\"starting\")\n",
        "for epoch in range(epochs):\n",
        "    print(f\"{epoch=}\")\n",
        "    epoch_loss = 0\n",
        "    epoch_correct = 0\n",
        "    epoch_count = 0\n",
        "    for idx, batch in enumerate(iter(train_iterator)):\n",
        "        predictions = model(batch.text.to(device))\n",
        "        labels = batch.label.to(device) - 1\n",
        "\n",
        "        loss = criterion(predictions, labels)\n",
        "\n",
        "        correct = predictions.argmax(axis=1) == labels\n",
        "        acc = correct.sum().item() / correct.size(0)\n",
        "\n",
        "        epoch_correct += correct.sum().item()\n",
        "        epoch_count += correct.size(0)\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_epoch_loss = 0\n",
        "        test_epoch_correct = 0\n",
        "        test_epoch_count = 0\n",
        "\n",
        "        for idx, batch in enumerate(iter(dev_iterator)):\n",
        "            predictions = model(batch.text.to(device))\n",
        "            labels = batch.label.to(device) - 1\n",
        "            test_loss = criterion(predictions, labels)\n",
        "\n",
        "            correct = predictions.argmax(axis=1) == labels\n",
        "            acc = correct.sum().item() / correct.size(0)\n",
        "\n",
        "            test_epoch_correct += correct.sum().item()\n",
        "            test_epoch_count += correct.size(0)\n",
        "            test_epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"{epoch_loss=}\")\n",
        "    print(f\"epoch accuracy: {epoch_correct / epoch_count}\")\n",
        "    print(f\"{test_epoch_loss=}\")\n",
        "    print(f\"test epoch accuracy: {test_epoch_correct / test_epoch_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
