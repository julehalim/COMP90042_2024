{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext==0.4.0 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (0.4.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (4.66.2)\n",
            "Requirement already satisfied: requests in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (2.31.0)\n",
            "Requirement already satisfied: torch in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (2.3.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (1.24.3)\n",
            "Requirement already satisfied: six in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torchtext==0.4.0) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from requests->torchtext==0.4.0) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from requests->torchtext==0.4.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from requests->torchtext==0.4.0) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from requests->torchtext==0.4.0) (2024.2.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (2024.3.1)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from torch->torchtext==0.4.0) (2021.4.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from tqdm->torchtext==0.4.0) (0.4.6)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->torchtext==0.4.0) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->torchtext==0.4.0) (2021.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from jinja2->torch->torchtext==0.4.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\julev\\anaconda3\\envs\\python-3-8\\lib\\site-packages (from sympy->torch->torchtext==0.4.0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "## import libraries\n",
        "\n",
        "!pip3 install torchtext==0.4.0\n",
        "\n",
        "# Standard libraries\n",
        "import math\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# Machine learning and deep learning libraries\n",
        "import tensorflow\n",
        "import keras\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.data import Field, LabelField, Example, Dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# Natural Language Processing (NLP) libraries\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [],
      "source": [
        "## reading in json files\n",
        "\n",
        "\"\"\"\n",
        "Description of json files\n",
        "* [train-claims,dev-claims].json: JSON files for the labelled training and development set; \n",
        "* evidence.json: JSON file containing a large number of evidence passages (i.e. the “knowledge source”); \n",
        "* dev-claims-baseline.json: JSON file containing predictions of a baseline system on the development set;\n",
        "\"\"\"\n",
        "\n",
        "## relative file paths\n",
        "\n",
        "## baseline system - will not be used for any training/evaluation\n",
        "devClaimsBaselineFile='./data/dev-claims-baseline.json'\n",
        "## use this for model training\n",
        "trainClaimsFile='./data/train-claims.json'\n",
        "## use this set for hyperparameter tuning and evaluation metric \n",
        "devClaimsFile='./data/dev-claims.json'\n",
        "## evidence files need to be downloaded through https://drive.google.com/file/d/1JlUzRufknsHzKzvrEjgw8D3n_IRpjzo6/view?usp=sharing as it is to big to be uploaded to github\n",
        "evidenceFile='./data/evidence.json'\n",
        "\n",
        "testFile='./data/test-claims-unlabelled.json'\n",
        "\n",
        "tokenizer=get_tokenizer(\"basic_english\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the JSON data\n",
        "with open(trainClaimsFile, 'r') as file:\n",
        "    trainClaims=json.load(file)\n",
        "with open(devClaimsFile, 'r') as file:\n",
        "    devClaims=json.load(file)\n",
        "with open(evidenceFile, 'r') as file:\n",
        "    evidenceData=json.load(file)\n",
        "\n",
        "## Preprocessing data -- lowercase, tokenize, and stopword removal\n",
        "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
        "tokenizer=get_tokenizer('basic_english')\n",
        "punctuations=string.punctuation\n",
        "\n",
        "def preprocess(text):\n",
        "    token=tokenizer(text.lower())\n",
        "    cleanedTokens=[t for t in token if (t not in stopwords) and (t not in punctuations)]\n",
        "    return ' '.join(cleanedTokens)\n",
        "\n",
        "for ids, texts in evidenceData.items():\n",
        "    evidenceData[ids]=preprocess(texts)\n",
        "\n",
        "# Function to create DataFrame and merge evidence IDs with Text\n",
        "def createDF(claims, evidence):\n",
        "    combinedData=[]\n",
        "    for claimID, claimText in claims.items():\n",
        "        # Combine the ID with its corresponding evidences\n",
        "        evidenceID=claimText['evidences']\n",
        "        evidenceText=(evidence[i] for i in evidenceID if i in evidence)\n",
        "        combinedData.append({\n",
        "            'claim_id': claimID,\n",
        "            'claim_text': preprocess(claimText['claim_text']),\n",
        "            'evidence_id': evidenceID,\n",
        "            'evidence_text': \" \".join(evidenceText),\n",
        "            'claim_label': claimText['claim_label']\n",
        "        })\n",
        "    # Create DataFrame\n",
        "    return pd.DataFrame(combinedData)\n",
        "\n",
        "# Create CSV Files\n",
        "trainFullMerged=createDF(trainClaims,evidenceData)\n",
        "devFullMerged=createDF(devClaims,evidenceData)\n",
        "trainFullMerged.to_csv(\"data/trainFullMerged.csv\", index=False)\n",
        "devFullMerged.to_csv(\"data/devFullMerged.csv\", index=False)\n",
        "\n",
        "# Convert evidence into csv as well\n",
        "# evidenceFinal=pd.DataFrame(list(evidenceData.items()),columns=['evidence_id','evidence_text'])\n",
        "# evidenceFinal.to_csv('data/evidencePreprocessed.csv',index=False)\n",
        "\n",
        "# Convert unlabelled Data into CSV as well\n",
        "with open(testFile, 'r') as file:\n",
        "    testData=json.load(file)\n",
        "for ids, texts in testData.items():\n",
        "    claim_text = texts['claim_text']\n",
        "    testData[ids]=preprocess(claim_text)\n",
        "testFinal=pd.DataFrame(list(testData.items()), columns=['claim_id', 'claim_text'])\n",
        "testFinal.to_csv('data/testPreprocessed.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "## use this for model training\n",
        "trainClaimsFile='./data/trainFullMerged.csv'\n",
        "## use this set for hyperparameter tuning and evaluation metric \n",
        "devClaimsFile='./data/devFullMerged.csv'\n",
        "## evidence files need to be downloaded through googledrive (https://drive.google.com/file/d/1OyihwdAWfqHIOueCB4bLBkYg4hTN_OKm/view?usp=sharing)\n",
        "evidenceFile='./data/evidencePreprocessed.csv'\n",
        "## test unlabelled dataset\n",
        "testFile='./data/testPreprocessed.csv'\n",
        "\n",
        "trainDataframe=pd.read_csv(trainClaimsFile)\n",
        "devDataframe=pd.read_csv(devClaimsFile)\n",
        "evidenceDataframe=pd.read_csv(evidenceFile)\n",
        "testDataframe=pd.read_csv(testFile)\n",
        "\n",
        "trainDataframe['claim_text']=trainDataframe['claim_text']\n",
        "#trainDataframe['evidence_id']=trainDataframe['evidence_id'].astype(str).str.strip('[]').str.strip(\"'\").apply(preprocess)\n",
        "#trainDataframe['combined_evidence']=trainDataframe['evidence_id']+\" \"+trainDataframe['evidence_text']\n",
        "\n",
        "devDataframe['claim_text']=devDataframe['claim_text']\n",
        "#devDataframe['evidence_id']=devDataframe['evidence_id'].astype(str).str.strip('[]').str.strip(\"'\").apply(preprocess)\n",
        "#devDataframe['combined_evidence']=devDataframe['evidence_id'] +\" \"+ devDataframe['evidence_text']\n",
        "\n",
        "#evidenceDataframe['combined_evidence']=evidenceDataframe['evidence_id']+\" \"+evidenceDataframe['evidence_text']\n",
        "#evidenceDataframe['combined_evidence']=evidenceDataframe['combined_evidence'].astype(str).str.strip(\"'\").apply(preprocess)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Preprocessing data -- lowercase, tokenize, and stopword removal\n",
        "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
        "tokenizer=get_tokenizer('basic_english')\n",
        "punctuations=set(string.punctuation)\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens=tokenizer(text)\n",
        "    cleanedTokens = [token for token in tokens if token not in stopwords and token not in punctuations]\n",
        "    return cleanedTokens\n",
        "\n",
        "\n",
        "## this one is used to get the prediction words directly\n",
        "TEXT = Field(tokenize=preprocess, lower=True, init_token='<sos>', eos_token='<eos>', batch_first=False)\n",
        "LABEL=LabelField(batch_first=False)\n",
        "def createDatasetEncoderInput(dataframe,textTransform,labelTransform):\n",
        "    field=[('reviewTextInput',textTransform),('evidenceTextOutput',labelTransform)]\n",
        "    examples=[]\n",
        "    for _, row in dataframe.iterrows():\n",
        "        reviewTextInput=row['claim_text']\n",
        "        evidenceTextOutput=row['evidence_id']\n",
        "        examples.append(Example.fromlist([reviewTextInput,evidenceTextOutput], field))\n",
        "    return Dataset(examples,fields=field)\n",
        "\n",
        "def createDatasetOutput(dataframe,textTransform,labelTransform):\n",
        "    field=[('evidenceTextOutput',labelTransform)]\n",
        "    examples=[]\n",
        "    for _, row in dataframe.iterrows():\n",
        "        evidenceTextOutput=row['evidence_id']\n",
        "        examples.append(Example.fromlist([evidenceTextOutput], field))\n",
        "    return Dataset(examples,fields=field)\n",
        "\n",
        "def createDatasetTest(dataframe,textTransform):\n",
        "    field=[('reviewTextInput',textTransform)]\n",
        "    examples=[]\n",
        "    for _, row in dataframe.iterrows():\n",
        "        reviewTextInput=row['claim_text']\n",
        "        examples.append(Example.fromlist([reviewTextInput], field))\n",
        "    return Dataset(examples,fields=field)\n",
        "\n",
        "## remove LABEL if you want to get the text as predictions instead of ids\n",
        "trainTensor = createDatasetEncoderInput(trainDataframe, TEXT,LABEL)\n",
        "devTensor = createDatasetEncoderInput(devDataframe, TEXT,LABEL)\n",
        "evidenceTensor=createDatasetOutput(evidenceDataframe,TEXT,LABEL)\n",
        "testTensor=createDatasetTest(testDataframe,TEXT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "TEXT.build_vocab(trainTensor)\n",
        "LABEL.build_vocab(evidenceTensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review Text: ['scientific', 'evidence', 'co2', 'pollutant', 'higher', 'co2', 'concentrations', 'actually', 'help', 'ecosystems', 'support', 'plant', 'animal', 'life']\n",
            "Evidence Text: ['evidence-442946', 'evidence-1194317', 'evidence-12171']\n",
            "\n",
            "Review Text: ['el', 'niño', 'drove', 'record', 'highs', 'global', 'temperatures', 'suggesting', 'rise', 'may', 'man-made', 'emissions']\n",
            "Evidence Text: ['evidence-338219', 'evidence-1127398']\n",
            "\n",
            "Review Text: ['1946', 'pdo', 'switched', 'cool', 'phase']\n",
            "Evidence Text: ['evidence-530063', 'evidence-984887']\n",
            "\n",
            "Review Text: ['weather', 'channel', 'co-founder', 'john', 'coleman', 'provided', 'evidence', 'convincingly', 'refutes', 'concept', 'anthropogenic', 'global', 'warming']\n",
            "Evidence Text: ['evidence-1177431', 'evidence-782448', 'evidence-540069', 'evidence-352655', 'evidence-1007867']\n",
            "\n",
            "Review Text: ['january', '2008', 'capped', '12', 'month', 'period', 'global', 'temperature', 'drops', 'major', 'well', 'respected', 'indicators']\n",
            "Evidence Text: ['evidence-1010750', 'evidence-91661', 'evidence-722725', 'evidence-554161', 'evidence-430839']\n",
            "\n",
            "{'reviewTextInput': ['scientific', 'evidence', 'co2', 'pollutant', 'higher', 'co2', 'concentrations', 'actually', 'help', 'ecosystems', 'support', 'plant', 'animal', 'life'], 'evidenceTextOutput': \"['evidence-442946', 'evidence-1194317', 'evidence-12171']\"}\n",
            "{'reviewTextInput': ['el', 'niño', 'drove', 'record', 'highs', 'global', 'temperatures', 'suggesting', 'rise', 'may', 'man-made', 'emissions'], 'evidenceTextOutput': \"['evidence-338219', 'evidence-1127398']\"}\n",
            "{'reviewTextInput': ['1946', 'pdo', 'switched', 'cool', 'phase'], 'evidenceTextOutput': \"['evidence-530063', 'evidence-984887']\"}\n",
            "{'reviewTextInput': ['weather', 'channel', 'co-founder', 'john', 'coleman', 'provided', 'evidence', 'convincingly', 'refutes', 'concept', 'anthropogenic', 'global', 'warming'], 'evidenceTextOutput': \"['evidence-1177431', 'evidence-782448', 'evidence-540069', 'evidence-352655', 'evidence-1007867']\"}\n",
            "{'reviewTextInput': ['january', '2008', 'capped', '12', 'month', 'period', 'global', 'temperature', 'drops', 'major', 'well', 'respected', 'indicators'], 'evidenceTextOutput': \"['evidence-1010750', 'evidence-91661', 'evidence-722725', 'evidence-554161', 'evidence-430839']\"}\n"
          ]
        }
      ],
      "source": [
        "for example in trainTensor.examples[:5]:\n",
        "    print(\"Review Text:\", example.reviewTextInput)\n",
        "    print(\"Evidence Text:\", example.evidenceTextOutput)\n",
        "    print()\n",
        "    \n",
        "for example in trainTensor.examples[:5]:\n",
        "    print(vars(example))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'reviewTextInput': ['scientific', 'evidence', 'co2', 'pollutant', 'higher', 'co2', 'concentrations', 'actually', 'help', 'ecosystems', 'support', 'plant', 'animal', 'life'], 'evidenceTextOutput': \"['evidence-442946', 'evidence-1194317', 'evidence-12171']\"}\n",
            "{'reviewTextInput': ['el', 'niño', 'drove', 'record', 'highs', 'global', 'temperatures', 'suggesting', 'rise', 'may', 'man-made', 'emissions'], 'evidenceTextOutput': \"['evidence-338219', 'evidence-1127398']\"}\n",
            "{'reviewTextInput': ['1946', 'pdo', 'switched', 'cool', 'phase'], 'evidenceTextOutput': \"['evidence-530063', 'evidence-984887']\"}\n",
            "{'reviewTextInput': ['weather', 'channel', 'co-founder', 'john', 'coleman', 'provided', 'evidence', 'convincingly', 'refutes', 'concept', 'anthropogenic', 'global', 'warming'], 'evidenceTextOutput': \"['evidence-1177431', 'evidence-782448', 'evidence-540069', 'evidence-352655', 'evidence-1007867']\"}\n",
            "{'reviewTextInput': ['january', '2008', 'capped', '12', 'month', 'period', 'global', 'temperature', 'drops', 'major', 'well', 'respected', 'indicators'], 'evidenceTextOutput': \"['evidence-1010750', 'evidence-91661', 'evidence-722725', 'evidence-554161', 'evidence-430839']\"}\n",
            "1\n",
            "1228\n",
            "1\n",
            "154\n"
          ]
        }
      ],
      "source": [
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def preprocess_evidence_ids(evidence_id_str):\n",
        "    # Strip brackets and split by comma\n",
        "    evidence_ids = evidence_id_str.strip(\"[]\").replace(\"'\", \"\").split(', ')\n",
        "    return evidence_ids\n",
        "\n",
        "def batchify(dataset, bsz):\n",
        "    # Process text and labels to tensors\n",
        "    data = TEXT.process([getattr(x, 'reviewTextInput') for x in dataset.examples])\n",
        "    label = [preprocess_evidence_ids(getattr(x, 'evidenceTextOutput')) for x in dataset.examples]\n",
        "    label_temp = [item for sublist in label for item in sublist]\n",
        "    label = LABEL.process(label_temp)\n",
        "    # Calculate number of complete batches\n",
        "    nbatch = min(data.size(0), label.size(0)) // bsz\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    label = label.narrow(0, 0, nbatch * bsz)\n",
        "\n",
        "    # Reshape data into batches\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    label = label.view(bsz, -1).t().contiguous()\n",
        "    \n",
        "    # Ensure each has the same number of sequences\n",
        "    min_seq_len = min(data.size(0), label.size(0))\n",
        "    data = data[:min_seq_len]\n",
        "    label = label[:min_seq_len]\n",
        "    print(len(data))\n",
        "    print(len(dataset))\n",
        "    return data.to(device), label.to(device)\n",
        "\n",
        "TEST = Field(tokenize=preprocess, lower=True, init_token='<sos>', eos_token='<eos>', batch_first=True)\n",
        "TEST.build_vocab(trainTensor)\n",
        "def processTextOnly(dataset):\n",
        "    data = TEST.process([getattr(x, 'reviewTextInput') for x in dataset.examples])\n",
        "    return data.to(device)\n",
        "\n",
        "# Batch sizes\n",
        "batch_size = 20\n",
        "eval_batch_size = 35\n",
        "\n",
        "# Process the datasets\n",
        "for example in trainTensor.examples[:5]:\n",
        "    print(vars(example))\n",
        "        \n",
        "train_data,train_label = batchify(trainTensor, batch_size)\n",
        "dev_data,dev_label = batchify(devTensor, eval_batch_size)\n",
        "test_data=processTextOnly(testTensor)\n",
        "dev_accuracy_data=processTextOnly(devTensor)\n",
        "\n",
        "bptt = 35\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x0000023A13B2BDF0>>, {'<unk>': 0, '<pad>': 1, '<sos>': 2, '<eos>': 3, 'warming': 4, 'climate': 5, 'global': 6, 'co2': 7, 'ice': 8, 'change': 9, 'temperature': 10, 'sea': 11, 'years': 12, 'temperatures': 13, 'carbon': 14, 'scientists': 15, 'rise': 16, 'emissions': 17, 'level': 18, 'record': 19, 'greenhouse': 20, 'since': 21, 'dioxide': 22, 'levels': 23, 'ocean': 24, 'earth': 25, 'past': 26, 'could': 27, 'data': 28, 'century': 29, 'would': 30, 'year': 31, 'solar': 32, 'arctic': 33, 'increase': 34, 'much': 35, 'atmosphere': 36, 'energy': 37, 'evidence': 38, 'trend': 39, '”': 40, 'heat': 41, 'natural': 42, 'surface': 43, 'water': 44, 'human': 45, 'ipcc': 46, 'last': 47, 'planet': 48, 'effect': 49, 'models': 50, 'said': 51, '1': 52, 'degrees': 53, 'per': 54, 'show': 55, '0': 56, 'atmospheric': 57, 'caused': 58, 'cooling': 59, 'gas': 60, 'one': 61, 'due': 62, 'less': 63, 'recent': 64, 'weather': 65, 'also': 66, 'cause': 67, 'humans': 68, 'likely': 69, 'may': 70, '000': 71, 'new': 72, 'rising': 73, 'time': 74, 'world': 75, 'increasing': 76, 'warmer': 77, 'even': 78, '2': 79, 'decades': 80, 'events': 81, 'extreme': 82, 'little': 83, 'many': 84, 'percent': 85, 'period': 86, 'polar': 87, 'predicted': 88, 'warm': 89, '—': 90, 'around': 91, 'average': 92, 'causing': 93, 'changes': 94, 'found': 95, 'increased': 96, 'times': 97, 'amount': 98, 'greenland': 99, 'high': 100, 'sun': 101, 'gases': 102, 'measurements': 103, 'scientific': 104, 'study': 105, 'two': 106, 'u': 107, 'antarctica': 108, 'far': 109, 'half': 110, 'long': 111, 'research': 112, 'satellite': 113, 'shows': 114, 'activity': 115, 'cold': 116, 'rate': 117, 'states': 118, 'united': 119, '30': 120, 'cent': 121, 'fact': 122, 'great': 123, 'higher': 124, 'impact': 125, 'oceans': 126, 'science': 127, 'studies': 128, 'today': 129, 'ago': 130, 'cycle': 131, 'every': 132, 'fossil': 133, 'major': 134, 'make': 135, 'observed': 136, 'report': 137, 'stations': 138, 'summer': 139, 'according': 140, 'air': 141, 'antarctic': 142, 'million': 143, 'published': 144, 'say': 145, 'significant': 146, '10': 147, '20th': 148, 'accelerating': 149, 'celsius': 150, 'coming': 151, 'el': 152, 'land': 153, 'like': 154, 'low': 155, 'man-made': 156, 'mean': 157, 'melting': 158, 'might': 159, '–': 160, '“the': 161, 'earth’s': 162, 'enough': 163, 'feet': 164, 'glaciers': 165, 'influence': 166, 'obama': 167, 'pollution': 168, 'power': 169, 'us': 170, 'age': 171, 'become': 172, 'coral': 173, 'degree': 174, 'fuels': 175, 'going': 176, 'however': 177, 'loss': 178, 'lower': 179, 'means': 180, 'nature': 181, 'peer-reviewed': 182, 'predictions': 183, 'records': 184, 'scientist': 185, 'term': 186, 'three': 187, 'trends': 188, 'used': 189, 'yet': 190, '’': 191, 'based': 192, 'current': 193, 'decade': 194, 'early': 195, 'ever': 196, 'feedback': 197, 'floods': 198, 'forests': 199, 'future': 200, 'long-term': 201, 'methane': 202, 'model': 203, 'nasa': 204, 'sea-level': 205, 'several': 206, 'sheet': 207, 'small': 208, 'total': 209, 'vapor': 210, 'warmest': 211, 'worse': 212, 'actually': 213, 'clear': 214, 'consensus': 215, 'decline': 216, 'end': 217, 'fahrenheit': 218, 'find': 219, 'first': 220, 'five': 221, 'including': 222, 'increases': 223, 'life': 224, 'made': 225, 'mass': 226, 'nearly': 227, 'need': 228, 'never': 229, 'number': 230, 'numbers': 231, 'occurred': 232, 'parts': 233, 'people': 234, 'periods': 235, 'primary': 236, 'rapid': 237, 'recently': 238, 'regions': 239, 'result': 240, 'see': 241, 'seen': 242, 'shown': 243, 'system': 244, 'well': 245, 'without': 246, '100': 247, 'account': 248, 'area': 249, 'areas': 250, 'claims': 251, 'cosmic': 252, 'cost': 253, 'costs': 254, 'currently': 255, 'cycles': 256, 'days': 257, 'despite': 258, 'different': 259, 'drop': 260, 'fast': 261, 'finds': 262, 'growing': 263, 'independent': 264, 'it’s': 265, 'jobs': 266, 'known': 267, 'large': 268, 'national': 269, 'niño': 270, 'northern': 271, 'pacific': 272, 'panel': 273, 'part': 274, 'radiation': 275, 'reef': 276, 'relatively': 277, 'seem': 278, 'set': 279, 'southern': 280, 'think': 281, 'university': 282, 'wet': 283, 'wind': 284, '5': 285, '7': 286, 'across': 287, 'appears': 288, 'australia': 289, 'balance': 290, 'bears': 291, 'causes': 292, 'coal': 293, 'confirmed': 294, 'cooler': 295, 'correlation': 296, 'dry': 297, 'effects': 298, 'empirical': 299, 'entire': 300, 'experts': 301, 'factors': 302, 'get': 303, 'health': 304, 'leading': 305, 'nothing': 306, 'papers': 307, 'rays': 308, 'six': 309, 'still': 310, 'strong': 311, 'suggests': 312, 'tax': 313, 'theory': 314, 'way': 315, 'winter': 316, '15': 317, '20': 318, '2007': 319, '2016': 320, '50': 321, '8': 322, 'add': 323, 'agree': 324, 'al': 325, 'already': 326, 'analysis': 327, 'barrier': 328, 'becoming': 329, 'began': 330, 'burning': 331, 'case': 332, 'changing': 333, 'cloud': 334, 'clouds': 335, 'coastal': 336, 'concentration': 337, 'cru': 338, 'droughts': 339, 'economic': 340, 'estimate': 341, 'extent': 342, 'fall': 343, 'gone': 344, 'good': 345, 'happen': 346, 'heating': 347, 'hemisphere': 348, 'industry': 349, 'intense': 350, 'least': 351, 'losing': 352, 'main': 353, 'melt': 354, 'month': 355, 'next': 356, 'observations': 357, 'occur': 358, 'occurring': 359, 'oil': 360, 'phase': 361, 'population': 362, 'positive': 363, 'problem': 364, 'put': 365, 'rather': 366, 'release': 367, 'renewable': 368, 'responsible': 369, 'satellites': 370, 'sheets': 371, 'slightly': 372, 'use': 373, 'variability': 374, '1970': 375, '2000': 376, '2010': 377, '2017': 378, '400': 379, '6': 380, 'absorbed': 381, 'acidification': 382, 'action': 383, 'administration': 384, 'agreement': 385, 'alaska': 386, 'albedo': 387, 'america': 388, 'another': 389, 'anthropogenic': 390, 'back': 391, 'bear': 392, 'cannot': 393, 'clean': 394, 'colder': 395, 'compared': 396, 'conditions': 397, 'content': 398, 'countries': 399, 'cover': 400, 'damage': 401, 'decreasing': 402, 'difference': 403, 'doubt': 404, 'dr': 405, 'dropped': 406, 'drought': 407, 'expect': 408, 'expected': 409, 'extremely': 410, 'faster': 411, 'followed': 412, 'forcing': 413, 'globally': 414, 'government': 415, 'hundreds': 416, 'hurricane': 417, 'hurricanes': 418, 'intergovernmental': 419, 'largely': 420, 'larger': 421, 'lead': 422, 'linked': 423, 'makes': 424, 'making': 425, 'marine': 426, 'medieval': 427, 'near': 428, 'negative': 429, 'noaa': 430, 'normal': 431, 'often': 432, 'orbit': 433, 'oxygen': 434, 'paper': 435, 'petition': 436, 'plants': 437, 'point': 438, 'precipitation': 439, 'present': 440, 'process': 441, 'rapidly': 442, 'response': 443, 'review': 444, 'risen': 445, 'says': 446, 'scale': 447, 'showing': 448, 'simply': 449, 'sources': 450, 'south': 451, 'stable': 452, 'suggest': 453, 'there’s': 454, 'though': 455, 'thousands': 456, 'threat': 457, 'volcanic': 458, 'we’re': 459, 'we’ve': 460, 'world’s': 461, 'wrong': 462, '1990': 463, '1998': 464, '2013': 465, '2015': 466, '2100': 467, '3': 468, '35': 469, '4': 470, '500': 471, '95': 472, 'absorb': 473, 'actual': 474, 'aerosols': 475, 'affect': 476, 'almost': 477, 'american': 478, 'away': 479, 'billion': 480, 'called': 481, 'catastrophic': 482, 'central': 483, 'certainly': 484, 'claim': 485, 'claimed': 486, 'close': 487, 'coast': 488, 'computer': 489, 'cool': 490, 'damages': 491, 'day': 492, 'deaths': 493, 'declined': 494, 'dominant': 495, 'driven': 496, 'electricity': 497, 'ended': 498, 'europe': 499, 'flow': 500, 'forces': 501, 'forcings': 502, 'frequent': 503, 'greener': 504, 'grow': 505, 'gulf': 506, 'heatwaves': 507, 'hot': 508, 'hottest': 509, 'human-caused': 510, 'january': 511, 'jet': 512, 'journal': 513, 'late': 514, 'link': 515, 'measured': 516, 'north': 517, 'paris': 518, 'pdo': 519, 'peer': 520, 'planetary': 521, 'ppm': 522, 'predict': 523, 'previously': 524, 'price': 525, 'provide': 526, 'public': 527, 'recorded': 528, 'reduce': 529, 'reduction': 530, 'region': 531, 'reports': 532, 'researchers': 533, 'role': 534, 'seeing': 535, 'sensitivity': 536, 'showed': 537, 'signed': 538, 'slow': 539, 'smaller': 540, 'source': 541, 'space': 542, 'state': 543, 'statement': 544, 'stream': 545, 'take': 546, 'tell': 547, 'throughout': 548, 'trump': 549, 'unprecedented': 550, 'urban': 551, 'vapour': 552, 'variations': 553, 'vulnerable': 554, 'waters': 555, 'west': 556, 'western': 557, 'work': 558, '’”': 559, '11': 560, '1880': 561, '1940': 562, '1970s': 563, '1979': 564, '[…]': 565, 'absorbing': 566, 'along': 567, 'amounts': 568, 'april': 569, 'assessment': 570, 'associated': 571, 'bad': 572, 'big': 573, 'bill': 574, 'bleaching': 575, 'business': 576, 'changed': 577, 'clearly': 578, 'collapse': 579, 'consistent': 580, 'continue': 581, 'continued': 582, 'dataset': 583, 'determined': 584, 'direct': 585, 'doubling': 586, 'dramatically': 587, 'ecosystems': 588, 'environmental': 589, 'et': 590, 'event': 591, 'example': 592, 'existence': 593, 'extinction': 594, 'february': 595, 'feedbacks': 596, 'flooding': 597, 'florida': 598, 'food': 599, 'four': 600, 'frequency': 601, 'fuel': 602, 'full': 603, 'gdp': 604, 'geological': 605, 'germany': 606, 'getting': 607, 'got': 608, 'growth': 609, 'hansen': 610, 'hotter': 611, 'house': 612, 'humanity': 613, 'impacts': 614, 'important': 615, 'inches': 616, 'internal': 617, 'islands': 618, 'isn’t': 619, 'jones': 620, 'june': 621, 'kill': 622, 'kills': 623, 'la': 624, 'lakes': 625, 'line': 626, 'lines': 627, 'literature': 628, 'longer': 629, 'look': 630, 'mainly': 631, 'metres': 632, 'middle': 633, 'months': 634, 'mostly': 635, 'multiple': 636, 'must': 637, 'nation': 638, 'nations': 639, 'net': 640, 'news': 641, 'overall': 642, 'particularly': 643, 'permafrost': 644, 'phil': 645, 'place': 646, 'plan': 647, 'plant': 648, 'points': 649, 'pollutant': 650, 'president': 651, 'probably': 652, 'project': 653, 'quite': 654, 'raise': 655, 'reality': 656, 'really': 657, 'reduced': 658, 'reliable': 659, 'risk': 660, 'roughly': 661, 'saying': 662, 'short': 663, 'significantly': 664, 'similar': 665, 'simple': 666, 'single': 667, 'snow': 668, 'statistical': 669, 'sunlight': 670, 'thought': 671, 'thus': 672, 'told': 673, 'top': 674, 'tropical': 675, 'troposphere': 676, 'true': 677, 'unusually': 678, 'upper': 679, 'using': 680, 'warmed': 681, 'warms': 682, 'waves': 683, 'whether': 684, 'whole': 685, 'within': 686, 'working': 687, '18': 688, '1980s': 689, '1988': 690, '1995': 691, '1997': 692, '2009': 693, '2012': 694, '2018': 695, '31': 696, '80': 697, '9': 698, '97': 699, '[the': 700, 'able': 701, 'accelerated': 702, 'acidity': 703, 'actions': 704, 'adding': 705, 'additional': 706, 'ages': 707, 'americans': 708, 'anything': 709, 'approximately': 710, 'article': 711, 'aspects': 712, 'atlantic': 713, 'atolls': 714, 'australian': 715, 'averaged': 716, 'beginning': 717, 'believe': 718, 'beneficial': 719, 'california': 720, 'caps': 721, 'cars': 722, 'centuries': 723, 'cheap': 724, 'cities': 725, 'common': 726, 'confidence': 727, 'confirm': 728, 'connection': 729, 'consequence': 730, 'consequences': 731, 'consistently': 732, 'continent': 733, 'contribute': 734, 'december': 735, 'decrease': 736, 'depend': 737, 'disappear': 738, 'disasters': 739, 'disease': 740, 'disproven': 741, 'don’t': 742, 'dramatic': 743, 'drive': 744, 'earlier': 745, 'economy': 746, 'email': 747, 'emails': 748, 'emitting': 749, 'endorse': 750, 'environment': 751, 'error': 752, 'errors': 753, 'eruptions': 754, 'especially': 755, 'exactly': 756, 'expert': 757, 'explain': 758, 'factor': 759, 'failed': 760, 'federal': 761, 'fewer': 762, 'field': 763, 'forest': 764, 'gets': 765, 'glacial': 766, 'graphs': 767, 'greater': 768, 'green': 769, 'group': 770, 'happening': 771, 'heatwave': 772, 'help': 773, 'hence': 774, 'highest': 775, 'idea': 776, 'indicate': 777, 'industrial': 778, 'intensity': 779, 'ipcc’s': 780, 'island': 781, 'john': 782, 'july': 783, 'jupiter': 784, 'latitudes': 785, 'limit': 786, 'lows': 787, 'mars': 788, 'massive': 789, 'measurement': 790, 'minimum': 791, 'minor': 792, 'mwp': 793, 'nino': 794, 'numerous': 795, 'occurs': 796, 'oceanic': 797, 'open': 798, 'original': 799, 'overestimated': 800, 'ozone': 801, 'pattern': 802, 'ph': 803, 'planet’s': 804, 'pluto': 805, 'possibility': 806, 'possible': 807, 'potential': 808, 'potentially': 809, 'powerful': 810, 'preventing': 811, 'problems': 812, 'produce': 813, 'professor': 814, 'projections': 815, 'provides': 816, 'pushed': 817, 'readings': 818, 'reducing': 819, 'related': 820, 'renewables': 821, 'reported': 822, 'requires': 823, 'results': 824, 'rises': 825, 'rural': 826, 'saw': 827, 'security': 828, 'seems': 829, 'serious': 830, 'severe': 831, 'short-term': 832, 'shrinking': 833, 'side': 834, 'sinking': 835, 'slight': 836, 'soon': 837, 'sort': 838, 'species': 839, 'start': 840, 'steadily': 841, 'stopped': 842, 'storm': 843, 'storms': 844, 'surrounding': 845, 'taken': 846, 'test': 847, 'thermal': 848, 'thermometers': 849, 'thing': 850, 'tidal': 851, 'together': 852, 'took': 853, 'towards': 854, 'transition': 855, 'trying': 856, 'turn': 857, 'twice': 858, 'underestimate': 859, 'understanding': 860, 'unless': 861, 'variety': 862, 'wave': 863, 'week': 864, 'winds': 865, 'worsening': 866, '…': 867, '--': 868, '1000': 869, '1950': 870, '1950s': 871, '1960': 872, '1980': 873, '200': 874, '2003': 875, '2005': 876, '21st': 877, '2°c': 878, '300': 879, '40': 880, '60': 881, '60-year': 882, '90%': 883, '97%': 884, '[of': 885, 'accelerate': 886, 'acid': 887, 'act': 888, 'activities': 889, 'adapt': 890, 'agency': 891, 'alarmist': 892, 'allowed': 893, 'alter': 894, 'although': 895, 'amazon': 896, 'among': 897, 'analyses': 898, 'ancient': 899, 'and/or': 900, 'annual': 901, 'antarctica’s': 902, 'anyone': 903, 'apart': 904, 'appear': 905, 'approach': 906, 'argued': 907, 'asia': 908, 'attribution': 909, 'authors': 910, 'barack': 911, 'baseload': 912, 'bats': 913, 'becomes': 914, 'ben': 915, 'benefit': 916, 'biggest': 917, 'biodiversity': 918, 'blame': 919, 'bottom': 920, 'bring': 921, 'brown': 922, 'call': 923, 'came': 924, 'certain': 925, 'challenges': 926, 'china': 927, 'christy': 928, 'circulation': 929, 'cited': 930, 'civilization': 931, 'climates': 932, 'coalition': 933, 'come': 934, 'comes': 935, 'commercial': 936, 'companies': 937, 'concentrations': 938, 'concluded': 939, 'concludes': 940, 'conclusion': 941, 'context': 942, 'continues': 943, 'contributing': 944, 'contributor': 945, 'control': 946, 'corals': 947, 'correct': 948, 'council': 949, 'covered': 950, 'create': 951, 'critical': 952, 'cuts': 953, 'daily': 954, 'dangerous': 955, 'debate': 956, 'declining': 957, 'demonstrate': 958, 'details': 959, 'die': 960, 'directly': 961, 'discussed': 962, 'donald': 963, 'done': 964, 'downward': 965, 'drastically': 966, 'drier': 967, 'driver': 968, 'driving': 969, 'dynamic': 970, 'east': 971, 'economics': 972, 'economists': 973, 'ecs': 974, 'eg': 975, 'eight': 976, 'either': 977, 'eliminate': 978, 'emit': 979, 'employed': 980, 'ending': 981, 'enhanced': 982, 'equal': 983, 'equator': 984, 'escaping': 985, 'estimated': 986, 'estimates': 987, 'european': 988, 'evening': 989, 'exact': 990, 'except': 991, 'expansion': 992, 'experienced': 993, 'experiencing': 994, 'explained': 995, 'f': 996, 'fail': 997, 'fairly': 998, 'falls': 999, 'families': 1000, 'fields': 1001, 'figure': 1002, 'fingerprints': 1003, 'flawed': 1004, 'forecasts': 1005, 'forms': 1006, 'fraction': 1007, 'fully': 1008, 'gain': 1009, 'geoscience': 1010, 'gigatonnes': 1011, 'globe': 1012, 'go': 1013, 'gore': 1014, 'ground': 1015, 'happened': 1016, 'harvey': 1017, 'head': 1018, 'heading': 1019, 'highs': 1020, 'himalayan': 1021, 'history': 1022, 'hoax': 1023, 'hockey': 1024, 'holds': 1025, 'huge': 1026, 'humidity': 1027, 'hypothesis': 1028, 'ice-mass': 1029, 'indicators': 1030, 'infrared': 1031, 'instrumental': 1032, 'interesting': 1033, 'international': 1034, 'investment': 1035, 'james': 1036, 'keep': 1037, 'kept': 1038, 'killed': 1039, 'know': 1040, 'lack': 1041, 'largest': 1042, 'law': 1043, 'layer': 1044, 'leads': 1045, 'limits': 1046, 'loa': 1047, 'local': 1048, 'looking': 1049, 'lose': 1050, 'mann': 1051, 'material': 1052, 'mauna': 1053, 'maximum': 1054, 'maybe': 1055, 'message': 1056, 'met': 1057, 'methods': 1058, 'mid-century': 1059, 'mike': 1060, 'miles': 1061, 'millions': 1062, 'moisture': 1063, 'money': 1064, 'mount': 1065, 'myth': 1066, 'necessary': 1067, 'needs': 1068, 'network': 1069, 'nine': 1070, 'nitrogen': 1071, 'none': 1072, 'noticed': 1073, 'october': 1074, 'oism': 1075, 'old': 1076, 'openly': 1077, 'oscillation': 1078, 'output': 1079, 'overpeck': 1080, 'pace': 1081, 'panels': 1082, 'partly': 1083, 'passenger': 1084, 'patterns': 1085, 'pay': 1086, 'physics': 1087, 'places': 1088, 'poles': 1089, 'policy': 1090, 'polluters': 1091, 'poorly': 1092, 'position': 1093, 'possibly': 1094, 'postma': 1095, 'pre-industrial': 1096, 'previous': 1097, 'primarily': 1098, 'probability': 1099, 'proof': 1100, 'protection': 1101, 'prove': 1102, 'proxies': 1103, 'pumped': 1104, 'pushing': 1105, 'question': 1106, 'radiative': 1107, 'rain': 1108, 'rainforests': 1109, 'rates': 1110, 'real': 1111, 'reconstruction': 1112, 'reconstructions': 1113, 'regional': 1114, 'relationship': 1115, 'remained': 1116, 'removed': 1117, 'represent': 1118, 'required': 1119, 'requirements': 1120, 'residents': 1121, 'resulting': 1122, 'revolution': 1123, 'rivers': 1124, 'runaway': 1125, 'running': 1126, 'santer': 1127, 'scandal': 1128, 'scenarios': 1129, 'season': 1130, 'second': 1131, 'sen': 1132, 'sensitive': 1133, 'september': 1134, 'sets': 1135, 'shattering': 1136, 'shelf': 1137, 'sinks': 1138, 'situation': 1139, 'skeptics': 1140, 'slowly': 1141, 'spells': 1142, 'spring': 1143, 'square': 1144, 'station': 1145, 'stick': 1146, 'stomata': 1147, 'strongly': 1148, 'subsequent': 1149, 'substantially': 1150, 'sunspots': 1151, 'supporting': 1152, 'switched': 1153, 'tens': 1154, 'texas': 1155, 'therefore': 1156, 'thermodynamics': 1157, 'today’s': 1158, 'toward': 1159, 'tree-ring': 1160, 'tree-rings': 1161, 'triggered': 1162, 'triggering': 1163, 'twentieth': 1164, 'typically': 1165, 'un': 1166, 'unable': 1167, 'understand': 1168, 'understood': 1169, 'variable': 1170, 'vegetation': 1171, 'venus': 1172, 'virginia': 1173, 'volcanoes': 1174, 'voted': 1175, 'warmth': 1176, 'warning': 1177, 'we’d': 1178, 'wildfires': 1179, 'worldwide': 1180, 'worst': 1181, 'york': 1182, '“a': 1183, '“global': 1184, '“in': 1185, '$1': 1186, '05': 1187, '12': 1188, '120': 1189, '13': 1190, '14': 1191, '17': 1192, '1800s': 1193, '1850': 1194, '19': 1195, '1900': 1196, '1920s': 1197, '1975': 1198, '1977': 1199, '1999': 1200, '19th': 1201, '2001': 1202, '2004': 1203, '2008': 1204, '2011': 1205, '2014': 1206, '2030': 1207, '2035': 1208, '23': 1209, '250': 1210, '26': 1211, '29': 1212, '2nd': 1213, '3°c': 1214, '44': 1215, '58': 1216, '60%': 1217, '7%': 1218, '700': 1219, '90': 1220, '95%': 1221, '[climate': 1222, '[t]he': 1223, 'ability': 1224, 'abnormal': 1225, 'abruptly': 1226, 'absence': 1227, 'absorption': 1228, 'acceleration': 1229, 'accounting': 1230, 'accounts': 1231, 'accurate': 1232, 'accurately': 1233, 'achieve': 1234, 'acidic': 1235, 'acknowledged': 1236, 'acts': 1237, 'adjusted': 1238, 'adjusting': 1239, 'affected': 1240, 'agrees': 1241, 'ahead': 1242, 'aircraft': 1243, 'alarming': 1244, 'alarmists': 1245, 'alone': 1246, 'alternative': 1247, 'altitudes': 1248, 'ambitious': 1249, 'anomaly': 1250, 'anymore': 1251, 'anyway': 1252, 'applied': 1253, 'arctic’s': 1254, 'argue': 1255, 'ask': 1256, 'assume': 1257, 'august': 1258, 'author': 1259, 'available': 1260, 'avoiding': 1261, 'became': 1262, 'behavior': 1263, 'behind': 1264, 'belief': 1265, 'believed': 1266, 'believers': 1267, 'benny': 1268, 'better': 1269, 'bias': 1270, 'birds': 1271, 'bit': 1272, 'bjorn': 1273, 'blamed': 1274, 'blanket': 1275, 'blatantly': 1276, 'blood': 1277, 'body': 1278, 'break': 1279, 'brightening': 1280, 'britain': 1281, 'budget': 1282, 'business-as-usual': 1283, 'calculate': 1284, 'calculated': 1285, 'calculations': 1286, 'calls': 1287, 'canada': 1288, 'canadian': 1289, 'center': 1290, 'centre': 1291, 'cern': 1292, 'certainty': 1293, 'cfcs': 1294, 'chain': 1295, 'change]': 1296, 'channel': 1297, 'cheaper': 1298, 'chevron': 1299, 'chicago': 1300, 'children': 1301, 'chimneys': 1302, 'chinese': 1303, 'citizens': 1304, 'city': 1305, 'climatic': 1306, 'climatologists': 1307, 'closely': 1308, 'co-founder': 1309, 'coleman': 1310, 'colorless': 1311, 'comment': 1312, 'communities': 1313, 'community': 1314, 'comparing': 1315, 'comparisons': 1316, 'complex': 1317, 'complicated': 1318, 'conclude': 1319, 'conducted': 1320, 'congress': 1321, 'consequently': 1322, 'conservatives': 1323, 'considerably': 1324, 'conspiracy': 1325, 'consumption': 1326, 'contains': 1327, 'continuing': 1328, 'contradicted': 1329, 'contradicts': 1330, 'contrary': 1331, 'contributed': 1332, 'controlling': 1333, 'conventional': 1334, 'convincing': 1335, 'cooked': 1336, 'cooks': 1337, 'core': 1338, 'cores': 1339, 'correlate': 1340, 'country': 1341, 'course': 1342, 'created': 1343, 'culprit': 1344, 'damaging': 1345, 'datasets': 1346, 'date': 1347, 'david': 1348, 'deadly': 1349, 'deal': 1350, 'decadal': 1351, 'declaration': 1352, 'deconto': 1353, 'deforestation': 1354, 'dehydration': 1355, 'delayed': 1356, 'describes': 1357, 'desert': 1358, 'destroyed': 1359, 'determine': 1360, 'determining': 1361, 'didn’t': 1362, 'die-off': 1363, 'dieback': 1364, 'diminished': 1365, 'director': 1366, 'disappearing': 1367, 'discovered': 1368, 'discovery': 1369, 'discredits': 1370, 'distributed': 1371, 'doctored': 1372, 'documented': 1373, 'domestic': 1374, 'double': 1375, 'downwards': 1376, 'drew': 1377, 'drowned': 1378, 'dust': 1379, 'easily': 1380, 'effective': 1381, 'eleven': 1382, 'emergency': 1383, 'emerging': 1384, 'emitted': 1385, 'energy]': 1386, 'england': 1387, 'entered': 1388, 'equivalent': 1389, 'essentially': 1390, 'established': 1391, 'evaporation': 1392, 'everyone': 1393, 'evident': 1394, 'evolving': 1395, 'exaggerated': 1396, 'exception': 1397, 'exhaust': 1398, 'existential': 1399, 'expensive': 1400, 'experience': 1401, 'experiment': 1402, 'explosion': 1403, 'extensive': 1404, 'external': 1405, 'extra': 1406, 'failing': 1407, 'fails': 1408, 'fallen': 1409, 'falling': 1410, 'famine': 1411, 'fastest': 1412, 'fear': 1413, 'features': 1414, 'feed': 1415, 'fight': 1416, 'figures—during': 1417, 'film': 1418, 'final': 1419, 'fits': 1420, 'fixed': 1421, 'flights': 1422, 'fluxes': 1423, 'foi': 1424, 'forced': 1425, 'forecast': 1426, 'form': 1427, 'forseeable': 1428, 'france': 1429, 'frank': 1430, 'freeze': 1431, 'freezing': 1432, 'fund': 1433, 'fundamental': 1434, 'funded': 1435, 'gauge': 1436, 'general': 1437, 'generally': 1438, 'generates': 1439, 'geologic': 1440, 'geologists': 1441, 'geophysical': 1442, 'geothermal': 1443, 'giant': 1444, 'goklany': 1445, 'grace': 1446, 'graduates': 1447, 'grande': 1448, 'graphic': 1449, 'grazing': 1450, 'greatest': 1451, 'greatly': 1452, 'greening': 1453, 'grew': 1454, 'hadley': 1455, 'half-degree': 1456, 'halve': 1457, 'hardly': 1458, 'harming': 1459, 'harmless': 1460, 'heads': 1461, 'heat-trapping': 1462, 'helped': 1463, 'helps': 1464, 'hide': 1465, 'highway': 1466, 'historical': 1467, 'hit': 1468, 'home': 1469, 'honesty': 1470, 'human-produced': 1471, 'hypothetically': 1472, 'ice-free': 1473, 'icecaps': 1474, 'icemelt': 1475, 'ignore': 1476, 'implemented': 1477, 'implications': 1478, 'impossible': 1479, 'improving': 1480, 'inaction': 1481, 'inch': 1482, 'include': 1483, 'incorrect': 1484, 'increasingly': 1485, 'india': 1486, 'indicated': 1487, 'indicates': 1488, 'indicating': 1489, 'individual': 1490, 'industries': 1491, 'influences': 1492, 'influencing': 1493, 'information': 1494, 'insignificant': 1495, 'instabilities': 1496, 'instance': 1497, 'instead': 1498, 'institute': 1499, 'intensifying[…]': 1500, 'invented': 1501, 'investigated': 1502, 'involved': 1503, 'involving': 1504, 'irrelevant': 1505, 'isotopes': 1506, 'issue': 1507, 'job': 1508, 'joe': 1509, 'jupiter/saturn': 1510, 'keeping': 1511, 'key': 1512, 'kilimanjaro': 1513, 'kind': 1514, 'knob': 1515, 'lake': 1516, 'latest': 1517, 'latter': 1518, 'laws': 1519, 'learned': 1520, 'led': 1521, 'let': 1522, 'lies': 1523, 'lifetime': 1524, 'light': 1525, 'limited': 1526, 'lindzen': 1527, 'linear': 1528, 'list': 1529, 'live': 1530, 'located': 1531, 'loehle': 1532, 'looks': 1533, 'losses': 1534, 'lots': 1535, 'lowest': 1536, 'maintain': 1537, 'maintained': 1538, 'manageable': 1539, 'managed': 1540, 'mankind': 1541, 'manmade': 1542, 'margin': 1543, 'marked': 1544, 'marshall': 1545, 'martian': 1546, 'mckitrick': 1547, 'measure': 1548, 'meat': 1549, 'media': 1550, 'meier': 1551, 'melts': 1552, 'mention': 1553, 'merely': 1554, 'meter': 1555, 'methodology': 1556, 'mexico': 1557, 'miami': 1558, 'michael': 1559, 'michigan': 1560, 'mid-1970s': 1561, 'migration': 1562, 'mild': 1563, 'millennia': 1564, 'millennial': 1565, 'minimal': 1566, 'mining': 1567, 'minister': 1568, 'minuscule': 1569, 'misled': 1570, 'mode': 1571, 'modelling': 1572, 'moderate': 1573, 'modestly': 1574, 'monckton': 1575, 'moreover': 1576, 'mother': 1577, 'move': 1578, 'movement': 1579, 'mr': 1580, 'n': 1581, 'name': 1582, 'nation’s': 1583, 'near-surface': 1584, 'nina': 1585, 'nine-tenths': 1586, 'noaa’s': 1587, 'note': 1588, 'noticeable': 1589, 'notion': 1590, 'nuclear': 1591, 'observational': 1592, 'observatory': 1593, 'observe': 1594, 'obvious': 1595, 'odorless': 1596, 'office': 1597, 'official': 1598, 'onto': 1599, 'opposite': 1600, 'oreskes': 1601, 'organization': 1602, 'oscillates': 1603, 'others': 1604, 'outgoing': 1605, 'outweigh': 1606, 'overlooked': 1607, 'overturn': 1608, 'overwhelming': 1609, 'pandemic': 1610, 'particular': 1611, 'pass': 1612, 'past[': 1613, 'path': 1614, 'peak': 1615, 'peiser': 1616, 'perhaps': 1617, 'perish': 1618, 'phenomenon': 1619, 'photosynthesis': 1620, 'physical': 1621, 'pointed': 1622, 'policies': 1623, 'pollard': 1624, 'pollutants': 1625, 'post-2000': 1626, 'potent': 1627, 'poverty': 1628, 'ppmv': 1629, 'prediction': 1630, 'pressure': 1631, 'prevent': 1632, 'prices': 1633, 'priority': 1634, 'produced': 1635, 'product': 1636, 'production': 1637, 'professionals': 1638, 'prominent': 1639, 'properly': 1640, 'proposed': 1641, 'proposing': 1642, 'protecting': 1643, 'proves': 1644, 'provided': 1645, 'proxy': 1646, 'pumping': 1647, 'puts': 1648, 'putting': 1649, 'quarter': 1650, 'queensland': 1651, 'rainfall': 1652, 'raised': 1653, 'range': 1654, 'raw': 1655, 'reached': 1656, 'reaching': 1657, 'read': 1658, 'reason': 1659, 'recovery': 1660, 'red': 1661, 'reductions': 1662, 'reefs': 1663, 'referenced': 1664, 'reflected': 1665, 'relating': 1666, 'released': 1667, 'releasing': 1668, 'rely': 1669, 'remain': 1670, 'remote': 1671, 'removing': 1672, 'repeatedly': 1673, 'replacing': 1674, 'reproduce': 1675, 'resilience': 1676, 'resources': 1677, 'respect': 1678, 'respected': 1679, 'respectively': 1680, 'retreating': 1681, 'reviewed': 1682, 'revised': 1683, 'rewrote': 1684, 'rich': 1685, 'richard': 1686, 'rigour': 1687, 'rings': 1688, 'rio': 1689, 'river': 1690, 'rooftop': 1691, 'rose': 1692, 'ross': 1693, 'sahara': 1694, 'saturation': 1695, 'save': 1696, 'scafetta': 1697, 'scenario': 1698, 'schmittner': 1699, 'school': 1700, 'seas': 1701, 'seasons': 1702, 'sediments': 1703, 'selected': 1704, 'senior': 1705, 'sense': 1706, 'sensing': 1707, 'sensitivities': 1708, 'sent': 1709, 'series': 1710, 'settled': 1711, 'shape': 1712, 'sharp': 1713, 'sharply': 1714, 'shells': 1715, 'shrunk': 1716, 'signing': 1717, 'sink': 1718, 'size': 1719, 'slowed': 1720, 'slower': 1721, 'snowfall': 1722, 'snowpack': 1723, 'so-called': 1724, 'soares': 1725, 'societies': 1726, 'somebody': 1727, 'something': 1728, 'somewhat': 1729, 'soot': 1730, 'specific': 1731, 'spot': 1732, 'spread': 1733, 'stating': 1734, 'statistically': 1735, 'statistics': 1736, 'steep': 1737, 'stop': 1738, 'store': 1739, 'strange': 1740, 'stratosphere': 1741, 'stratospheric': 1742, 'stress': 1743, 'stronger': 1744, 'studied': 1745, 'suggesting': 1746, 'suggestive': 1747, 'sulfate': 1748, 'supplied': 1749, 'support': 1750, 'surprisingly': 1751, 'survey': 1752, 'survived': 1753, 'swamped': 1754, 'systems': 1755, 'takes': 1756, 'taking': 1757, 'taylor': 1758, 'technology': 1759, 'telling': 1760, 'tend': 1761, 'terrifying': 1762, 'testable': 1763, 'thaw': 1764, 'theories': 1765, 'thick': 1766, 'thickness': 1767, 'thin': 1768, 'third': 1769, 'thousand': 1770, 'threatening': 1771, 'tide': 1772, 'tide-gauge': 1773, 'tied': 1774, 'timing': 1775, 'tiny': 1776, 'tonne': 1777, 'track': 1778, 'traffic': 1779, 'transport': 1780, 'tree': 1781, 'trenberth': 1782, 'tropics': 1783, 'turbines': 1784, 'turning': 1785, 'tuvalu': 1786, 'two-thirds': 1787, 'uah': 1788, 'unadjusted': 1789, 'unchecked': 1790, 'underground': 1791, 'underlying': 1792, 'undermine': 1793, 'unequivocal': 1794, 'unknown': 1795, 'un’s': 1796, 'upstream': 1797, 'upwards': 1798, 'usa': 1799, 'usgcrp': 1800, 'usual': 1801, 'usually': 1802, 'various': 1803, 'vast': 1804, 'video': 1805, 'viewed': 1806, 'virtually': 1807, 'volcano': 1808, 'want': 1809, 'war': 1810, 'wasn’t': 1811, 'watts': 1812, 'wavelengths': 1813, 'wealth': 1814, 'web': 1815, 'welfare': 1816, 'wetter': 1817, 'we’ll': 1818, 'whatever': 1819, 'whatsoever': 1820, 'whereby': 1821, 'whose': 1822, 'widely': 1823, 'widespread': 1824, 'winters': 1825, 'withstand': 1826, 'words': 1827, 'works': 1828, 'worst-case': 1829, 'yesterday': 1830, 'yields': 1831, 'zero': 1832, '~0': 1833, '‘hiatus’': 1834, '‘the': 1835, '‘we': 1836, '“as': 1837, '“but': 1838, '“it’s': 1839, '“sea': 1840, '“we': 1841, '$10': 1842, '$125': 1843, '$13': 1844, '$14': 1845, '$225': 1846, '$250': 1847, '$400': 1848, '$550': 1849, '-2c': 1850, '-30°c': 1851, '008%': 1852, '01°c': 1853, '02': 1854, '02°c': 1855, '04': 1856, '1-in-177-year': 1857, '10-14': 1858, '100%': 1859, '1079': 1860, '10km': 1861, '11-year': 1862, '11°c': 1863, '120°f': 1864, '122': 1865, '123': 1866, '1250ad': 1867, '127': 1868, '13c': 1869, '14%': 1870, '140': 1871, '15mm': 1872, '16': 1873, '16%': 1874, '1635': 1875, '164': 1876, '1675': 1877, '18-year': 1878, '1850-2005': 1879, '1860': 1880, '1900s': 1881, '1901': 1882, '1910': 1883, '1918': 1884, '1920': 1885, '1922': 1886, '1930s': 1887, '1934': 1888, '1938-43': 1889, '194': 1890, '1946': 1891, '1948': 1892, '195': 1893, '1951': 1894, '1958-2010': 1895, '1963': 1896, '1969': 1897, '1972': 1898, '1976': 1899, '1978': 1900, '1979–2000': 1901, '1982': 1902, '1983-2001': 1903, '1989': 1904, '1990s': 1905, '1991': 1906, '1992-2007': 1907, '1993': 1908, '1993-2003': 1909, '1994': 1910, '19c': 1911, '1c': 1912, '1°c': 1913, '1â°c': 1914, '1–200': 1915, '2%': 1916, '20-30': 1917, '200/mwh': 1918, '2000s': 1919, '2002': 1920, '2006': 1921, '2009-2010': 1922, '2016-17': 1923, '2019': 1924, '2020': 1925, '2025': 1926, '2030s': 1927, '2040': 1928, '2050': 1929, '2070s': 1930, '2080': 1931, '20th-century': 1932, '21': 1933, '22': 1934, '22nd': 1935, '23f': 1936, '24%': 1937, '25': 1938, '252': 1939, '25°c': 1940, '260': 1941, '27': 1942, '3%': 1943, '30-40': 1944, '30-year': 1945, '3000': 1946, '32': 1947, '33': 1948, '33%': 1949, '34': 1950, '34f': 1951, '36th': 1952, '385': 1953, '3c': 1954, '3rd-warmest': 1955, '3â°c': 1956, '4%': 1957, '40%': 1958, '4000': 1959, '401st': 1960, '42': 1961, '43%': 1962, '45': 1963, '4567': 1964, '45th': 1965, '45°c': 1966, '46': 1967, '4c': 1968, '4°c': 1969, '5%': 1970, '5%—half': 1971, '50%': 1972, '51': 1973, '52%': 1974, '53': 1975, '54f': 1976, '550': 1977, '57': 1978, '5th': 1979, '5th-warmest': 1980, '6-10c': 1981, '62': 1982, '62f': 1983, '62˚f': 1984, '64-90°n': 1985, '65': 1986, '66': 1987, '67-per-cent': 1988, '68': 1989, '70': 1990, '72': 1991, '73': 1992, '74': 1993, '75%': 1994, '75cm': 1995, '800': 1996, '80cm': 1997, '89': 1998, '9/11': 1999, '900ad': 2000, '900–1100': 2001, '918': 2002, '93%': 2003, '94': 2004, '950': 2005, '9c': 2006, '9°c': 2007, '9˚c': 2008, '[1988]': 2009, '[carbon]': 2010, '[cloudcover]': 2011, '[co2]': 2012, '[data]': 2013, '[energy]': 2014, '[find]': 2015, '[ipcc]': 2016, '[is]': 2017, '[jonathan': 2018, '[models]': 2019, '[nasa]': 2020, '[o]ne': 2021, '[ocean': 2022, '[on': 2023, '[other': 2024, '[oxygen': 2025, '[s]unspot': 2026, '[sea': 2027, '[sic]': 2028, '[those': 2029, '[u': 2030, '[which': 2031, '[wind': 2032, '[…]you': 2033, 'abbott': 2034, 'absolutely': 2035, 'absorptions': 2036, 'abundant': 2037, 'academic': 2038, 'academies': 2039, 'academy': 2040, 'accord': 2041, 'accord]': 2042, 'accretion': 2043, 'accumulate': 2044, 'accumulating': 2045, 'accumulation': 2046, 'accuracy': 2047, 'acidic]': 2048, 'acidify': 2049, 'acknowledges': 2050, 'active': 2051, 'actively': 2052, 'activists': 2053, 'acute': 2054, 'adam': 2055, 'adapting': 2056, 'added': 2057, 'addition': 2058, 'additions': 2059, 'addressed': 2060, 'adds': 2061, 'adequately': 2062, 'adjustment': 2063, 'adjustments': 2064, 'adjustments—compensation': 2065, 'administration’s': 2066, 'admit': 2067, 'advancing': 2068, 'advocacy': 2069, 'aerosol': 2070, 'affecting': 2071, 'affirm': 2072, 'afford': 2073, 'africa': 2074, 'african': 2075, 'afterwards': 2076, 'agencies': 2077, 'age’': 2078, 'ago…': 2079, 'agreeing': 2080, 'agricultural': 2081, 'agriculture': 2082, 'airplane': 2083, 'airplanes': 2084, 'airports': 2085, 'airtime': 2086, 'alarmists’': 2087, 'alaskan': 2088, 'albany': 2089, 'alcoa': 2090, 'alexander': 2091, 'algae': 2092, 'alignments': 2093, 'all-time': 2094, 'allege': 2095, 'alleged': 2096, 'allow': 2097, 'allowing': 2098, 'allows': 2099, 'alpine': 2100, 'alters': 2101, 'altimetry': 2102, 'altitude': 2103, 'altogether': 2104, 'aluminium': 2105, 'always': 2106, 'amendment': 2107, 'amendments': 2108, 'amherst': 2109, 'amounting': 2110, 'ample': 2111, 'amplifies': 2112, 'amsre': 2113, 'analysing': 2114, 'andrew': 2115, 'anecdotal': 2116, 'animal': 2117, 'announces': 2118, 'anomalous': 2119, 'answer': 2120, 'antenna': 2121, 'anti-business': 2122, 'anticipate': 2123, 'anticipated': 2124, 'anyone”': 2125, 'apparent': 2126, 'apparently': 2127, 'appeared': 2128, 'approached': 2129, 'approaching': 2130, 'apt': 2131, 'aqua': 2132, 'ar4': 2133, 'arbitrarily-chosen': 2134, 'arc': 2135, 'aren’t': 2136, 'argo': 2137, 'argument': 2138, 'arising': 2139, 'armed': 2140, 'arsenal': 2141, 'arsonists': 2142, 'arthur': 2143, 'artificial': 2144, 'asked': 2145, 'asphalt': 2146, 'assault': 2147, 'asserted': 2148, 'assessments': 2149, 'assumptions': 2150, 'asteroid': 2151, 'astounding': 2152, 'atmosphere’s': 2153, 'atmosphere“': 2154, 'atop': 2155, 'attack': 2156, 'attacks': 2157, 'attempt': 2158, 'attempts': 2159, 'attention': 2160, 'attributable': 2161, 'attributed': 2162, 'attributing': 2163, 'australasian': 2164, 'australians': 2165, 'authored': 2166, 'authority': 2167, 'averages': 2168, 'averaging': 2169, 'avert': 2170, 'aviation': 2171, 'aviation-related': 2172, 'aware': 2173, 'axed': 2174, 'backed': 2175, 'bacterial': 2176, 'balanced': 2177, 'barely': 2178, 'barreling': 2179, 'barrow': 2180, 'basements': 2181, 'basically': 2182, 'bath': 2183, 'beat': 2184, 'beaufort': 2185, 'beetle': 2186, 'before-': 2187, 'begich': 2188, 'begins': 2189, 'begun': 2190, 'behaviour': 2191, 'belgian': 2192, 'believe—2015': 2193, 'believing': 2194, 'benchmark': 2195, 'beneath': 2196, 'benefits': 2197, 'berg': 2198, 'bergen': 2199, 'best': 2200, 'beyond': 2201, 'bias’': 2202, 'bible': 2203, 'biden': 2204, 'bigger': 2205, 'billions-year-old': 2206, 'bills': 2207, 'biologist': 2208, 'biosphere': 2209, 'bipartisan': 2210, 'bitterly': 2211, 'bjerknes': 2212, 'bleached': 2213, 'blistering-hot': 2214, 'blocking': 2215, 'blogosphere': 2216, 'blue': 2217, 'blunt': 2218, 'bodies': 2219, 'bomb': 2220, 'boreholes': 2221, 'bother': 2222, 'bottle-stopper': 2223, 'boundary': 2224, 'bowl': 2225, 'bp': 2226, 'branching': 2227, 'brannen': 2228, 'breaking': 2229, 'breathing': 2230, 'brief': 2231, 'bright': 2232, 'bringing': 2233, 'brink': 2234, 'bristle': 2235, 'broad': 2236, 'broadly': 2237, 'broke': 2238, 'broken': 2239, 'brooks': 2240, 'brought': 2241, 'browman': 2242, 'budget’': 2243, 'buffer': 2244, 'buffering': 2245, 'build': 2246, 'building': 2247, 'buildings': 2248, 'buildup': 2249, 'bulbs': 2250, 'bulk': 2251, 'buoys': 2252, 'burned': 2253, 'burping': 2254, 'bursts': 2255, 'bushfires': 2256, 'buy': 2257, 'buying': 2258, 'c': 2259, 'c12': 2260, 'c3s': 2261, 'calcium': 2262, 'calculates': 2263, 'calendar': 2264, 'calling': 2265, 'caloric': 2266, 'cambridge': 2267, 'campaign': 2268, 'cancel': 2269, 'cap': 2270, 'capability': 2271, 'capacity': 2272, 'capped': 2273, 'captured': 2274, 'car': 2275, 'carbonate': 2276, 'cardiac': 2277, 'careers': 2278, 'carefully': 2279, 'carlos': 2280, 'carpool': 2281, 'carpooled': 2282, 'carried': 2283, 'carries': 2284, 'cascading': 2285, 'cases': 2286, 'casts': 2287, 'catastrophe': 2288, 'catastrophically': 2289, 'cattle': 2290, 'causal': 2291, 'ceased': 2292, 'center’s': 2293, 'centimeters': 2294, 'centimetres': 2295, 'cents/litre': 2296, 'certainty’': 2297, 'challenged': 2298, 'chamber': 2299, 'change’s': 2300, 'change”': 2301, 'channels': 2302, 'chaos': 2303, 'charge': 2304, 'chart': 2305, 'checked': 2306, 'cherry-picked': 2307, 'cherrypicked': 2308, 'choice': 2309, 'choices': 2310, 'choi’s': 2311, 'chronic': 2312, 'chunk': 2313, 'churns': 2314, 'circulations': 2315, 'citation': 2316, 'claiming': 2317, 'classic': 2318, 'cleaner': 2319, 'cleanest': 2320, 'clearest': 2321, 'clearing': 2322, 'cliff': 2323, 'climate-changing': 2324, 'climate-related': 2325, 'climategate': 2326, 'climb': 2327, 'climbed': 2328, 'clinton': 2329, 'closure': 2330, 'cm': 2331, 'coal-burning': 2332, 'coal’': 2333, 'coastlines': 2334, 'cold-weather': 2335, 'coldest': 2336, 'cold—counting': 2337, 'collapsing': 2338, 'collecting': 2339, 'color': 2340, 'columbia': 2341, 'combat': 2342, 'combating': 2343, 'combined': 2344, 'comfortably': 2345, 'commerce': 2346, 'commission': 2347, 'companies]': 2348, 'comparable': 2349, 'comparatively': 2350, 'comparison': 2351, 'compensate': 2352, 'compensated': 2353, 'compensatory': 2354, 'compiled': 2355, 'completely': 2356, 'component': 2357, 'compounded': 2358, 'comprehensive': 2359, 'compressed': 2360, 'comprises': 2361, 'computed': 2362, 'computerized': 2363, 'concedes': 2364, 'concentrated': 2365, 'concept': 2366, 'conclusions': 2367, 'concur': 2368, 'concurred': 2369, 'conditioning': 2370, 'confidently': 2371, 'confirms': 2372, 'conflicts': 2373, 'confused': 2374, 'congressman': 2375, 'connie': 2376, 'consecutive': 2377, 'consensus’': 2378, 'consequential': 2379, 'conservancy': 2380, 'considerable': 2381, 'considered': 2382, 'considering': 2383, 'conspiring': 2384, 'constant': 2385, 'constitute': 2386, 'constrained': 2387, 'construct': 2388, 'consumed': 2389, 'contacted': 2390, 'contain': 2391, 'contemporary': 2392, 'contention': 2393, 'continents': 2394, 'continent’s': 2395, 'continual': 2396, 'continuation': 2397, 'continuous': 2398, 'contradicting': 2399, 'contradiction': 2400, 'contrarian': 2401, 'contrarians': 2402, 'contrast': 2403, 'contribution': 2404, 'contributions': 2405, 'controlled': 2406, 'controls': 2407, 'converting': 2408, 'convinced': 2409, 'convincingly': 2410, 'cooking': 2411, 'cooled': 2412, 'copenhagen': 2413, 'core-derived': 2414, 'correspondingly': 2415, 'corrupted': 2416, 'corruption': 2417, 'costing': 2418, 'costlier': 2419, 'costly': 2420, 'costs—are': 2421, 'couldn’t': 2422, 'counteracting': 2423, 'counterpart': 2424, 'counties': 2425, 'countless': 2426, 'counts': 2427, 'couple': 2428, 'courts': 2429, 'coverage': 2430, 'covering': 2431, 'cows': 2432, 'co₂': 2433, 'crabs': 2434, 'crack': 2435, 'crap': 2436, 'creates': 2437, 'creator': 2438, 'creatures': 2439, 'critic': 2440, 'criticism': 2441, 'criticizing': 2442, 'crops': 2443, 'crossed': 2444, 'crude': 2445, 'crustal': 2446, 'cry': 2447, 'csiro': 2448, 'cultivation': 2449, 'curbelo': 2450, 'currents': 2451, 'curry': 2452, 'curve': 2453, 'cycles\\xad': 2454, 'cyclicity': 2455, 'damaged': 2456, 'damning': 2457, 'dampening': 2458, 'danger': 2459, 'darker': 2460, 'database': 2461, 'datasets[…]': 2462, 'dates': 2463, 'dating': 2464, 'daylight': 2465, 'dead': 2466, 'deadly—killing': 2467, 'death': 2468, 'debating': 2469, 'debt': 2470, 'decelerating': 2471, 'decision': 2472, 'decisions': 2473, 'declared': 2474, 'declaring': 2475, 'decline’': 2476, 'decreased': 2477, 'dedicated': 2478, 'deep': 2479, 'deepen': 2480, 'deepest': 2481, 'defended': 2482, 'deficits': 2483, 'definitely': 2484, 'definitions': 2485, 'definitive': 2486, 'deflect': 2487, 'degassing': 2488, 'degraded': 2489, 'deleted': 2490, 'deleting': 2491, 'delhoran': 2492, 'delivery': 2493, 'deluge': 2494, 'demise': 2495, 'democrats': 2496, 'demonstrated': 2497, 'demonstrates': 2498, 'deniers': 2499, 'dense': 2500, 'density': 2501, 'deny': 2502, 'depart': 2503, 'departing': 2504, 'department': 2505, 'dependent': 2506, 'depends': 2507, 'depriving': 2508, 'derive': 2509, 'derived': 2510, 'described': 2511, 'dessler': 2512, 'destabilize': 2513, 'destroy': 2514, 'detail': 2515, 'determines': 2516, 'devastated': 2517, 'develop': 2518, 'developed': 2519, 'development': 2520, 'diametrically': 2521, 'dielectric': 2522, 'dies': 2523, 'diesel': 2524, 'differences': 2525, 'difficult': 2526, 'digestion': 2527, 'diminishing': 2528, 'dimming': 2529, 'dioxide[…]': 2530, 'dioxide]': 2531, 'dip': 2532, 'direction': 2533, 'directions': 2534, 'dirty': 2535, 'disagree': 2536, 'disaster': 2537, 'discernible': 2538, 'discontinuity': 2539, 'discrepancies': 2540, 'discrepancy': 2541, 'discusses': 2542, 'discussing': 2543, 'discussion': 2544, 'disintegrated': 2545, 'disintegrating': 2546, 'disintegration': 2547, 'disparity': 2548, 'display': 2549, 'displayed': 2550, 'disproved': 2551, 'disputes': 2552, 'disruptions': 2553, 'distant': 2554, 'distract': 2555, 'district': 2556, 'diverge': 2557, 'divergence': 2558, 'dividing': 2559, 'dmi': 2560, 'documents': 2561, 'doesn’t': 2562, 'dollar': 2563, 'dominating': 2564, 'domino-effect': 2565, 'doomsday': 2566, 'downs': 2567, 'downsizing': 2568, 'downstream': 2569, 'dozens': 2570, 'draconian': 2571, 'draft': 2572, 'drain': 2573, 'drainage': 2574, 'drastic': 2575, 'drawing': 2576, 'drifting': 2577, 'drilling': 2578, 'drivers': 2579, 'drives': 2580, 'drops': 2581, 'drought-resistant': 2582, 'drove': 2583, 'dubious': 2584, 'duffy': 2585, 'dung': 2586, 'e-mail—kevin': 2587, 'earthquakes': 2588, 'eastern': 2589, 'eat': 2590, 'economies': 2591, 'economist': 2592, 'edges': 2593, 'effect]': 2594, 'elderberries': 2595, 'elderly': 2596, 'election': 2597, 'electric\\xadity': 2598, 'electromagnetic': 2599, 'elementary': 2600, 'elevated': 2601, 'eliminating': 2602, 'elliptical': 2603, 'elongated': 2604, 'elsewhere': 2605, 'emerged': 2606, 'emerges': 2607, 'emeritus': 2608, 'eminent': 2609, 'emits': 2610, 'emitters': 2611, 'employers': 2612, 'encyclopedic': 2613, 'endanger': 2614, 'ends': 2615, 'engagement': 2616, 'enormous': 2617, 'enriching': 2618, 'enso': 2619, 'enter': 2620, 'entering': 2621, 'enters': 2622, 'entirely': 2623, 'entitled': 2624, 'entitlement': 2625, 'epa': 2626, 'episode': 2627, 'episodes': 2628, 'epoch': 2629, 'equally': 2630, 'equation': 2631, 'equilibrium': 2632, 'era': 2633, 'eras': 2634, 'eric': 2635, 'escalate': 2636, 'escape': 2637, 'essay': 2638, 'establish': 2639, 'estimated’': 2640, 'etc': 2641, 'etna': 2642, 'europeans': 2643, 'evenly': 2644, 'events—and': 2645, 'eventually': 2646, 'everybody': 2647, 'everything': 2648, 'examine': 2649, 'examined': 2650, 'examples': 2651, 'exceeding': 2652, 'exceeds': 2653, 'exceptions': 2654, 'excess': 2655, 'executive': 2656, 'exerted': 2657, 'exhibit': 2658, 'exist': 2659, 'existed': 2660, 'existing': 2661, 'exists': 2662, 'expanding': 2663, 'expectation': 2664, 'expecting': 2665, 'expels': 2666, 'experiences': 2667, 'explodes': 2668, 'exploring': 2669, 'explosive': 2670, 'exponential': 2671, 'expressing': 2672, 'exsolving': 2673, 'externality': 2674, 'extinct': 2675, 'extinctions': 2676, 'extract': 2677, 'extrapolating': 2678, 'extremes': 2679, 'eyewitnesses': 2680, 'face': 2681, 'facilities': 2682, 'facing': 2683, 'fact-checked': 2684, 'factored': 2685, 'facts': 2686, 'failure': 2687, 'fall…': 2688, 'false': 2689, 'famine”': 2690, 'famous': 2691, 'fans': 2692, 'farm': 2693, 'farms': 2694, 'farther': 2695, 'fashion': 2696, 'fate': 2697, 'faults': 2698, 'faulty': 2699, 'fearful': 2700, 'feature': 2701, 'feb': 2702, 'feeding': 2703, 'feet/century': 2704, 'fellow': 2705, 'ferocity': 2706, 'fertiliser': 2707, 'fertilizer': 2708, 'fettle': 2709, 'fifty': 2710, 'fifty-five': 2711, 'fiji': 2712, 'filters': 2713, 'finance': 2714, 'finding': 2715, 'findings': 2716, 'fine': 2717, 'fingerprint': 2718, 'finite': 2719, 'fire': 2720, 'fire-dominated': 2721, 'fires': 2722, 'fish': 2723, 'fit': 2724, 'fitted': 2725, 'five-star': 2726, 'fix': 2727, 'fleeing': 2728, 'flood': 2729, 'flooding’': 2730, 'floodwater': 2731, 'flora': 2732, 'florida’s': 2733, 'flowering': 2734, 'flowers': 2735, 'flows': 2736, 'flu': 2737, 'fluctuate': 2738, 'fluctuates': 2739, 'fluctuations': 2740, 'flying': 2741, 'focus': 2742, 'focusing': 2743, 'foot': 2744, 'football': 2745, 'footprint': 2746, 'foraminifera]': 2747, 'force': 2748, 'foreseeable': 2749, 'forestall': 2750, 'forget': 2751, 'formed': 2752, 'former': 2753, 'forward': 2754, 'fourth': 2755, 'francisco': 2756, 'frankly': 2757, 'fraud': 2758, 'freedom': 2759, 'frequently': 2760, 'fresh': 2761, 'friederike': 2762, 'frigid': 2763, 'frost': 2764, 'frozen': 2765, 'fudge': 2766, 'fueled': 2767, 'fulfilling': 2768, 'fundamentally': 2769, 'funneled': 2770, 'f°': 2771, 'gains': 2772, 'galactic': 2773, 'galileo': 2774, 'gasses': 2775, 'gates': 2776, 'gauges': 2777, 'gave': 2778, 'generalists': 2779, 'generate': 2780, 'generated': 2781, 'generator': 2782, 'generous': 2783, 'gentle': 2784, 'genuine': 2785, 'geographic': 2786, 'george': 2787, 'georgia': 2788, 'giants': 2789, 'gillard': 2790, 'give': 2791, 'given': 2792, 'gives': 2793, 'glaciation': 2794, 'glacier': 2795, 'glaciologist': 2796, 'glaring': 2797, 'glasshouses': 2798, 'globe’s': 2799, 'gloom': 2800, 'goals': 2801, 'goes': 2802, 'gondwana': 2803, 'gordon': 2804, 'gore’s': 2805, 'gov': 2806, 'government-funded': 2807, 'governments': 2808, 'governor': 2809, 'gradual': 2810, 'gradually': 2811, 'grain': 2812, 'grandchildren': 2813, 'graph': 2814, 'gravity': 2815, 'greeland': 2816, 'greenhouse-gas': 2817, 'greg': 2818, 'grid': 2819, 'grip': 2820, 'grossly': 2821, 'grounded': 2822, 'guard': 2823, 'guelph': 2824, 'gutzler': 2825, 'h2o': 2826, 'habitable': 2827, 'habitat': 2828, 'habits': 2829, 'hal': 2830, 'halt': 2831, 'halved': 2832, 'hamburg': 2833, 'hand': 2834, 'happens': 2835, 'harbour': 2836, 'hard': 2837, 'harder-hitting': 2838, 'harmed': 2839, 'harnessing': 2840, 'harry': 2841, 'harvard': 2842, 'harvest': 2843, 'hasnain': 2844, 'hasn’t': 2845, 'hawaii': 2846, 'hazard': 2847, 'headache': 2848, 'headlines': 2849, 'healthier': 2850, 'hearings': 2851, 'heart': 2852, 'heated': 2853, 'heaviest': 2854, 'heavily': 2855, 'hectare': 2856, 'heine': 2857, 'held': 2858, 'helping': 2859, 'henry': 2860, 'herds': 2861, 'here-and-now': 2862, 'heretofore': 2863, 'heritage': 2864, 'hfcs': 2865, 'hid': 2866, 'high-end': 2867, 'high-latitude': 2868, 'high-pressure': 2869, 'highlighting': 2870, 'highly-regarded': 2871, 'hilda': 2872, 'hillary': 2873, 'himalayas': 2874, 'hints': 2875, 'hitting': 2876, 'hocker': 2877, 'hole': 2878, 'hole‘': 2879, 'homeland': 2880, 'hook': 2881, 'hopefully': 2882, 'horrifying': 2883, 'horticulturalists': 2884, 'hotel': 2885, 'houlton’s': 2886, 'households': 2887, 'houston': 2888, 'hudson': 2889, 'hulme': 2890, 'human-driven': 2891, 'human-induced': 2892, 'human-made': 2893, 'humankind’s': 2894, 'hundred': 2895, 'hung': 2896, 'hunt': 2897, 'hurricane-related': 2898, 'hurt': 2899, 'hurts': 2900, 'hydrates': 2901, 'hydrocarbons': 2902, 'hydrofluorocarbons': 2903, 'hydrologist': 2904, 'hypothermia': 2905, 'ian': 2906, 'ice-age”': 2907, 'ice-cliff': 2908, 'ice-sheet': 2909, 'ice]': 2910, 'icebergs': 2911, 'icecap': 2912, 'identical': 2913, 'identified': 2914, 'ignored': 2915, 'ignoring': 2916, 'illegal': 2917, 'illustrated': 2918, 'imagines': 2919, 'imbalance': 2920, 'immediate': 2921, 'immediately': 2922, 'imminent': 2923, 'immune': 2924, 'impacted': 2925, 'implement': 2926, 'implies': 2927, 'imply': 2928, 'implying': 2929, 'importantly': 2930, 'imposes': 2931, 'improved': 2932, 'inaccurately': 2933, 'inappropriate': 2934, 'incontrovertible': 2935, 'inconvenient': 2936, 'incorporated': 2937, 'incredibly': 2938, 'indeed': 2939, 'indian': 2940, 'indicative': 2941, 'indirectly': 2942, 'individuals': 2943, 'induced': 2944, 'indur': 2945, 'infected': 2946, 'infestations': 2947, 'inflated': 2948, 'information[…]': 2949, 'infrastructure': 2950, 'ingeniously': 2951, 'inhale': 2952, 'initiated': 2953, 'initiative': 2954, 'inland': 2955, 'innovative': 2956, 'input': 2957, 'inquiry': 2958, 'insects': 2959, 'inside': 2960, 'insider': 2961, 'insistence': 2962, 'instability': 2963, 'installing': 2964, 'instructed': 2965, 'insulating': 2966, 'intake': 2967, 'integrity': 2968, 'intensely': 2969, 'intensifying': 2970, 'interactive': 2971, 'interior': 2972, 'internet': 2973, 'intervention': 2974, 'interview': 2975, 'introduce': 2976, 'introduced': 2977, 'inundating': 2978, 'invested': 2979, 'investigations': 2980, 'iran': 2981, 'iris': 2982, 'ironic': 2983, 'irradiance': 2984, 'irreversible': 2985, 'isi': 2986, 'isis': 2987, 'islamist': 2988, 'islands’': 2989, 'isolated': 2990, 'isostatic': 2991, 'isotopic': 2992, 'issues': 2993, 'j': 2994, 'jail': 2995, 'japan': 2996, 'jaxa': 2997, 'jeffrey': 2998, 'jens': 2999, 'jensen': 3000, 'jersey': 3001, 'jets': 3002, 'johnson': 3003, 'journals': 3004, 'jr': 3005, 'judith': 3006, 'julia': 3007, 'jump': 3008, 'jumped': 3009, 'jurassic-cretaceous': 3010, 'just-released': 3011, 'justify': 3012, 'k': 3013, 'keeling': 3014, 'keeps': 3015, 'kench': 3016, 'kerry': 3017, 'kibosh': 3018, 'kick': 3019, 'kidney': 3020, 'killer': 3021, 'killing': 3022, 'kilometres': 3023, 'kinds': 3024, 'kingdom': 3025, 'klaus-martin': 3026, 'knock-on': 3027, 'knowledge': 3028, 'knows': 3029, 'koch': 3030, 'labelled': 3031, 'labor': 3032, 'laboratory': 3033, 'lag': 3034, 'lagged': 3035, 'lamont-doherty': 3036, 'lancet': 3037, 'land-based': 3038, 'landmark': 3039, 'lansner': 3040, 'lasted': 3041, 'later': 3042, 'laureates': 3043, 'lawns': 3044, 'lawrence': 3045, 'lawsuit': 3046, 'lay': 3047, 'layers': 3048, 'leaders': 3049, 'leaking': 3050, 'learning': 3051, 'leave': 3052, 'leaves': 3053, 'leaving': 3054, 'left': 3055, 'legal': 3056, 'legislation': 3057, 'legislative': 3058, 'legislators': 3059, 'legislature': 3060, 'lemieux': 3061, 'length': 3062, 'lesser': 3063, 'lesson': 3064, 'let’s': 3065, 'level]': 3066, 'leveled': 3067, 'lewis': 3068, 'lieberman': 3069, 'lighter': 3070, 'lightning': 3071, 'lights': 3072, 'likelihood': 3073, 'likewise': 3074, 'limpets': 3075, 'lingered': 3076, 'links': 3077, 'liquid': 3078, 'listed': 3079, 'litigation': 3080, 'litre': 3081, 'livermore': 3082, 'lives': 3083, 'livestock': 3084, 'ljungqvist': 3085, 'logarithmic': 3086, 'lomborg': 3087, 'longtime': 3088, 'looked': 3089, 'loses': 3090, 'lost': 3091, 'lot': 3092, 'low-carbon': 3093, 'low-level': 3094, 'low-lying': 3095, 'low-pressure': 3096, 'lull': 3097, 'machine': 3098, 'mack': 3099, 'magazine': 3100, 'magnetic': 3101, 'magnitude': 3102, 'majority': 3103, 'makers': 3104, 'man': 3105, 'management': 3106, 'manages': 3107, 'mandate': 3108, 'mangroves': 3109, 'manipulation': 3110, 'manner': 3111, 'manure': 3112, 'marginal': 3113, 'mark': 3114, 'market': 3115, 'market-oriented': 3116, 'marshes': 3117, 'massachusetts': 3118, 'match': 3119, 'matched': 3120, 'materials': 3121, 'matters': 3122, 'matthew': 3123, 'mature': 3124, 'maunder': 3125, 'max': 3126, 'maximums': 3127, 'mcintyre': 3128, 'meaning': 3129, 'meaningful': 3130, 'meaningfully': 3131, 'meanwhile': 3132, 'measuring': 3133, 'mechanism': 3134, 'meet': 3135, 'meeting': 3136, 'meets': 3137, 'mega-droughts': 3138, 'melted': 3139, 'men': 3140, 'mercury': 3141, 'meteorology': 3142, 'meters': 3143, 'methane…': 3144, 'method': 3145, 'metre': 3146, 'metric': 3147, 'metrics': 3148, 'michaels': 3149, 'microsite': 3150, 'microwaves': 3151, 'mid': 3152, 'mid-': 3153, 'mid-troposphere': 3154, 'mid-twentieth': 3155, 'middle-class': 3156, 'milder': 3157, 'military': 3158, 'millennium': 3159, 'millimeters': 3160, 'million”': 3161, 'mind': 3162, 'mine': 3163, 'minerals': 3164, 'minnesota': 3165, 'minus': 3166, 'minute': 3167, 'minutes': 3168, 'mired': 3169, 'misery': 3170, 'misinterpreted': 3171, 'misjudging': 3172, 'misleading': 3173, 'mismatch': 3174, 'mississippi': 3175, 'mistaken': 3176, 'mistakes': 3177, 'misunderstood': 3178, 'mitch': 3179, 'mitigation': 3180, 'mitigatory': 3181, 'mix': 3182, 'mixes': 3183, 'mm/year': 3184, 'moberg': 3185, 'modelers': 3186, 'modeling': 3187, 'modellers': 3188, 'models]': 3189, 'modern': 3190, 'modest': 3191, 'molecular': 3192, 'moment': 3193, 'monday': 3194, 'monetary': 3195, 'monitored': 3196, 'monitoring': 3197, 'monotonic': 3198, 'monsoons': 3199, 'mortality': 3200, 'mosquitoes': 3201, 'mountain': 3202, 'moved': 3203, 'movie': 3204, 'mt': 3205, 'multi-billion': 3206, 'multi-year': 3207, 'multiplying': 3208, 'multitude': 3209, 'murry': 3210, 'naming': 3211, 'nancy': 3212, 'narisma': 3213, 'narrative': 3214, 'narrowbody': 3215, 'naturally-occurring': 3216, 'near-100-degree-fahrenheit': 3217, 'neatly': 3218, 'necessity': 3219, 'needed': 3220, 'neglected': 3221, 'negligible': 3222, 'negotiated': 3223, 'negotiating': 3224, 'neither': 3225, 'neptune': 3226, 'neptunian': 3227, 'nevada': 3228, 'nevertheless': 3229, 'newspaper': 3230, 'newspapers': 3231, 'night': 3232, 'nimbus7/erb': 3233, 'niña': 3234, 'niñas': 3235, 'nobel': 3236, 'nobody’s': 3237, 'noise': 3238, 'non': 3239, 'non-condensable': 3240, 'non-problem': 3241, 'non-toxic': 3242, 'norfolk': 3243, 'norm': 3244, 'normally': 3245, 'northerly': 3246, 'northwest': 3247, 'norway': 3248, 'notably': 3249, 'noteworthy': 3250, 'notorious': 3251, 'november': 3252, 'nuisance': 3253, 'nye': 3254, 'observing': 3255, 'occasions': 3256, 'occuring': 3257, 'ocean’s': 3258, 'offered': 3259, 'offset': 3260, 'offshore': 3261, 'omitted': 3262, 'on-site': 3263, 'one-fifth': 3264, 'one-off': 3265, 'one-quarter': 3266, 'one-third': 3267, 'ones': 3268, 'one’s': 3269, 'ongoing': 3270, 'online': 3271, 'onslaught': 3272, 'onward': 3273, 'openness': 3274, 'operations': 3275, 'opponents': 3276, 'opposition': 3277, 'option': 3278, 'order': 3279, 'ordovician-': 3280, 'organism': 3281, 'organisms': 3282, 'organizations': 3283, 'organ\\xadisms': 3284, 'originates': 3285, 'orleans': 3286, 'oscillating': 3287, 'otto': 3288, 'outbreaks': 3289, 'outlet': 3290, 'outside': 3291, 'outstrips': 3292, 'outvoted': 3293, 'over-development': 3294, 'overestimate': 3295, 'overlaid': 3296, 'overlying': 3297, 'overstatement': 3298, 'overwhelmingly': 3299, 'oxford': 3300, 'oxygen-rich': 3301, 'page': 3302, 'paint': 3303, 'painted': 3304, 'paleotemperatures': 3305, 'pales': 3306, 'palin': 3307, 'palm-oil': 3308, 'pandemics': 3309, 'paragraph': 3310, 'parents': 3311, 'park': 3312, 'parking': 3313, 'particulate': 3314, 'party': 3315, 'patrick': 3316, 'paul': 3317, 'paused': 3318, 'paying': 3319, 'pays': 3320, 'peaked': 3321, 'pelosi': 3322, 'penalizing': 3323, 'penguins': 3324, 'pennsylvania': 3325, 'perceived': 3326, 'percentage': 3327, 'percent—of': 3328, 'periodically': 3329, 'permanent': 3330, 'perpetual': 3331, 'perry': 3332, 'persistent': 3333, 'persistently': 3334, 'person': 3335, 'pervasive': 3336, 'pessimistic': 3337, 'peter': 3338, 'petrol': 3339, 'phalodi': 3340, 'phases': 3341, 'phd': 3342, 'phone': 3343, 'photographs': 3344, 'pick': 3345, 'picked': 3346, 'picture': 3347, 'piece': 3348, 'pielke': 3349, 'piled': 3350, 'pine': 3351, 'pinker': 3352, 'pipelines': 3353, 'pipes': 3354, 'placed': 3355, 'plains': 3356, 'planck': 3357, 'planes': 3358, 'planets': 3359, 'plantations': 3360, 'plasticity': 3361, 'plate': 3362, 'plausible': 3363, 'plays': 3364, 'plenty': 3365, 'plot': 3366, 'plummet': 3367, 'plummeted': 3368, 'plus': 3369, 'pmod': 3370, 'poetically': 3371, 'pointing': 3372, 'poison': 3373, 'policy-makers': 3374, 'political': 3375, 'politically': 3376, 'pollen': 3377, 'polluter': 3378, 'polluting': 3379, 'poor': 3380, 'poorer': 3381, 'portfolio': 3382, 'portion': 3383, 'portman': 3384, 'pose': 3385, 'posed': 3386, 'poses': 3387, 'posing': 3388, 'possibilities': 3389, 'post-1979': 3390, 'practically': 3391, 'precede': 3392, 'precise': 3393, 'preclude': 3394, 'predators': 3395, 'predicting': 3396, 'predicts': 3397, 'predisposed': 3398, 'preferred': 3399, 'prejudice': 3400, 'premature': 3401, 'prematurely': 3402, 'prepared': 3403, 'presence': 3404, 'presented': 3405, 'president-elect': 3406, 'press': 3407, 'presumably': 3408, 'presumed': 3409, 'prey': 3410, 'pricing': 3411, 'prime': 3412, 'principally': 3413, 'prior': 3414, 'probabilities': 3415, 'probable': 3416, 'processes': 3417, 'produces': 3418, 'producing': 3419, 'productive': 3420, 'productivity': 3421, 'prof': 3422, 'profound': 3423, 'prognosis': 3424, 'progresses': 3425, 'prohibit': 3426, 'projected': 3427, 'projection': 3428, 'projects': 3429, 'prolonged': 3430, 'promote': 3431, 'pronounced': 3432, 'propaganda': 3433, 'proper': 3434, 'proponents': 3435, 'proportion': 3436, 'proposal': 3437, 'propose': 3438, 'prospect': 3439, 'prosperity': 3440, 'protect': 3441, 'protections': 3442, 'protracted': 3443, 'proved': 3444, 'proven': 3445, 'providence': 3446, 'proving': 3447, 'publication': 3448, 'publish': 3449, 'publishing': 3450, 'punctuated': 3451, 'purely': 3452, 'push': 3453, 'quantify': 3454, 'quantities': 3455, 'quantity': 3456, 'questioning': 3457, 'quicker': 3458, 'quiet': 3459, 'quitting': 3460, 'radiate': 3461, 'radiates': 3462, 'radiating/reflecting': 3463, 'radiative-transfer': 3464, 'radicalizing': 3465, 'radio': 3466, 'raining': 3467, 'raises': 3468, 'raising': 3469, 'ramp': 3470, 'ramped': 3471, 'ranging': 3472, 'ranks': 3473, 'rapidity': 3474, 'rate”': 3475, 'raunsø': 3476, 'ray': 3477, 're-equilibration': 3478, 'reach': 3479, 'reaches': 3480, 'react': 3481, 'reacts': 3482, 'readily': 3483, 'real-world': 3484, 'realized': 3485, 'reanalyses': 3486, 'reasonable': 3487, 'reasons': 3488, 'rebound': 3489, 'receded': 3490, 'receding': 3491, 'receives': 3492, 'reconsidered': 3493, 'record-breaking': 3494, 'record-low': 3495, 'records’': 3496, 'reduces': 3497, 'refering': 3498, 'refers': 3499, 'reflect': 3500, 'reflecting': 3501, 'reflection': 3502, 'reflective': 3503, 'reflectivity': 3504, 'reform': 3505, 'refutes': 3506, 'regularly': 3507, 'regulate': 3508, 'regulating': 3509, 'regulatory': 3510, 'reid': 3511, 'reinforces': 3512, 'relation': 3513, 'relative': 3514, 'relentlessly': 3515, 'reliability': 3516, 'relies': 3517, 'remains': 3518, 'remarkable’': 3519, 'remarkably': 3520, 'reminder': 3521, 'remnants': 3522, 'remotely': 3523, 'remove': 3524, 'removes': 3525, 'repainted': 3526, 'repealed': 3527, 'replaced': 3528, 'reporters': 3529, 'reporting': 3530, 'report—told': 3531, 'representatives': 3532, 'represented': 3533, 'request': 3534, 'requesters': 3535, 'requests': 3536, 'require': 3537, 'research-atmosphere': 3538, 'researching': 3539, 'reservoirs': 3540, 'residency': 3541, 'resigns': 3542, 'resilient': 3543, 'resorted': 3544, 'resource': 3545, 'respective': 3546, 'respects': 3547, 'respiration': 3548, 'respond': 3549, 'responds': 3550, 'restore': 3551, 'restoring': 3552, 'restricting': 3553, 'restricts': 3554, 'resulted': 3555, 'retract': 3556, 'retracted': 3557, 'retreat': 3558, 'return': 3559, 'reveal': 3560, 'revealed': 3561, 'reveals': 3562, 'revisit': 3563, 'rice': 3564, 'rick': 3565, 'rid': 3566, 'right': 3567, 'rigorous': 3568, 'ripen': 3569, 'risks': 3570, 'river]': 3571, 'roads': 3572, 'rob': 3573, 'robert': 3574, 'rock': 3575, 'roger': 3576, 'roman': 3577, 'ron': 3578, 'rooftops': 3579, 'room': 3580, 'root': 3581, 'round': 3582, 'rounded': 3583, 'rps': 3584, 'ruled': 3585, 'rules': 3586, 'run': 3587, 'runs': 3588, 'sacrificed': 3589, 'safe': 3590, 'said]': 3591, 'sake': 3592, 'salby': 3593, 'salt': 3594, 'salvador': 3595, 'san': 3596, 'sand': 3597, 'saturated': 3598, 'savanna': 3599, 'saving': 3600, 'scaife': 3601, 'scales': 3602, 'scattered': 3603, 'scenario”': 3604, 'scepticism”': 3605, 'schematic': 3606, 'scheme': 3607, 'schmidt': 3608, 'schools': 3609, 'schulte': 3610, 'scientists]': 3611, 'scorching': 3612, 'scotland': 3613, 'scrap': 3614, 'sea-ice': 3615, 'seager': 3616, 'seals': 3617, 'search': 3618, 'seasonal': 3619, 'seawalls': 3620, 'second-warmest': 3621, 'secretary-general': 3622, 'sections': 3623, 'sediment': 3624, 'seed': 3625, 'seeks': 3626, 'seemed': 3627, 'self-evident': 3628, 'senator': 3629, 'sending': 3630, 'sens': 3631, 'sensor': 3632, 'sensors': 3633, 'separate': 3634, 'seriously': 3635, 'serves': 3636, 'service': 3637, 'service’s': 3638, 'setting': 3639, 'seven': 3640, 'seventeen': 3641, 'seventy-five': 3642, 'severity': 3643, 'shadow': 3644, 'shanghai': 3645, 'shell': 3646, 'shelters': 3647, 'shelves': 3648, 'shift': 3649, 'shifts': 3650, 'ships’': 3651, 'shopped': 3652, 'shoreline': 3653, 'shortages': 3654, 'shorter-lived': 3655, 'shortwave': 3656, 'showering': 3657, 'shrink': 3658, 'shutting': 3659, 'sidewalks': 3660, 'sif': 3661, 'signal': 3662, 'silurian': 3663, 'singer': 3664, 'single-handedly': 3665, 'site': 3666, 'siting': 3667, 'skeptic': 3668, 'skeptical': 3669, 'skewed': 3670, 'sky': 3671, 'slowdown': 3672, 'slowing': 3673, 'slows': 3674, 'smelter': 3675, 'smog': 3676, 'smokestacks': 3677, 'snowcap': 3678, 'snowy': 3679, 'soared': 3680, 'soaring': 3681, 'social': 3682, 'solar-driven': 3683, 'solar-heated': 3684, 'sold': 3685, 'solid': 3686, 'solution': 3687, 'solutions': 3688, 'solving': 3689, 'somehow': 3690, 'somewhere': 3691, 'sooner': 3692, 'southwest': 3693, 'southwestern': 3694, 'soviet': 3695, 'so—a': 3696, 'spain': 3697, 'spain’s': 3698, 'span': 3699, 'spawn': 3700, 'specifically': 3701, 'speed': 3702, 'spell': 3703, 'spencer': 3704, 'spends': 3705, 'spent': 3706, 'spike': 3707, 'spikes': 3708, 'spin': 3709, 'spliced': 3710, 'sponges': 3711, 'sponsored': 3712, 'spotted': 3713, 'spot”': 3714, 'spurious': 3715, 'ssts': 3716, 'stabilized': 3717, 'stages': 3718, 'stalagmites': 3719, 'stand': 3720, 'standard': 3721, 'star': 3722, 'started': 3723, 'starting': 3724, 'startling': 3725, 'starts': 3726, 'starving': 3727, 'statements': 3728, 'statistic': 3729, 'statistician': 3730, 'stay': 3731, 'stayed': 3732, 'staying': 3733, 'stays': 3734, 'steepest': 3735, 'steig': 3736, 'step': 3737, 'steps': 3738, 'steve': 3739, 'stevens': 3740, 'stimulation': 3741, 'stirling': 3742, 'stirring': 3743, 'stole': 3744, 'stolen': 3745, 'stomata-derived': 3746, 'stood': 3747, 'stopping': 3748, 'stops': 3749, 'storage': 3750, 'stored': 3751, 'stores': 3752, 'stories': 3753, 'stories’': 3754, 'stoves': 3755, 'stow': 3756, 'straight': 3757, 'stream]': 3758, 'streams”': 3759, 'stream’s': 3760, 'street': 3761, 'strengthening': 3762, 'stressed': 3763, 'stretches': 3764, 'strikes': 3765, 'stroke': 3766, 'struggle': 3767, 'struggling': 3768, 'study’s': 3769, 'subject': 3770, 'subjects': 3771, 'subordinate': 3772, 'subsequently': 3773, 'subsidize': 3774, 'substances': 3775, 'subtract': 3776, 'successfully': 3777, 'suffer': 3778, 'suffers': 3779, 'sufficient': 3780, 'sufficiently': 3781, 'sugarcane': 3782, 'sulphuric': 3783, 'summary': 3784, 'summer’s': 3785, 'summit': 3786, 'sun’s': 3787, 'super': 3788, 'super-volcano': 3789, 'superimposed': 3790, 'supernova': 3791, 'superstorms': 3792, 'supply': 3793, 'supported': 3794, 'supports': 3795, 'sure': 3796, 'surfaced': 3797, 'surge': 3798, 'surged': 3799, 'surprise': 3800, 'surprising': 3801, 'surrounded': 3802, 'surveys': 3803, 'survival': 3804, 'survive': 3805, 'survived[…]': 3806, 'suspects': 3807, 'sussex': 3808, 'sustainable': 3809, 'suvs': 3810, 'swamp': 3811, 'sweet': 3812, 'swift': 3813, 'swiftly': 3814, 'swings': 3815, 'sydney': 3816, 'syed': 3817, 'symbol': 3818, 'synchronous': 3819, 'systematic': 3820, 'taalas': 3821, 'take-home': 3822, 'talking': 3823, 'tallahassee': 3824, 'tampered': 3825, 'tamping': 3826, 'tank': 3827, 'target': 3828, 'targets': 3829, 'tax-payer': 3830, 'team': 3831, 'teams': 3832, 'technical': 3833, 'technique': 3834, 'technologies': 3835, 'temperature-amplifying': 3836, 'temperature-change': 3837, 'temporarily': 3838, 'temporary': 3839, 'ten': 3840, 'termed': 3841, 'terms': 3842, 'terrestrial': 3843, 'terrible': 3844, 'terribly': 3845, 'territories': 3846, 'terrorist': 3847, 'terrorists': 3848, 'tested': 3849, 'testified': 3850, 'texans': 3851, 'thames': 3852, 'thanks': 3853, 'that’s': 3854, 'then-british': 3855, 'thereby': 3856, 'thermometer': 3857, 'thicker': 3858, 'things': 3859, 'thinks': 3860, 'thousands”': 3861, 'threaten': 3862, 'threatens': 3863, 'threats': 3864, 'three-synodic': 3865, 'three-tenths': 3866, 'thrive': 3867, 'thursday': 3868, 'till': 3869, 'time”': 3870, 'tipping': 3871, 'titanic': 3872, 'today[': 3873, 'tol': 3874, 'tolerant': 3875, 'tonnes': 3876, 'tons': 3877, 'tons—2': 3878, 'tony': 3879, 'tool': 3880, 'topic': 3881, 'topped': 3882, 'tornadoes': 3883, 'torrential': 3884, 'totaled': 3885, 'toughest': 3886, 'toxic': 3887, 'trace': 3888, 'tracking': 3889, 'tracks': 3890, 'traditionally': 3891, 'transient': 3892, 'translate': 3893, 'transmissions': 3894, 'transport-related': 3895, 'trapped': 3896, 'trapping': 3897, 'traps': 3898, 'treated': 3899, 'treatment': 3900, 'trenberth’s': 3901, 'trend-lines': 3902, 'trick': 3903, 'trick’': 3904, 'tried': 3905, 'triennial': 3906, 'trifling': 3907, 'trillion': 3908, 'trip': 3909, 'triton': 3910, 'trucks': 3911, 'trudeau': 3912, 'truly': 3913, 'trupin': 3914, 'truth': 3915, 'truths': 3916, 'tsi': 3917, 'tundra': 3918, 'tune': 3919, 'tuned': 3920, 'turbulence': 3921, 'turned': 3922, 'twenty': 3923, 'type': 3924, 'types': 3925, 'uk': 3926, 'uk’s': 3927, 'ultimately': 3928, 'unacceptably': 3929, 'unaltered': 3930, 'uncertainties': 3931, 'unclear': 3932, 'uncontaminated': 3933, 'undermines': 3934, 'underwater': 3935, 'undoubtedly': 3936, 'unfold': 3937, 'unfold”': 3938, 'unfounded': 3939, 'unhealthy': 3940, 'unhelpful': 3941, 'unimportant': 3942, 'uninhabitable': 3943, 'union': 3944, 'unit': 3945, 'units': 3946, 'universally': 3947, 'universities': 3948, 'unleash': 3949, 'unlike': 3950, 'unlimited': 3951, 'unnatural': 3952, 'unquestionably': 3953, 'unreliable': 3954, 'unspecificed': 3955, 'unstoppable': 3956, 'unsure': 3957, 'unsympathetic': 3958, 'unusual': 3959, 'up-to-date': 3960, 'upcoming': 3961, 'upon': 3962, 'ups': 3963, 'upset': 3964, 'upswell': 3965, 'upward': 3966, 'urchins': 3967, 'urgent': 3968, 'us$': 3969, 'utility': 3970, 'uv': 3971, 'vacuum': 3972, 'valero': 3973, 'validity': 3974, 'value': 3975, 'vanish': 3976, 'vanishing': 3977, 'vapour[…]': 3978, 'variation': 3979, 'variously': 3980, 'varying': 3981, 'vastly': 3982, 'vegetated': 3983, 'vehicles': 3984, 'veteran': 3985, 'view': 3986, 'views': 3987, 'vigilant': 3988, 'village': 3989, 'virus': 3990, 'viscosity': 3991, 'visualization': 3992, 'voice': 3993, 'volume': 3994, 'vortex': 3995, 'vs': 3996, 'wahr': 3997, 'wallace': 3998, 'walt': 3999, 'ward': 4000, 'warmer-than-present': 4001, 'warming]': 4002, 'warming—it': 4003, 'warming—perhaps': 4004, 'warming”': 4005, 'warmist': 4006, 'warned': 4007, 'wash': 4008, 'washes': 4009, 'washington': 4010, 'waste': 4011, 'wastewater': 4012, 'water-vapor': 4013, 'waterplace': 4014, 'waterways': 4015, 'watson': 4016, 'watts/sq': 4017, 'ways': 4018, 'weakened': 4019, 'weakening': 4020, 'weaker': 4021, 'weather]': 4022, 'weathering': 4023, 'webb': 4024, 'website': 4025, 'weeks': 4026, 'weight': 4027, 'well-known': 4028, 'wells': 4029, 'went': 4030, 'wetlands': 4031, 'we\\x92ve': 4032, 'whenever': 4033, 'whereas': 4034, 'white': 4035, 'wickford': 4036, 'wide': 4037, 'widebody': 4038, 'wild': 4039, 'wildfire': 4040, 'wildlife': 4041, 'wildly': 4042, 'windmill': 4043, 'wiped': 4044, 'wisconsin': 4045, 'wisdom': 4046, 'wise': 4047, 'withstood': 4048, 'witness': 4049, 'witnessed': 4050, 'wm–2': 4051, 'wobbles': 4052, 'wong': 4053, 'wood': 4054, 'workers': 4055, 'working-level': 4056, 'world-wide': 4057, 'worry': 4058, 'worth': 4059, 'wreak': 4060, 'writer': 4061, 'written': 4062, 'wu': 4063, 'wwf': 4064, 'wyoming': 4065, 'yardsticks': 4066, 'year-to-date': 4067, 'year-to-year': 4068, 'years[…]': 4069, 'years’': 4070, 'y\\u200b\\u200b\\u200b\\u200b\\u200b\\u200bet': 4071, 'zika': 4072, 'zone': 4073, '~1': 4074, '\\xadatmospheric': 4075, '°f': 4076, 'à': 4077, 'â\\x80\\x94': 4078, 'â\\x80¦': 4079, 'â\\xadhumans[': 4080, '–1': 4081, '‘': 4082, '‘but': 4083, '‘calving': 4084, '‘clean': 4085, '‘doom': 4086, '‘emissions': 4087, '‘generally': 4088, '‘getting': 4089, '‘greater': 4090, '‘heat': 4091, '‘heatwaves': 4092, '‘hide': 4093, '‘hothouse’': 4094, '‘indicate': 4095, '‘inherent': 4096, '‘is': 4097, '‘lockdown’': 4098, '‘mike’s': 4099, '‘mini': 4100, '‘no': 4101, '‘paused’': 4102, '‘slowdown’': 4103, '‘snowmageddon’': 4104, '‘summers': 4105, '‘sunny-day': 4106, '‘warming': 4107, '‘we’re': 4108, '‘while': 4109, '‘with': 4110, '’97%': 4111, '’extremely': 4112, '’fever’': 4113, '“': 4114, '“2050': 4115, '“[carbon': 4116, '“[sea': 4117, '“[t]he': 4118, '“adjusted”': 4119, '“although': 4120, '“at': 4121, '“because': 4122, '“blocking”': 4123, '“carbon': 4124, '“catherine': 4125, '“certainly': 4126, '“climate': 4127, '“co2': 4128, '“continue': 4129, '“dr': 4130, '“during': 4131, '“earlier': 4132, '“even': 4133, '“every': 4134, '“feast': 4135, '“federal': 4136, '“for': 4137, '“healthy': 4138, '“hot': 4139, '“houlton': 4140, '“industrial”': 4141, '“it': 4142, '“lyme': 4143, '“mini': 4144, '“natural”': 4145, '“oceanography”': 4146, '“our': 4147, '“recent': 4148, '“right': 4149, '“scientists': 4150, '“several': 4151, '“so': 4152, '“some': 4153, '“stuck': 4154, '“suggest': 4155, '“temperature': 4156, '“tend': 4157, '“there': 4158, '“thousands': 4159, '“to': 4160, '“typically': 4161, '“underneath': 4162, '“unlike': 4163, '“we’ve': 4164, '“with': 4165, '“yet': 4166, '“‘if': 4167, '“‘the': 4168, '“‘those': 4169, '“…climate': 4170, '…obsessing': 4171, '[south': 0, 'australia]': 0, 'prod\\xaduces': 0, 'reductio\\xadn': 0, 'worrying': 0, 'mutation': 0, '[riebesell]': 0, 'communicated': 0, 'cautiously': 0, 'species…': 0, 'ethiopia': 0, '“intense': 0, 'widespread”': 0, 'indonesia': 0, 'belched': 0, 'waerming': 0, 'accepted': 0, 'correctly': 0, 'ensuing': 0, 'greenland’s': 0, 'speeding': 0, 'travel': 0, 'spectra': 0, 'spreading': 0, '‘next': 0, 'free': 0, 'ice’': 0, 'assumed': 0, 'pivotal': 0, 'back-radiation': 0, 'trivial': 0, 'alligators': 0, 'spitzbergen': 0, '“moreover': 0, 'zones': 0, 'depths': 0, 'debunked': 0, 'british': 0, 'pollsters': 0, 'nightmares': 0, 'challenge': 0, '[feeding': 0, 'people]': 0, 'solubility': 0, 'handbook': 0, 'chemistry': 0, 'function': 0, 'wolfgang': 0, 'knorr': 0, 'sciences': 0, 'bristol': 0, '160': 0, 'quoted': 0, 'ken': 0, 'rocky': 0, 'ball': 0, 'linkage': 0, 'uncertain': 0, 'disputed': 0, 'recognize': 0, 'positives': 0, 'negatives': 0, 'proposals': 0, 'tories': 0, 'labour': 0, '3mm': 0, 'beach': 0, '9mm': 0, 'annually': 0, '“greenland’s': 0, 'susceptible': 0, 'meltdown': 0, '1912': 0, 'zealand': 0, 'contained': 0, 'story': 0, 'pathbreaking': 0, 'rosamond': 0, 'naylor': 0, 'battisti': 0, 'efficiently': 0, 'optimal': 0, 'slope': 0, 'grasslands': 0, 'territorial': 0, 'award': 0, 'winning': 0, 'book': 0, '‘taken': 0, 'storm’': 0, 'christopher': 0, 'essex': 0, '‘temperature': 0, '[like': 0, 'height': 0, 'weight]': 0, 'manufacturing': 0, 'penalise': 0, 'tough': 0, 'updated': 0, 'ghgs': 0, 'matter': 0, 'induce': 0, '“which': 0, 'beans': 0, 'eaten': 0, 'convert': 0, 'broader': 0, 'covers': 0, 'shifting': 0, '24-7': 0, 'meanders': 0, 'loops': 0, 'climes': 0, 'passage': 0, 'opened': 0, 'signature': 0, 'revealing': 0, 'noting': 0, 'stratosphere…': 0, 'oppose': 0, 'closer': 0, 'belief-based': 0, 'critics': 0, 'catholic': 0, 'church': 0, 'austria': 0, 'earliest': 0, 'mountains': 0, 'deceitful': 0, 'termperature': 0, 'jonathan': 0, 'wrote': 0, 'colleague': 0, '[ice]': 0, 'continental': 0, 'accompanied': 0, 'scientifically': 0, 'determinable': 0, 'assigned': 0, 'moisture-laden': 0, 'blew': 0, 'harvey—and': 0, 'cyclones': 0, 'worldwide—suggest': 0, 'soils': 0, 'trees': 0, '39': 0, 'specialise': 0, 'underwent': 0, '‘moist’': 0, 'exposure': 0, 'respiratory': 0, 'infection': 0, 'obstructive': 0, 'pulmonary': 0, 'lung': 0, 'cancer': 0, 'serial': 0, 'tended': 0, 'members': 0, 'nod': 0, '[about': 0, 'extremes]': 0, 'paints': 0, 'ar5': 0, 'exceeded': 0, '‘if': 0, 'young': 0, 'son': 0, 'grown': 0, 'builds': 0, 'soil': 0, 'forming': 0, 'mounds': 0, '‘pingoes': 0, '“[…]the': 0, 'calcification': 0, 'metabolism': 0, 'fertility': 0, 'calcifying': 0, 'lowered': 0, 'shrinks': 0, 'sheet’s': 0, 'gravitational': 0, 'pull': 0, 'relaxes': 0, 'pile': 0, 'coasts': 0, 'destination': 0, '[carbon': 0, 'emissions]': 0, 'black': 0, 'furthermore': 0, 'physically': 0, 'recovering': 0, 'australia’s': 0, 'suicide': 0, '[with': 0, 'notice': 0, 'commit': 0, 'articles': 0, 'trouble': 0, 'subsurface': 0, 'kerr': 0, 'sunlight-reflecting': 0, 'haze': 0, 'cools': 0, 'thinned': 0, 'occurence': 0, 'coincides': 0, 'microwave': 0, 'frequencies': 0, 'outer': 0, 'greenpeace': 0, 'whales': 0, 'switching': 0, 'whale': 0, 'petroleum': 0, 'palm': 0, 'newt': 0, 'gingrich': 0, 'teamed': 0, 'unlikely': 0, '“today': 0, 'obsessed': 0, 'installations': 0, 'eu': 0, 'achieved': 0, 'jim': 0, 'mid-level': 0, 'b': 0, 'presents': 0, 'qualifies': 0, 'studying': 0, 'warn': 0, 'spark': 0, 'books': 0, 'north-east': 0, 'midwest': 0, '30%': 0, 'heavy': 0, 'once-in-every-five': 0, 'downpours': 0, 'parched': 0, 'orbital': 0, 'impacting': 0, 'highlight': 0, 'dwarfed': 0, 'phenomena': 0, 'utterly': 0, 'totally': 0, 'debunks': 0, 'lays': 0, '5°c': 0, 'agreement’s': 0, 'meadows': 0, 'molecules': 0, 'regarding': 0, 'dispersed': 0, 'year’s': 0, 'manifested': 0, 'northeastern': 0, 'regular': 0, 'monthly': 0, 'notes': 0, 'increases…': 0, 'wrecking': 0, 'scheduled': 0, 'partially': 0, 'multiplies': 0, '86': 0, 'choi': 0, 'lake-bottom': 0, 'noted': 0, 'oceanographer': 0, 'walter': 0, 'munk': 0, 'referred': 0, '“enigma”': 0, 'clarified': 0, 'imperative': 0, 'planning': 0, 'basic': 0, 'bbc': 0, 'admitted': 0, 'motions': 0, 'gaining': 0, '18%': 0, 'sudden': 0, 'includes': 0, '‘hockey': 0, 'stick’': 0, 'involve': 0, 'gravimetry': 0, 'estimating': 0, 'terrain': 0, 'detecting': 0, 'passes': 0, 'overhead': 0, 'pollution-free': 0, 'crashing': 0, 'renew\\xadables': 0, 'environmentally': 0, 'disastrous': 0, 'pollute': 0, 'slice': 0, 'dice': 0, 'bird': 0, 'toxins': 0, 'turbine': 0, 'despoil': 0, 'landscape': 0, 'myriad': 0, 'trap': 0, 'strongest': 0, '36–72': 0, 'surprised': 0, 'ringed': 0, 'bearded': 0, 'bering': 0, 'strait': 0, 'thrived': 0, 'open-water': 0, 'conducive': 0, 'fishing': 0, '“nowhere': 0, 'track”': 0, 'commitments': 0, 'suddenly': 0, 'label': 0, 'disservice': 0, 'played': 0, 'sustainability': 0, 'wonderful': 0, 'orbitally': 0, 'barren': 0, 'ran': 0, '‘overestimation’': 0, '‘partly': 0, 'deficiencies': 0, 'simulations': 0, 'attempting': 0, 'moving': 0, 'establishing': 0})\n"
          ]
        }
      ],
      "source": [
        "print(TEXT.vocab.stoi)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "        self.model_type = 'Transformer'\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        #triu returns the upper triangular part of a matrix (2-D tensor) or batch of matrices (see section below)\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src):\n",
        "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "            device = src.device\n",
        "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "            self.src_mask = mask\n",
        "\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.],\n",
              "        [0., 1., 1.],\n",
              "        [0., 0., 1.]])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#triu returns the upper triangular part of a matrix (2-D tensor) or batch of matrices (see section below)\n",
        "torch.triu(torch.ones(3, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf, -inf],\n",
              "        [0., 0., -inf, -inf],\n",
              "        [0., 0., 0., -inf],\n",
              "        [0., 0., 0., 0.]])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Masking\n",
        "def masking():\n",
        "  sz = 4\n",
        "  mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "  mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "\n",
        "  return mask\n",
        "\n",
        "masking()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\julev\\anaconda3\\envs\\Python-3-8\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) #0::2 means starting with index 0, step = 2\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "    \n",
        "ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
        "emsize = 200 # embedding dimension\n",
        "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2 # the number of heads in the multiheadattention models\n",
        "dropout = 0.2 # the dropout value\n",
        "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [
        {
          "ename": "ZeroDivisionError",
          "evalue": "float division by zero",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[34], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     59\u001b[0m train()\n\u001b[1;32m---> 60\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m89\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m| end of epoch \u001b[39m\u001b[38;5;132;01m{:3d}\u001b[39;00m\u001b[38;5;124m | time: \u001b[39m\u001b[38;5;132;01m{:5.2f}\u001b[39;00m\u001b[38;5;124ms | valid loss \u001b[39m\u001b[38;5;132;01m{:5.2f}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     63\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid ppl \u001b[39m\u001b[38;5;132;01m{:8.2f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m epoch_start_time),\n\u001b[0;32m     64\u001b[0m                                  val_loss, math\u001b[38;5;241m.\u001b[39mexp(val_loss)))\n",
            "Cell \u001b[1;32mIn[34], line 51\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(eval_model, data_source)\u001b[0m\n\u001b[0;32m     49\u001b[0m         output_flat \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, ntokens)\n\u001b[0;32m     50\u001b[0m         total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m*\u001b[39m criterion(output_flat, targets)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtotal_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_source\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
          ]
        }
      ],
      "source": [
        "sos_token_id = TEXT.vocab.stoi['<sos>']\n",
        "eos_token_id = TEXT.vocab.stoi['<eos>']\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=sos_token_id)\n",
        "lr = 5.0 # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "import time\n",
        "def train():\n",
        "    model.train()  # Turn on the train mode\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    # The data and targets need to be loaded differently now\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        \n",
        "        ## THIS ONE IS FOR EVIDENCE TEXT OUTPUT\n",
        "        data, _ = get_batch(train_data, i) \n",
        "        targets, _ = get_batch(train_label, i)  # Load targets from train_label\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output.view(-1, ntokens), targets.view(-1))  # Ensure targets are properly shaped\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        log_interval = 200\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
        "                    elapsed * 1000 / log_interval,\n",
        "                    cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(eval_model, data_source):\n",
        "    eval_model.eval() # Turn on the evaluation mode\n",
        "    total_loss = 0.\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output = eval_model(data)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "epochs = 3# The number of epochs\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train()\n",
        "    val_loss = evaluate(model, dev_data)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                     val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = model\n",
        "\n",
        "    scheduler.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  8.42 | test ppl  4538.57\n",
            "=========================================================================================\n"
          ]
        }
      ],
      "source": [
        "test_loss = evaluate(best_model, dev_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)\n",
        "\n",
        "model.eval()\n",
        "output=model(dev_data)\n",
        "probabilities=torch.nn.functional.softmax(output,dim=-1)\n",
        "_,predictedIndex=torch.max(probabilities,dim=-1)\n",
        "# for i in predictedIndex:\n",
        "#     print(predictedIndex[i])\n",
        "\n",
        "# print(LABEL.vocab.stoi)\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "vocab = TEXT.vocab.stoi \n",
        "reverseVocab = {index: word for word, index in vocab.items()} \n",
        "def tensor_to_text(tensor, index_to_word):\n",
        "    words = [index_to_word[index.item()] for index in tensor if index in index_to_word]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# print(reverseVocab)\n",
        "\n",
        "# for i in predictedIndex:\n",
        "#     #print(i)\n",
        "#     words = list(set([reverseVocab[index.item()] for index in i]))\n",
        "#     print(words)\n",
        "\n",
        "\n",
        "#accuracyScore=accuracy_score()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 8.1715e-01,  2.8134e+00,  3.3177e-01,  ..., -1.8889e-01,\n",
            "          -6.7526e-01,  1.9182e-03],\n",
            "         [-1.1501e-01,  2.6535e+00, -4.1210e-01,  ..., -7.6813e-01,\n",
            "          -6.2037e-01, -2.5765e-01],\n",
            "         [-1.1501e-01,  2.6535e+00, -4.1210e-01,  ..., -7.6813e-01,\n",
            "          -6.2037e-01, -2.5765e-01],\n",
            "         ...,\n",
            "         [-4.1553e-01,  3.1095e+00, -8.4322e-01,  ..., -1.1639e-01,\n",
            "          -1.6485e-01, -1.0620e+00],\n",
            "         [-4.1553e-01,  3.1095e+00, -8.4322e-01,  ..., -1.1639e-01,\n",
            "          -1.6485e-01, -1.0620e+00],\n",
            "         [-4.1553e-01,  3.1095e+00, -8.4322e-01,  ..., -1.1639e-01,\n",
            "          -1.6485e-01, -1.0620e+00]],\n",
            "\n",
            "        [[ 9.6800e-01,  2.7248e+00,  5.2278e-01,  ..., -2.4137e-01,\n",
            "          -7.0250e-01,  2.4040e-01],\n",
            "         [-1.8926e-02,  2.5646e+00, -1.9435e-01,  ..., -7.7838e-01,\n",
            "          -6.9473e-01, -2.2776e-02],\n",
            "         [-1.8926e-02,  2.5646e+00, -1.9435e-01,  ..., -7.7838e-01,\n",
            "          -6.9473e-01, -2.2776e-02],\n",
            "         ...,\n",
            "         [-4.0758e-01,  3.1014e+00, -8.2982e-01,  ..., -1.2432e-01,\n",
            "          -1.6596e-01, -1.0473e+00],\n",
            "         [-4.0758e-01,  3.1014e+00, -8.2982e-01,  ..., -1.2432e-01,\n",
            "          -1.6596e-01, -1.0473e+00],\n",
            "         [-4.0758e-01,  3.1014e+00, -8.2982e-01,  ..., -1.2432e-01,\n",
            "          -1.6596e-01, -1.0473e+00]],\n",
            "\n",
            "        [[ 1.1036e+00,  2.7071e+00,  7.5250e-01,  ..., -4.5736e-01,\n",
            "          -6.8846e-01,  4.3506e-01],\n",
            "         [ 5.7705e-02,  2.5703e+00,  8.1820e-02,  ..., -9.7697e-01,\n",
            "          -6.4837e-01,  1.2856e-01],\n",
            "         [ 5.7705e-02,  2.5703e+00,  8.1820e-02,  ..., -9.7697e-01,\n",
            "          -6.4837e-01,  1.2856e-01],\n",
            "         ...,\n",
            "         [-4.0118e-01,  3.1001e+00, -8.1014e-01,  ..., -1.4426e-01,\n",
            "          -1.6088e-01, -1.0388e+00],\n",
            "         [-4.0118e-01,  3.1001e+00, -8.1014e-01,  ..., -1.4426e-01,\n",
            "          -1.6088e-01, -1.0388e+00],\n",
            "         [-4.0118e-01,  3.1001e+00, -8.1014e-01,  ..., -1.4426e-01,\n",
            "          -1.6088e-01, -1.0388e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 8.7528e-01,  2.5684e+00, -6.0759e-02,  ...,  2.2906e-01,\n",
            "          -4.3776e-01,  3.6684e-01],\n",
            "         [-9.5018e-02,  2.7225e+00, -9.7244e-01,  ..., -5.2505e-01,\n",
            "          -1.6692e-01,  3.8915e-02],\n",
            "         [-9.5018e-02,  2.7225e+00, -9.7244e-01,  ..., -5.2505e-01,\n",
            "          -1.6692e-01,  3.8915e-02],\n",
            "         ...,\n",
            "         [-3.4071e-01,  3.1873e+00, -9.3035e-01,  ..., -1.4403e-01,\n",
            "          -2.0292e-01, -1.0568e+00],\n",
            "         [-3.4400e-01,  3.1818e+00, -9.2645e-01,  ..., -1.3827e-01,\n",
            "          -1.9827e-01, -1.0567e+00],\n",
            "         [-3.9009e-01,  3.1192e+00, -9.0213e-01,  ..., -1.3555e-01,\n",
            "          -1.6457e-01, -1.0463e+00]],\n",
            "\n",
            "        [[ 1.0335e+00,  2.5626e+00, -3.2854e-02,  ...,  2.4450e-01,\n",
            "          -4.9684e-01,  3.7902e-01],\n",
            "         [ 6.1217e-02,  2.7051e+00, -9.6831e-01,  ..., -4.8282e-01,\n",
            "          -2.5718e-01,  1.2914e-01],\n",
            "         [ 6.1217e-02,  2.7051e+00, -9.6831e-01,  ..., -4.8282e-01,\n",
            "          -2.5718e-01,  1.2914e-01],\n",
            "         ...,\n",
            "         [-3.3104e-01,  3.1820e+00, -9.2738e-01,  ..., -1.4390e-01,\n",
            "          -2.0616e-01, -1.0564e+00],\n",
            "         [-3.3423e-01,  3.1767e+00, -9.2381e-01,  ..., -1.3837e-01,\n",
            "          -2.0175e-01, -1.0562e+00],\n",
            "         [-3.7677e-01,  3.1182e+00, -9.0132e-01,  ..., -1.3478e-01,\n",
            "          -1.6973e-01, -1.0451e+00]],\n",
            "\n",
            "        [[ 1.1104e+00,  2.5812e+00,  1.1641e-01,  ...,  1.2196e-01,\n",
            "          -5.7256e-01,  3.9091e-01],\n",
            "         [ 1.2379e-01,  2.7027e+00, -7.7232e-01,  ..., -5.6500e-01,\n",
            "          -3.4012e-01,  2.1259e-01],\n",
            "         [ 1.2379e-01,  2.7027e+00, -7.7232e-01,  ..., -5.6500e-01,\n",
            "          -3.4012e-01,  2.1259e-01],\n",
            "         ...,\n",
            "         [-3.2682e-01,  3.1799e+00, -9.1163e-01,  ..., -1.5170e-01,\n",
            "          -2.0982e-01, -1.0570e+00],\n",
            "         [-3.2987e-01,  3.1749e+00, -9.0832e-01,  ..., -1.4644e-01,\n",
            "          -2.0565e-01, -1.0568e+00],\n",
            "         [-3.6959e-01,  3.1189e+00, -8.8748e-01,  ..., -1.4302e-01,\n",
            "          -1.7401e-01, -1.0460e+00]]], grad_fn=<ViewBackward0>)\n",
            "Sentence 1: evidence-583187 global global global global global global global global instrumental 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 2: evidence-583187 global global global global global global global global global 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 3: evidence-583187 global global global global global global global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 4: evidence-583187 global global global global global global global 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 5: evidence-583187 global global global global global global global may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 6: evidence-583187 global global global global global global global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 7: evidence-583187 global global global global global global global 1 1 1 1 1 1 1 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 8: evidence-583187 global global global global global global global 1 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 9: evidence-583187 global global global global global global global 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 may 0 0 0\n",
            "Sentence 10: evidence-583187 global global global global global global global 1 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 11: evidence-583187 global global global global global global global 1 1 1 1 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 12: evidence-583187 global global global global global global global 1 1 1 1 1 1 1 1 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 13: evidence-583187 global global global global global global global 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0\n",
            "Sentence 14: evidence-583187 global global global global global global global 1 1 1 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 15: evidence-583187 global global global global global global global 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 16: evidence-583187 global global global global global global global may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 17: evidence-583187 global global global global global global global 1 1 1 1 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 18: evidence-583187 global global global global global global global 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 19: evidence-583187 global global global global global global global 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 20: evidence-583187 global global global global global global global global may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 21: evidence-583187 global global global global global global global global 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 22: evidence-583187 global global global global global global global global 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 23: evidence-583187 global global global global global global global global 1 1 1 1 1 1 1 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 24: evidence-583187 global global global global global global global 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 25: evidence-583187 global global global global global global global 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 26: evidence-583187 evidence-562427 evidence-562427 evidence-562427 evidence-562427 evidence-562427 evidence-562427 global 1 1 1 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 27: evidence-583187 evidence-562427 evidence-562427 evidence-562427 evidence-562427 evidence-562427 evidence-562427 global 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 28: evidence-583187 evidence-889933 evidence-889933 evidence-889933 evidence-889933 evidence-889933 evidence-889933 global 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 29: evidence-583187 multiple multiple multiple multiple multiple multiple global 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 30: evidence-583187 multiple multiple multiple multiple multiple multiple global change 1 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 31: evidence-583187 multiple multiple multiple multiple multiple multiple global change may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 32: evidence-583187 multiple multiple multiple multiple multiple multiple global change change 1 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 33: evidence-583187 multiple multiple multiple multiple multiple multiple global change change 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 may 0 0 0 0 0 0 0\n",
            "Sentence 34: evidence-583187 evidence-368192 evidence-368192 evidence-368192 evidence-368192 evidence-368192 evidence-368192 global global change 1 1 1 1 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 35: evidence-583187 evidence-368192 evidence-368192 evidence-368192 evidence-368192 evidence-368192 evidence-368192 global global global 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 36: evidence-583187 evidence-368192 evidence-368192 evidence-368192 evidence-368192 evidence-368192 evidence-368192 global global global 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 37: evidence-583187 evidence-368192 evidence-368192 evidence-368192 evidence-368192 evidence-368192 evidence-368192 global global global 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 38: evidence-583187 global global global global global global global global global 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 39: evidence-583187 global global global global global global global global 1 1 1 1 1 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 40: evidence-583187 global global global global global global global global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 41: evidence-583187 global global global global global global global global global 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 42: evidence-583187 global global global global global global global global global 1 1 1 1 1 1 1 1 1 1 1 decade decade may 0 0 0 0 0 0 0 0 0\n",
            "Sentence 43: evidence-583187 global global global global global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 44: evidence-583187 global global global global global global global global may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 45: evidence-583187 global global global global global global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 46: evidence-583187 global global global global global global global global global 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 47: evidence-583187 global global global global global global global global global 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 48: evidence-583187 global global global global global global global global global 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 49: evidence-583187 global global global global global global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 50: evidence-583187 global global global global global global global global global global global 1 1 1 1 decade may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 51: evidence-583187 global global global global global global global global global 1 global 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 52: evidence-583187 global global global global global global global global global 1 global may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 53: evidence-583187 global global global global global global global global global global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 54: evidence-583187 global global global global global global global global global 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 55: evidence-583187 global global global global global global global global global 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 56: evidence-583187 global global global global global global global global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 57: evidence-583187 global global global global global global global global global 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 58: evidence-583187 global global global global global global global global global may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 59: evidence-583187 global global global global global global global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 60: evidence-583187 global global global global global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 61: evidence-583187 global global global global global global global global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 62: evidence-583187 global global global global global global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 63: evidence-583187 global global global global global global global global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 64: evidence-583187 global global global global global global global global global global global 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 65: evidence-583187 global global global global global global global global global may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 66: evidence-583187 global global global global global global global global global global may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 67: evidence-583187 global global global global global global global global global global may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 68: evidence-583187 global global global global global global global global global 1 1 1 1 1 1 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 69: evidence-583187 global global global global global global global global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 70: evidence-583187 global global global global global global global global may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 71: evidence-583187 global global global global global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 72: evidence-583187 global global global global global global global global influenced influenced 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 73: evidence-583187 global global global global global global global global global warming warming warming warming warming warming warming may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 74: evidence-583187 global global global global global global global global global warming warming warming warming warming may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 75: evidence-583187 global global global global global global global global global 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 76: evidence-583187 evidence-368192 evidence-368192 evidence-368192 evidence-368192 global global global global global may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 77: evidence-583187 global global global global global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 78: evidence-583187 global global global global global global global 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 79: evidence-583187 global global global global global global since 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 80: evidence-583187 global global global global global global global global 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 81: evidence-583187 global global global global global global global global 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 82: evidence-583187 global global global global global global global global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 83: evidence-583187 global global global global global global global global global 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 may 0 0 0 0 0 0 0\n",
            "Sentence 84: evidence-583187 global global global global global global global global 1 1 1 1 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 85: evidence-583187 global global global global global global global may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 86: evidence-583187 global global global global global global 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 87: evidence-583187 global global global global global global 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 88: evidence-583187 global global global global global global 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0\n",
            "Sentence 89: evidence-583187 global global global global global global global may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 90: evidence-583187 global global global global global global global may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 91: evidence-583187 evidence-368192 evidence-368192 evidence-368192 evidence-368192 global global global global 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 92: evidence-583187 evidence-368192 evidence-368192 evidence-368192 evidence-368192 global global global global may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 93: evidence-583187 evidence-368192 evidence-368192 evidence-368192 evidence-368192 global global global 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 94: evidence-583187 global global global global global global global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 95: evidence-583187 global global global global global global global 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 96: evidence-583187 global global global global global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 97: evidence-583187 global global global global global global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 98: evidence-583187 global global global global global global global global 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 99: evidence-583187 global global global global global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 100: evidence-583187 global global global global global global global global 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 decade decade decade 1 may 0 0\n",
            "Sentence 101: evidence-583187 global global global global global global global global 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 102: evidence-583187 evidence-368192 evidence-368192 evidence-368192 evidence-368192 global global global global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 103: evidence-583187 evidence-368192 evidence-368192 evidence-368192 evidence-368192 global global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 104: evidence-583187 evidence-368192 evidence-368192 evidence-368192 evidence-368192 global global global global influenced 1 1 1 warming warming 1 decade decade decade decade decade decade decade decade decade decade decade may 0 0 0 0 0\n",
            "Sentence 105: evidence-583187 evidence-368192 evidence-368192 evidence-368192 evidence-368192 global global global global 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 106: evidence-583187 global global global global global global global may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 107: evidence-583187 global global global global global global global global 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 108: evidence-583187 global global global global global global global global 1 1 1 1 1 1 1 1 1 decade decade decade decade decade decade decade decade decade decade decade decade decade decade may\n",
            "Sentence 109: evidence-583187 global global global global global global global global 1 1 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 110: evidence-583187 global global global global global global global global 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 111: evidence-583187 global global global global global global global global 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 112: evidence-583187 global global global global global global may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 113: evidence-583187 global global global global global may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 114: evidence-583187 global global global global global global global global 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 115: evidence-583187 global global global global global global global global 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 116: evidence-583187 global global global global global global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 117: evidence-583187 global global global global global global global global 1 1 1 1 1 1 1 decade decade decade decade decade decade may 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 118: evidence-583187 global global global global global global global global 1 1 1 1 1 1 1 decade decade decade decade decade decade decade decade decade may 0 0 0 0 0 0 0\n",
            "Sentence 119: evidence-583187 global global global global global global global global may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 120: evidence-583187 global global global global global global global 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0\n",
            "Sentence 121: evidence-583187 global global global global global global 1 1 1 1 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 122: evidence-583187 global global global global global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 123: evidence-583187 global global global global global dioxide 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 124: evidence-583187 evidence-269919 evidence-269919 evidence-269919 evidence-269919 global since 1 1 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 125: evidence-583187 since since since since global since 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 126: evidence-583187 global global global global global since 1 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 127: evidence-583187 global global global global global since 1 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 128: evidence-583187 since since since since global global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 129: evidence-583187 since since since since global since since 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 130: evidence-583187 since since since since global since since 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 131: evidence-583187 since since since since global since since 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 132: evidence-583187 since since since since global since since 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 decade may 0 0 0 0 0 0 0 0\n",
            "Sentence 133: evidence-583187 global global global global global since 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 may 0 0 0 0\n",
            "Sentence 134: evidence-583187 global global global global global global 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 may 0 0 0 0\n",
            "Sentence 135: evidence-583187 global global global global global global global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 136: evidence-583187 evidence-368192 evidence-368192 evidence-368192 evidence-368192 global global 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 137: evidence-583187 evidence-368192 evidence-368192 evidence-368192 evidence-368192 global global 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 138: evidence-583187 evidence-368192 evidence-368192 evidence-368192 evidence-368192 global global influenced influenced influenced 1 1 1 1 1 1 1 1 decade decade decade may 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 139: evidence-583187 evidence-368192 evidence-368192 evidence-368192 evidence-368192 global influenced influenced influenced influenced influenced may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 140: evidence-583187 evidence-368192 evidence-368192 evidence-368192 evidence-368192 global global influenced influenced influenced influenced influenced influenced warming warming warming warming warming decade decade decade decade decade decade decade decade decade may 0 0 0 0 0\n",
            "Sentence 141: evidence-583187 evidence-368192 evidence-368192 evidence-368192 evidence-368192 global global influenced 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 142: evidence-583187 evidence-368192 evidence-368192 evidence-368192 evidence-368192 global global 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 143: evidence-583187 global global global global global global 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 144: evidence-583187 global global global global global global 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 145: evidence-583187 global global global global global global 1 global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 146: evidence-583187 global global global global global global 1 1 1 1 1 1 1 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 147: evidence-583187 global global global global global global 1 1 1 1 1 1 1 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 148: evidence-583187 global global global global global global 1 1 1 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 149: evidence-583187 global global global global global global melting melting 1 may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 150: evidence-583187 global global global global global global melting melting 1 1 1 1 1 1 decade may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 151: evidence-583187 global global global global global global melting melting 1 1 1 1 1 1 1 decade decade decade decade decade decade decade decade may 0 0 0 0 0 0 0 0\n",
            "Sentence 152: evidence-583187 global global global global global global global global 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Sentence 153: evidence-583187 global global global global global global influenced influenced influenced may 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        }
      ],
      "source": [
        "pred=model(test_data)\n",
        "print(pred)\n",
        "import torch\n",
        "\n",
        "# use softmax\n",
        "probabilities = torch.softmax(pred, dim=-1)\n",
        "_, predicted_indices = torch.max(probabilities, dim=-1)\n",
        "predicted_words = [[TEXT.vocab.itos[index] for index in example] for example in predicted_indices]\n",
        "predicted_text={}\n",
        "# Print the words\n",
        "for i, sentence in enumerate(predicted_words):\n",
        "    print(f\"Sentence {i+1}: {' '.join(sentence)}\")\n",
        "    predicted_text[f'Sentence{i}']={' '.join(sentence)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Sentence0': ['5 °c °c °c °c °c °c °c °c 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence1': ['5 °c °c °c °c °c °c °c °c 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence2': ['5 °c °c °c °c °c °c 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence3': ['5 °c °c °c °c °c °c 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence4': ['5 °c °c °c °c °c °c 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence5': ['5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence6': ['5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence7': ['5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 may 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence8': ['5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 may 1 1 1'], 'Sentence9': ['5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 may 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence10': ['5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence11': ['5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1'], 'Sentence12': ['5 global global global global global global 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1'], 'Sentence13': ['5 global global global global global global 1 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence14': ['5 global global global global global global 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence15': ['5 global global global global global global 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence16': ['5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 may 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence17': ['5 5 5 5 5 5 5 1 1 1 1 1 may 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence18': ['5 5 5 5 5 5 5 1 1 1 1 1 1 1 may 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence19': ['5 5 5 5 5 5 5 1 1 may 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence20': ['5 5 5 5 5 5 5 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence21': ['5 5 5 5 5 5 5 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence22': ['5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 may 1 1 1 1 1 1 1 1 1 1'], 'Sentence23': ['5 5 5 5 5 5 5 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence24': ['5 5 5 5 5 5 5 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence25': ['5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence26': ['5 5 5 5 5 5 5 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence27': ['5 5 5 5 5 5 5 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence28': ['5 5 5 5 5 5 5 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence29': ['5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence30': ['5 5 5 5 5 5 5 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence31': ['5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence32': ['5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence33': ['5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence34': ['5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence35': ['5 5 5 5 5 5 5 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence36': ['5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 may 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence37': ['5 5 5 5 5 5 5 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence38': ['5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence39': ['5 5 5 5 5 5 5 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence40': ['5 5 5 5 5 5 5 5 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence41': ['5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1'], 'Sentence42': ['5 5 5 5 5 global global global global 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence43': ['5 5 5 5 5 5 1 1 1 may 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence44': ['5 5 5 5 5 5 may global 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence45': ['5 5 5 5 5 global 1 1 1 1 1 1 1 1 1 may 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence46': ['5 global global global global global 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence47': ['5 global global global global global 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence48': ['5 global global global global global global global global 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence49': ['5 global global global global global 1 1 1 1 1 1 1 1 1 1 1 may 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence50': ['5 global global global global global 1 1 1 1 1 1 1 1 1 may 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence51': ['5 global global global global global 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence52': ['5 global global global global global 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence53': ['5 global global global global global 1 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence54': ['5 global global global global global 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence55': ['5 5 5 5 5 global 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence56': ['5 5 5 5 5 global 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence57': ['5 5 5 5 5 5 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence58': ['5 5 5 5 5 5 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence59': ['5 5 5 5 5 global global global 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence60': ['5 5 5 5 5 global 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence61': ['5 5 5 5 5 global warming global global 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence62': ['5 global global global global global 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence63': ['5 global global global global global 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence64': ['5 global global global global global 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence65': ['5 5 5 5 5 global 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence66': ['5 5 5 5 5 global 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence67': ['5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1'], 'Sentence68': ['5 5 5 5 5 5 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence69': ['5 5 5 5 5 5 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence70': ['5 5 5 5 5 global global global global 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence71': ['5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence72': ['5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence73': ['5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence74': ['5 5 5 5 5 5 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence75': ['5 5 5 5 5 5 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence76': ['5 5 5 5 5 global global global 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence77': ['5 5 5 5 5 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence78': ['5 5 5 5 5 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence79': ['5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence80': ['5 5 5 5 5 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence81': ['5 5 5 5 5 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence82': ['5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence83': ['5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence84': ['5 5 5 5 5 5 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence85': ['5 5 5 5 5 5 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence86': ['5 5 5 5 5 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence87': ['5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence88': ['5 5 5 5 5 1 1 1 may 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence89': ['5 5 5 5 5 1 1 1 may 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence90': ['5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 may 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence91': ['5 5 5 5 5 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence92': ['5 °c °c °c °c 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence93': ['5 °c °c °c °c 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence94': ['5 °c °c °c °c 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence95': ['5 °c °c °c °c global global global 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence96': ['5 °c °c °c °c 1 warming global 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence97': ['5 °c °c °c °c 1 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence98': ['5 °c °c °c °c global global global 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence99': ['5 °c °c °c °c 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence100': ['5 °c °c °c °c 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence101': ['5 °c °c °c °c 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence102': ['5 °c °c °c °c 1 warming global 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence103': ['5 °c °c °c °c 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1'], 'Sentence104': ['5 global global global global 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence105': ['5 global global global global 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence106': ['5 global global global global 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence107': ['5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence108': ['5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence109': ['5 5 5 5 5 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence110': ['5 5 5 5 5 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence111': ['5 5 5 5 5 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence112': ['5 5 5 5 5 1 warming global 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence113': ['5 5 5 5 5 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence114': ['5 5 5 5 5 global 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence115': ['5 5 5 5 5 global warming global global 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence116': ['5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1'], 'Sentence117': ['5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence118': ['5 5 5 5 5 5 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence119': ['5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence120': ['5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence121': ['5 °c °c °c °c global global global 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence122': ['5 °c °c °c °c 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence123': ['5 °c °c °c °c 1 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence124': ['5 °c °c °c °c 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence125': ['5 5 5 5 5 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence126': ['5 5 5 5 5 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence127': ['5 5 5 5 5 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence128': ['5 5 5 5 5 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence129': ['5 5 5 5 5 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence130': ['5 °c °c °c °c 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence131': ['5 °c °c °c °c 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1'], 'Sentence132': ['5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence133': ['5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence134': ['5 5 5 5 5 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence135': ['5 5 5 5 5 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence136': ['5 5 5 5 5 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence137': ['5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence138': ['5 5 5 5 5 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence139': ['5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1'], 'Sentence140': ['5 5 5 5 5 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence141': ['5 5 5 5 5 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence142': ['5 5 5 5 5 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence143': ['5 °c °c °c °c 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence144': ['5 °c °c °c °c 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence145': ['5 °c °c °c °c 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence146': ['5 °c °c °c °c 1 1 1 1 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence147': ['5 °c °c °c °c 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence148': ['5 °c °c °c °c 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence149': ['5 °c °c °c °c 5 1 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence150': ['5 °c °c °c °c 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 warming 1 1 1 1 1 1 1 1'], 'Sentence151': ['5 5 5 5 5 global 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1'], 'Sentence152': ['5 5 5 5 5 global 1 1 1 1 warming 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1']}\n",
            "{'Sentence0': {'1', '°c', '5'}, 'Sentence1': {'1', '°c', '5'}, 'Sentence2': {'1', '°c', '5'}, 'Sentence3': {'warming', '1', '°c', '5'}, 'Sentence4': {'warming', '1', '°c', '5'}, 'Sentence5': {'1', '5'}, 'Sentence6': {'1', '5'}, 'Sentence7': {'may', '1', '5'}, 'Sentence8': {'may', '1', '5'}, 'Sentence9': {'may', '1', '5'}, 'Sentence10': {'warming', '1', '5'}, 'Sentence11': {'warming', '1', '5'}, 'Sentence12': {'global', '1', 'warming', '5'}, 'Sentence13': {'global', '1', 'warming', '5'}, 'Sentence14': {'global', '1', 'warming', '5'}, 'Sentence15': {'global', '1', 'warming', '5'}, 'Sentence16': {'may', '1', '5'}, 'Sentence17': {'may', '1', '5'}, 'Sentence18': {'may', '1', '5'}, 'Sentence19': {'may', '1', '5'}, 'Sentence20': {'warming', '1', '5'}, 'Sentence21': {'warming', '1', '5'}, 'Sentence22': {'may', '1', '5'}, 'Sentence23': {'warming', '1', '5'}, 'Sentence24': {'warming', '1', '5'}, 'Sentence25': {'warming', '1', '5'}, 'Sentence26': {'warming', '1', '5'}, 'Sentence27': {'warming', '1', '5'}, 'Sentence28': {'warming', '1', '5'}, 'Sentence29': {'warming', '1', '5'}, 'Sentence30': {'warming', '1', '5'}, 'Sentence31': {'warming', '1', '5'}, 'Sentence32': {'1', '5'}, 'Sentence33': {'warming', '1', '5'}, 'Sentence34': {'warming', '1', '5'}, 'Sentence35': {'warming', '1', '5'}, 'Sentence36': {'may', '1', '5'}, 'Sentence37': {'warming', '1', '5'}, 'Sentence38': {'warming', '1', '5'}, 'Sentence39': {'warming', '1', '5'}, 'Sentence40': {'warming', '1', '5'}, 'Sentence41': {'warming', '1', '5'}, 'Sentence42': {'global', '1', '5'}, 'Sentence43': {'may', '1', '5'}, 'Sentence44': {'may', 'global', '1', '5'}, 'Sentence45': {'may', 'global', '1', '5'}, 'Sentence46': {'global', '1', 'warming', '5'}, 'Sentence47': {'global', '1', 'warming', '5'}, 'Sentence48': {'global', '1', '5'}, 'Sentence49': {'may', 'global', '1', '5'}, 'Sentence50': {'may', 'global', '1', '5'}, 'Sentence51': {'global', '1', 'warming', '5'}, 'Sentence52': {'global', '1', 'warming', '5'}, 'Sentence53': {'global', '1', 'warming', '5'}, 'Sentence54': {'global', '1', 'warming', '5'}, 'Sentence55': {'global', '1', 'warming', '5'}, 'Sentence56': {'global', '1', 'warming', '5'}, 'Sentence57': {'warming', '1', '5'}, 'Sentence58': {'warming', '1', '5'}, 'Sentence59': {'global', '1', '5'}, 'Sentence60': {'global', '1', 'warming', '5'}, 'Sentence61': {'global', '1', 'warming', '5'}, 'Sentence62': {'global', '1', 'warming', '5'}, 'Sentence63': {'global', '1', 'warming', '5'}, 'Sentence64': {'global', '1', 'warming', '5'}, 'Sentence65': {'global', '1', 'warming', '5'}, 'Sentence66': {'global', '1', 'warming', '5'}, 'Sentence67': {'warming', '1', '5'}, 'Sentence68': {'warming', '1', '5'}, 'Sentence69': {'warming', '1', '5'}, 'Sentence70': {'global', '1', '5'}, 'Sentence71': {'warming', '1', '5'}, 'Sentence72': {'warming', '1', '5'}, 'Sentence73': {'warming', '1', '5'}, 'Sentence74': {'warming', '1', '5'}, 'Sentence75': {'warming', '1', '5'}, 'Sentence76': {'global', '1', '5'}, 'Sentence77': {'warming', '1', '5'}, 'Sentence78': {'warming', '1', '5'}, 'Sentence79': {'warming', '1', '5'}, 'Sentence80': {'warming', '1', '5'}, 'Sentence81': {'warming', '1', '5'}, 'Sentence82': {'1', '5'}, 'Sentence83': {'warming', '1', '5'}, 'Sentence84': {'warming', '1', '5'}, 'Sentence85': {'warming', '1', '5'}, 'Sentence86': {'warming', '1', '5'}, 'Sentence87': {'1', '5'}, 'Sentence88': {'may', '1', '5'}, 'Sentence89': {'may', '1', '5'}, 'Sentence90': {'may', '1', '5'}, 'Sentence91': {'warming', '1', '5'}, 'Sentence92': {'warming', '1', '°c', '5'}, 'Sentence93': {'warming', '1', '°c', '5'}, 'Sentence94': {'warming', '1', '°c', '5'}, 'Sentence95': {'global', '1', '°c', '5'}, 'Sentence96': {'warming', '1', '°c', '5', 'global'}, 'Sentence97': {'warming', '1', '°c', '5'}, 'Sentence98': {'global', '1', '°c', '5'}, 'Sentence99': {'1', '°c', '5'}, 'Sentence100': {'warming', '1', '°c', '5'}, 'Sentence101': {'warming', '1', '°c', '5'}, 'Sentence102': {'warming', '1', '°c', '5', 'global'}, 'Sentence103': {'warming', '1', '°c', '5'}, 'Sentence104': {'global', '1', 'warming', '5'}, 'Sentence105': {'global', '1', 'warming', '5'}, 'Sentence106': {'global', '1', 'warming', '5'}, 'Sentence107': {'1', '5'}, 'Sentence108': {'warming', '1', '5'}, 'Sentence109': {'warming', '1', '5'}, 'Sentence110': {'warming', '1', '5'}, 'Sentence111': {'warming', '1', '5'}, 'Sentence112': {'global', 'warming', '1', '5'}, 'Sentence113': {'warming', '1', '5'}, 'Sentence114': {'global', '1', 'warming', '5'}, 'Sentence115': {'global', '1', 'warming', '5'}, 'Sentence116': {'warming', '1', '5'}, 'Sentence117': {'1', '5'}, 'Sentence118': {'warming', '1', '5'}, 'Sentence119': {'1', '5'}, 'Sentence120': {'warming', '1', '5'}, 'Sentence121': {'global', '1', '°c', '5'}, 'Sentence122': {'warming', '1', '°c', '5'}, 'Sentence123': {'warming', '1', '°c', '5'}, 'Sentence124': {'warming', '1', '°c', '5'}, 'Sentence125': {'warming', '1', '5'}, 'Sentence126': {'warming', '1', '5'}, 'Sentence127': {'warming', '1', '5'}, 'Sentence128': {'warming', '1', '5'}, 'Sentence129': {'warming', '1', '5'}, 'Sentence130': {'warming', '1', '°c', '5'}, 'Sentence131': {'warming', '1', '°c', '5'}, 'Sentence132': {'1', '5'}, 'Sentence133': {'1', '5'}, 'Sentence134': {'warming', '1', '5'}, 'Sentence135': {'warming', '1', '5'}, 'Sentence136': {'warming', '1', '5'}, 'Sentence137': {'warming', '1', '5'}, 'Sentence138': {'warming', '1', '5'}, 'Sentence139': {'warming', '1', '5'}, 'Sentence140': {'warming', '1', '5'}, 'Sentence141': {'warming', '1', '5'}, 'Sentence142': {'warming', '1', '5'}, 'Sentence143': {'warming', '1', '°c', '5'}, 'Sentence144': {'warming', '1', '°c', '5'}, 'Sentence145': {'warming', '1', '°c', '5'}, 'Sentence146': {'warming', '1', '°c', '5'}, 'Sentence147': {'warming', '1', '°c', '5'}, 'Sentence148': {'warming', '1', '°c', '5'}, 'Sentence149': {'warming', '1', '°c', '5'}, 'Sentence150': {'warming', '1', '°c', '5'}, 'Sentence151': {'global', '1', 'warming', '5'}, 'Sentence152': {'global', '1', 'warming', '5'}}\n"
          ]
        }
      ],
      "source": [
        "remove_tokens = ['<pad>', '<unks>', '<sos>', '<eos>']\n",
        "def clean_sentence(sentence, tokens_to_remove):\n",
        "    for token in tokens_to_remove:\n",
        "        sentence = sentence.replace(token, '')\n",
        "    return sentence.strip()\n",
        "cleaned_data = {key: [clean_sentence(sentence, remove_tokens) for sentence in value] for key, value in predicted_text.items()}\n",
        "print(cleaned_data)\n",
        "\n",
        "# vectorizer = TfidfVectorizer()\n",
        "# tfidf_matrix = vectorizer.fit_transform(evidenceDataframe['combined_evidence'])\n",
        "# print(tfidf_matrix)\n",
        "\n",
        "unique_words_per_sentence = {}\n",
        "for key, value in cleaned_data.items():\n",
        "    # Split by spaces and filter out numeric values and punctuation\n",
        "    words = set(word.strip('–,.') for word in value[0].split())\n",
        "    unique_words_per_sentence[key] = words\n",
        "\n",
        "print(unique_words_per_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# def find_evidence_ids(words):\n",
        "#     matched_ids = []\n",
        "#     for word in words:\n",
        "#         ids = evidenceDataframe[evidenceDataframe['evidence_text'].str.contains(fr'\\b{word}\\b', na=False, case=False)]['evidence_id'].tolist()\n",
        "#         matched_ids.extend(ids)\n",
        "#     return set(matched_ids)\n",
        "\n",
        "# evidence_ids_per_sentence = {key: find_evidence_ids(words) for key, words in unique_words_per_sentence.items()}\n",
        "\n",
        "# print(evidence_ids_per_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#print(evidence_ids_per_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
